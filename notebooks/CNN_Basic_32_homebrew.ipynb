{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ee38e8",
   "metadata": {},
   "source": [
    "# Implementation of 9-layer CNN (pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b2250c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model' from '..\\\\src\\\\model.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General imports \n",
    "import sys\n",
    "import os \n",
    "sys.path.insert(1, os.path.join(os.pardir, 'src'))\n",
    "from itertools import product\n",
    "\n",
    "# Data imports\n",
    "import torch\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Homebrew imports \n",
    "import model\n",
    "from utils import one_hot_encode_index\n",
    "from optimizers import Adam\n",
    "from activations import Softmax, ReLU\n",
    "from layers import Dropout, LinearLayer, ConvolutionLayer, PoolingLayer, FlattenLayer\n",
    "from loss import CategoricalCrossEntropyLoss\n",
    "\n",
    "## TESTING \n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dbb6cc",
   "metadata": {},
   "source": [
    "## Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d69b9a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(32),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "                                      ])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(33),\n",
    "                                      transforms.CenterCrop(32),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "                                    ])\n",
    "\n",
    "# setting up data loaders\n",
    "data_dir = os.path.join(os.pardir, 'data', 'Plant_leave_diseases_32')\n",
    "\n",
    "train_data = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=train_transforms)\n",
    "test_data = datasets.ImageFolder(os.path.join(data_dir, 'validation'), transform=test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51af43ca",
   "metadata": {},
   "source": [
    "### Train config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d59977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs \n",
    "config = {\n",
    "    'max_epochs': 100,\n",
    "    'learning_rate': 0.003,\n",
    "    'resolution': 32,\n",
    "    'name': 'CNN_Basic_1CNN_module_homebrew'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce71e6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3b2d3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture: \n",
      "\t (0): ConvolutionLayer (Trainable: False)\n",
      "\t (1): ReLU (Trainable: False)\n",
      "\t (2): PoolingLayer (Trainable: False)\n",
      "\t (3): FlattenLayer (Trainable: False)\n",
      "\t (4): LinearLayer (Trainable: True)\n",
      "\t (5): ReLU (Trainable: False)\n",
      "\t (6): LinearLayer (Trainable: True)\n",
      "\t (7): ReLU (Trainable: False)\n",
      "\t (8): LinearLayer (Trainable: True)\n",
      "\t (9): Softmax (Trainable: False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mdl = model.Model(Adam(learning_rate=config['learning_rate']),\n",
    "                      CategoricalCrossEntropyLoss())\n",
    "\n",
    "# Config early stop \n",
    "mdl.add_early_stop(5)\n",
    "\n",
    "mdl.set_save_config(model_name=config['name'], save_path=os.path.join(os.pardir, 'model'))\n",
    "\n",
    "# Defining architecture \n",
    "\n",
    "mdl.set_sequence([\n",
    "                    ConvolutionLayer(3, 32, 3),\n",
    "                    ReLU(),\n",
    "                    PoolingLayer(32, 2),\n",
    "                    FlattenLayer(),\n",
    "                    LinearLayer(7200, 1568),\n",
    "                    ReLU(),\n",
    "                    LinearLayer(1568, 128),\n",
    "                    ReLU(),\n",
    "                    LinearLayer(128, 39),\n",
    "                    Softmax()\n",
    "                ])\n",
    "print(mdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb41934",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14df8745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch: 1 ===\n",
      "Step: 0/865, accuracy0.047, loss3.687, learning rate 0.0030000 \n",
      "Step: 5/865, accuracy0.062, loss5.572, learning rate 0.0030000 \n",
      "Step: 10/865, accuracy0.094, loss6.248, learning rate 0.0030000 \n",
      "Step: 15/865, accuracy0.078, loss5.391, learning rate 0.0030000 \n",
      "Step: 20/865, accuracy0.094, loss4.016, learning rate 0.0030000 \n",
      "Step: 25/865, accuracy0.094, loss4.448, learning rate 0.0030000 \n",
      "Step: 30/865, accuracy0.031, loss3.667, learning rate 0.0030000 \n",
      "Step: 35/865, accuracy0.156, loss3.736, learning rate 0.0030000 \n",
      "Step: 40/865, accuracy0.094, loss3.625, learning rate 0.0030000 \n",
      "Step: 45/865, accuracy0.141, loss3.555, learning rate 0.0030000 \n",
      "Step: 50/865, accuracy0.031, loss3.659, learning rate 0.0030000 \n",
      "Step: 55/865, accuracy0.141, loss3.456, learning rate 0.0030000 \n",
      "Step: 60/865, accuracy0.172, loss3.501, learning rate 0.0030000 \n",
      "Step: 65/865, accuracy0.125, loss3.312, learning rate 0.0030000 \n",
      "Step: 70/865, accuracy0.109, loss3.403, learning rate 0.0030000 \n",
      "Step: 75/865, accuracy0.109, loss3.469, learning rate 0.0030000 \n",
      "Step: 80/865, accuracy0.109, loss3.350, learning rate 0.0030000 \n",
      "Step: 85/865, accuracy0.094, loss3.547, learning rate 0.0030000 \n",
      "Step: 90/865, accuracy0.172, loss3.398, learning rate 0.0030000 \n",
      "Step: 95/865, accuracy0.109, loss3.443, learning rate 0.0030000 \n",
      "Step: 100/865, accuracy0.094, loss3.432, learning rate 0.0030000 \n",
      "Step: 105/865, accuracy0.109, loss3.444, learning rate 0.0030000 \n",
      "Step: 110/865, accuracy0.078, loss3.334, learning rate 0.0030000 \n",
      "Step: 115/865, accuracy0.094, loss3.316, learning rate 0.0030000 \n",
      "Step: 120/865, accuracy0.062, loss3.572, learning rate 0.0030000 \n",
      "Step: 125/865, accuracy0.078, loss3.495, learning rate 0.0030000 \n",
      "Step: 130/865, accuracy0.141, loss3.582, learning rate 0.0030000 \n",
      "Step: 135/865, accuracy0.062, loss3.508, learning rate 0.0030000 \n",
      "Step: 140/865, accuracy0.078, loss3.571, learning rate 0.0030000 \n",
      "Step: 145/865, accuracy0.109, loss3.391, learning rate 0.0030000 \n",
      "Step: 150/865, accuracy0.047, loss3.460, learning rate 0.0030000 \n",
      "Step: 155/865, accuracy0.141, loss3.376, learning rate 0.0030000 \n",
      "Step: 160/865, accuracy0.203, loss3.415, learning rate 0.0030000 \n",
      "Step: 165/865, accuracy0.047, loss3.568, learning rate 0.0030000 \n",
      "Step: 170/865, accuracy0.078, loss3.395, learning rate 0.0030000 \n",
      "Step: 175/865, accuracy0.219, loss3.365, learning rate 0.0030000 \n",
      "Step: 180/865, accuracy0.125, loss3.398, learning rate 0.0030000 \n",
      "Step: 185/865, accuracy0.172, loss3.353, learning rate 0.0030000 \n",
      "Step: 190/865, accuracy0.109, loss3.398, learning rate 0.0030000 \n",
      "Step: 195/865, accuracy0.172, loss3.405, learning rate 0.0030000 \n",
      "Step: 200/865, accuracy0.109, loss3.563, learning rate 0.0030000 \n",
      "Step: 205/865, accuracy0.125, loss3.463, learning rate 0.0030000 \n",
      "Step: 210/865, accuracy0.125, loss3.458, learning rate 0.0030000 \n",
      "Step: 215/865, accuracy0.062, loss3.522, learning rate 0.0030000 \n",
      "Step: 220/865, accuracy0.203, loss3.335, learning rate 0.0030000 \n",
      "Step: 225/865, accuracy0.109, loss3.615, learning rate 0.0030000 \n",
      "Step: 230/865, accuracy0.172, loss3.476, learning rate 0.0030000 \n",
      "Step: 235/865, accuracy0.188, loss3.282, learning rate 0.0030000 \n",
      "Step: 240/865, accuracy0.141, loss3.477, learning rate 0.0030000 \n",
      "Step: 245/865, accuracy0.250, loss3.263, learning rate 0.0030000 \n",
      "Step: 250/865, accuracy0.203, loss3.229, learning rate 0.0030000 \n",
      "Step: 255/865, accuracy0.141, loss3.497, learning rate 0.0030000 \n",
      "Step: 260/865, accuracy0.203, loss3.410, learning rate 0.0030000 \n",
      "Step: 265/865, accuracy0.141, loss3.606, learning rate 0.0030000 \n",
      "Step: 270/865, accuracy0.172, loss3.517, learning rate 0.0030000 \n",
      "Step: 275/865, accuracy0.172, loss3.419, learning rate 0.0030000 \n",
      "Step: 280/865, accuracy0.141, loss3.452, learning rate 0.0030000 \n",
      "Step: 285/865, accuracy0.172, loss3.463, learning rate 0.0030000 \n",
      "Step: 290/865, accuracy0.125, loss3.513, learning rate 0.0030000 \n",
      "Step: 295/865, accuracy0.156, loss3.311, learning rate 0.0030000 \n",
      "Step: 300/865, accuracy0.141, loss3.299, learning rate 0.0030000 \n",
      "Step: 305/865, accuracy0.109, loss3.381, learning rate 0.0030000 \n",
      "Step: 310/865, accuracy0.109, loss3.472, learning rate 0.0030000 \n",
      "Step: 315/865, accuracy0.094, loss3.431, learning rate 0.0030000 \n",
      "Step: 320/865, accuracy0.109, loss3.515, learning rate 0.0030000 \n",
      "Step: 325/865, accuracy0.172, loss3.346, learning rate 0.0030000 \n",
      "Step: 330/865, accuracy0.234, loss3.232, learning rate 0.0030000 \n",
      "Step: 335/865, accuracy0.141, loss3.184, learning rate 0.0030000 \n",
      "Step: 340/865, accuracy0.203, loss3.328, learning rate 0.0030000 \n",
      "Step: 345/865, accuracy0.141, loss3.467, learning rate 0.0030000 \n",
      "Step: 350/865, accuracy0.125, loss3.469, learning rate 0.0030000 \n",
      "Step: 355/865, accuracy0.172, loss3.412, learning rate 0.0030000 \n",
      "Step: 360/865, accuracy0.125, loss3.565, learning rate 0.0030000 \n",
      "Step: 365/865, accuracy0.125, loss3.399, learning rate 0.0030000 \n",
      "Step: 370/865, accuracy0.062, loss3.496, learning rate 0.0030000 \n",
      "Step: 375/865, accuracy0.188, loss3.504, learning rate 0.0030000 \n",
      "Step: 380/865, accuracy0.188, loss3.336, learning rate 0.0030000 \n",
      "Step: 385/865, accuracy0.109, loss3.497, learning rate 0.0030000 \n",
      "Step: 390/865, accuracy0.188, loss3.328, learning rate 0.0030000 \n",
      "Step: 395/865, accuracy0.078, loss3.439, learning rate 0.0030000 \n",
      "Step: 400/865, accuracy0.141, loss3.451, learning rate 0.0030000 \n",
      "Step: 405/865, accuracy0.234, loss3.228, learning rate 0.0030000 \n",
      "Step: 410/865, accuracy0.125, loss3.545, learning rate 0.0030000 \n",
      "Step: 415/865, accuracy0.125, loss3.416, learning rate 0.0030000 \n",
      "Step: 420/865, accuracy0.156, loss3.433, learning rate 0.0030000 \n",
      "Step: 425/865, accuracy0.156, loss3.356, learning rate 0.0030000 \n",
      "Step: 430/865, accuracy0.125, loss3.338, learning rate 0.0030000 \n",
      "Step: 435/865, accuracy0.141, loss3.257, learning rate 0.0030000 \n",
      "Step: 440/865, accuracy0.125, loss3.359, learning rate 0.0030000 \n",
      "Step: 445/865, accuracy0.125, loss3.439, learning rate 0.0030000 \n",
      "Step: 450/865, accuracy0.219, loss3.146, learning rate 0.0030000 \n",
      "Step: 455/865, accuracy0.062, loss3.536, learning rate 0.0030000 \n",
      "Step: 460/865, accuracy0.141, loss3.378, learning rate 0.0030000 \n",
      "Step: 465/865, accuracy0.062, loss3.521, learning rate 0.0030000 \n",
      "Step: 470/865, accuracy0.156, loss3.328, learning rate 0.0030000 \n",
      "Step: 475/865, accuracy0.141, loss3.420, learning rate 0.0030000 \n",
      "Step: 480/865, accuracy0.219, loss3.260, learning rate 0.0030000 \n",
      "Step: 485/865, accuracy0.219, loss3.318, learning rate 0.0030000 \n",
      "Step: 490/865, accuracy0.109, loss3.474, learning rate 0.0030000 \n",
      "Step: 495/865, accuracy0.109, loss3.443, learning rate 0.0030000 \n",
      "Step: 500/865, accuracy0.141, loss3.330, learning rate 0.0030000 \n",
      "Step: 505/865, accuracy0.125, loss3.389, learning rate 0.0030000 \n",
      "Step: 510/865, accuracy0.141, loss3.557, learning rate 0.0030000 \n",
      "Step: 515/865, accuracy0.094, loss3.596, learning rate 0.0030000 \n",
      "Step: 520/865, accuracy0.141, loss3.405, learning rate 0.0030000 \n",
      "Step: 525/865, accuracy0.156, loss3.331, learning rate 0.0030000 \n",
      "Step: 530/865, accuracy0.172, loss3.441, learning rate 0.0030000 \n",
      "Step: 535/865, accuracy0.156, loss3.430, learning rate 0.0030000 \n",
      "Step: 540/865, accuracy0.062, loss3.579, learning rate 0.0030000 \n",
      "Step: 545/865, accuracy0.109, loss3.587, learning rate 0.0030000 \n",
      "Step: 550/865, accuracy0.125, loss3.532, learning rate 0.0030000 \n",
      "Step: 555/865, accuracy0.156, loss3.351, learning rate 0.0030000 \n",
      "Step: 560/865, accuracy0.125, loss3.351, learning rate 0.0030000 \n",
      "Step: 565/865, accuracy0.141, loss3.382, learning rate 0.0030000 \n",
      "Step: 570/865, accuracy0.219, loss3.309, learning rate 0.0030000 \n",
      "Step: 575/865, accuracy0.156, loss3.487, learning rate 0.0030000 \n",
      "Step: 580/865, accuracy0.109, loss3.492, learning rate 0.0030000 \n",
      "Step: 585/865, accuracy0.172, loss3.303, learning rate 0.0030000 \n",
      "Step: 590/865, accuracy0.125, loss3.422, learning rate 0.0030000 \n",
      "Step: 595/865, accuracy0.109, loss3.508, learning rate 0.0030000 \n",
      "Step: 600/865, accuracy0.156, loss3.371, learning rate 0.0030000 \n",
      "Step: 605/865, accuracy0.219, loss3.179, learning rate 0.0030000 \n",
      "Step: 610/865, accuracy0.062, loss3.495, learning rate 0.0030000 \n",
      "Step: 615/865, accuracy0.125, loss3.332, learning rate 0.0030000 \n",
      "Step: 620/865, accuracy0.125, loss3.345, learning rate 0.0030000 \n",
      "Step: 625/865, accuracy0.156, loss3.299, learning rate 0.0030000 \n",
      "Step: 630/865, accuracy0.156, loss3.378, learning rate 0.0030000 \n",
      "Step: 635/865, accuracy0.172, loss3.404, learning rate 0.0030000 \n",
      "Step: 640/865, accuracy0.109, loss3.436, learning rate 0.0030000 \n",
      "Step: 645/865, accuracy0.141, loss3.385, learning rate 0.0030000 \n",
      "Step: 650/865, accuracy0.109, loss3.540, learning rate 0.0030000 \n",
      "Step: 655/865, accuracy0.125, loss3.489, learning rate 0.0030000 \n",
      "Step: 660/865, accuracy0.125, loss3.335, learning rate 0.0030000 \n",
      "Step: 665/865, accuracy0.078, loss3.447, learning rate 0.0030000 \n",
      "Step: 670/865, accuracy0.141, loss3.392, learning rate 0.0030000 \n",
      "Step: 675/865, accuracy0.125, loss3.382, learning rate 0.0030000 \n",
      "Step: 680/865, accuracy0.141, loss3.394, learning rate 0.0030000 \n",
      "Step: 685/865, accuracy0.188, loss3.266, learning rate 0.0030000 \n",
      "Step: 690/865, accuracy0.141, loss3.297, learning rate 0.0030000 \n",
      "Step: 695/865, accuracy0.094, loss3.602, learning rate 0.0030000 \n",
      "Step: 700/865, accuracy0.156, loss3.322, learning rate 0.0030000 \n",
      "Step: 705/865, accuracy0.094, loss3.422, learning rate 0.0030000 \n",
      "Step: 710/865, accuracy0.156, loss3.430, learning rate 0.0030000 \n",
      "Step: 715/865, accuracy0.094, loss3.562, learning rate 0.0030000 \n",
      "Step: 720/865, accuracy0.188, loss3.212, learning rate 0.0030000 \n",
      "Step: 725/865, accuracy0.125, loss3.435, learning rate 0.0030000 \n",
      "Step: 730/865, accuracy0.172, loss3.434, learning rate 0.0030000 \n",
      "Step: 735/865, accuracy0.141, loss3.560, learning rate 0.0030000 \n",
      "Step: 740/865, accuracy0.094, loss3.503, learning rate 0.0030000 \n",
      "Step: 745/865, accuracy0.141, loss3.355, learning rate 0.0030000 \n",
      "Step: 750/865, accuracy0.219, loss3.398, learning rate 0.0030000 \n",
      "Step: 755/865, accuracy0.172, loss3.370, learning rate 0.0030000 \n",
      "Step: 760/865, accuracy0.172, loss3.256, learning rate 0.0030000 \n",
      "Step: 765/865, accuracy0.062, loss3.548, learning rate 0.0030000 \n",
      "Step: 770/865, accuracy0.125, loss3.411, learning rate 0.0030000 \n",
      "Step: 775/865, accuracy0.188, loss3.424, learning rate 0.0030000 \n",
      "Step: 780/865, accuracy0.062, loss3.538, learning rate 0.0030000 \n",
      "Step: 785/865, accuracy0.109, loss3.436, learning rate 0.0030000 \n",
      "Step: 790/865, accuracy0.109, loss3.424, learning rate 0.0030000 \n",
      "Step: 795/865, accuracy0.172, loss3.312, learning rate 0.0030000 \n",
      "Step: 800/865, accuracy0.125, loss3.426, learning rate 0.0030000 \n",
      "Step: 805/865, accuracy0.141, loss3.399, learning rate 0.0030000 \n",
      "Step: 810/865, accuracy0.109, loss3.383, learning rate 0.0030000 \n",
      "Step: 815/865, accuracy0.203, loss3.171, learning rate 0.0030000 \n",
      "Step: 820/865, accuracy0.156, loss3.438, learning rate 0.0030000 \n",
      "Step: 825/865, accuracy0.109, loss3.415, learning rate 0.0030000 \n",
      "Step: 830/865, accuracy0.094, loss3.399, learning rate 0.0030000 \n",
      "Step: 835/865, accuracy0.219, loss3.165, learning rate 0.0030000 \n",
      "Step: 840/865, accuracy0.141, loss3.443, learning rate 0.0030000 \n",
      "Step: 845/865, accuracy0.141, loss3.349, learning rate 0.0030000 \n",
      "Step: 850/865, accuracy0.109, loss3.298, learning rate 0.0030000 \n",
      "Step: 855/865, accuracy0.062, loss3.454, learning rate 0.0030000 \n",
      "Step: 860/865, accuracy0.125, loss3.452, learning rate 0.0030000 \n",
      "Step: 864/865, accuracy0.129, loss3.228, learning rate 0.0030000 \n",
      "Epoch: 1/100, accuracy0.131, loss3.502, learning rate 0.003\n",
      "Estimated reamining runtime: 7 days, 17:23:21.853824\n",
      "--Validation--\n",
      "Validation : Accuracy: 0.124, Loss: 3.430\n",
      "=== Epoch: 2 ===\n",
      "Step: 0/865, accuracy0.141, loss3.352, learning rate 0.0030000 \n",
      "Step: 5/865, accuracy0.094, loss3.581, learning rate 0.0030000 \n",
      "Step: 10/865, accuracy0.109, loss3.372, learning rate 0.0030000 \n",
      "Step: 15/865, accuracy0.156, loss3.338, learning rate 0.0030000 \n",
      "Step: 20/865, accuracy0.172, loss3.326, learning rate 0.0030000 \n",
      "Step: 25/865, accuracy0.109, loss3.546, learning rate 0.0030000 \n",
      "Step: 30/865, accuracy0.141, loss3.392, learning rate 0.0030000 \n",
      "Step: 35/865, accuracy0.125, loss3.302, learning rate 0.0030000 \n",
      "Step: 40/865, accuracy0.078, loss3.472, learning rate 0.0030000 \n",
      "Step: 45/865, accuracy0.141, loss3.461, learning rate 0.0030000 \n",
      "Step: 50/865, accuracy0.188, loss3.267, learning rate 0.0030000 \n",
      "Step: 55/865, accuracy0.094, loss3.424, learning rate 0.0030000 \n",
      "Step: 60/865, accuracy0.125, loss3.415, learning rate 0.0030000 \n",
      "Step: 65/865, accuracy0.109, loss3.405, learning rate 0.0030000 \n",
      "Step: 70/865, accuracy0.219, loss3.222, learning rate 0.0030000 \n",
      "Step: 75/865, accuracy0.141, loss3.479, learning rate 0.0030000 \n",
      "Step: 80/865, accuracy0.172, loss3.546, learning rate 0.0030000 \n",
      "Step: 85/865, accuracy0.156, loss3.222, learning rate 0.0030000 \n",
      "Step: 90/865, accuracy0.141, loss3.361, learning rate 0.0030000 \n",
      "Step: 95/865, accuracy0.188, loss3.276, learning rate 0.0030000 \n",
      "Step: 100/865, accuracy0.094, loss3.378, learning rate 0.0030000 \n",
      "Step: 105/865, accuracy0.109, loss3.427, learning rate 0.0030000 \n",
      "Step: 110/865, accuracy0.125, loss3.437, learning rate 0.0030000 \n",
      "Step: 115/865, accuracy0.094, loss3.441, learning rate 0.0030000 \n",
      "Step: 120/865, accuracy0.109, loss3.410, learning rate 0.0030000 \n",
      "Step: 125/865, accuracy0.156, loss3.288, learning rate 0.0030000 \n",
      "Step: 130/865, accuracy0.094, loss3.373, learning rate 0.0030000 \n",
      "Step: 135/865, accuracy0.078, loss3.600, learning rate 0.0030000 \n",
      "Step: 140/865, accuracy0.125, loss3.442, learning rate 0.0030000 \n",
      "Step: 145/865, accuracy0.094, loss3.438, learning rate 0.0030000 \n",
      "Step: 150/865, accuracy0.188, loss3.388, learning rate 0.0030000 \n",
      "Step: 155/865, accuracy0.172, loss3.324, learning rate 0.0030000 \n",
      "Step: 160/865, accuracy0.172, loss3.255, learning rate 0.0030000 \n",
      "Step: 165/865, accuracy0.219, loss3.316, learning rate 0.0030000 \n",
      "Step: 170/865, accuracy0.172, loss3.236, learning rate 0.0030000 \n",
      "Step: 175/865, accuracy0.156, loss3.370, learning rate 0.0030000 \n",
      "Step: 180/865, accuracy0.094, loss3.556, learning rate 0.0030000 \n",
      "Step: 185/865, accuracy0.141, loss3.425, learning rate 0.0030000 \n",
      "Step: 190/865, accuracy0.094, loss3.460, learning rate 0.0030000 \n",
      "Step: 195/865, accuracy0.109, loss3.335, learning rate 0.0030000 \n",
      "Step: 200/865, accuracy0.156, loss3.345, learning rate 0.0030000 \n",
      "Step: 205/865, accuracy0.188, loss3.370, learning rate 0.0030000 \n",
      "Step: 210/865, accuracy0.172, loss3.445, learning rate 0.0030000 \n",
      "Step: 215/865, accuracy0.078, loss3.435, learning rate 0.0030000 \n",
      "Step: 220/865, accuracy0.078, loss3.440, learning rate 0.0030000 \n",
      "Step: 225/865, accuracy0.203, loss3.283, learning rate 0.0030000 \n",
      "Step: 230/865, accuracy0.094, loss3.395, learning rate 0.0030000 \n",
      "Step: 235/865, accuracy0.125, loss3.316, learning rate 0.0030000 \n",
      "Step: 240/865, accuracy0.203, loss3.151, learning rate 0.0030000 \n",
      "Step: 245/865, accuracy0.062, loss3.527, learning rate 0.0030000 \n",
      "Step: 250/865, accuracy0.141, loss3.257, learning rate 0.0030000 \n",
      "Step: 255/865, accuracy0.156, loss3.486, learning rate 0.0030000 \n",
      "Step: 260/865, accuracy0.141, loss3.444, learning rate 0.0030000 \n",
      "Step: 265/865, accuracy0.078, loss3.588, learning rate 0.0030000 \n",
      "Step: 270/865, accuracy0.188, loss3.301, learning rate 0.0030000 \n",
      "Step: 275/865, accuracy0.109, loss3.418, learning rate 0.0030000 \n",
      "Step: 280/865, accuracy0.094, loss3.498, learning rate 0.0030000 \n",
      "Step: 285/865, accuracy0.109, loss3.430, learning rate 0.0030000 \n",
      "Step: 290/865, accuracy0.094, loss3.417, learning rate 0.0030000 \n",
      "Step: 295/865, accuracy0.156, loss3.417, learning rate 0.0030000 \n",
      "Step: 300/865, accuracy0.172, loss3.241, learning rate 0.0030000 \n",
      "Step: 305/865, accuracy0.125, loss3.362, learning rate 0.0030000 \n",
      "Step: 310/865, accuracy0.094, loss3.570, learning rate 0.0030000 \n",
      "Step: 315/865, accuracy0.203, loss3.340, learning rate 0.0030000 \n",
      "Step: 320/865, accuracy0.156, loss3.351, learning rate 0.0030000 \n",
      "Step: 325/865, accuracy0.188, loss3.402, learning rate 0.0030000 \n",
      "Step: 330/865, accuracy0.141, loss3.540, learning rate 0.0030000 \n",
      "Step: 335/865, accuracy0.125, loss3.429, learning rate 0.0030000 \n",
      "Step: 340/865, accuracy0.016, loss3.684, learning rate 0.0030000 \n",
      "Step: 345/865, accuracy0.141, loss3.448, learning rate 0.0030000 \n",
      "Step: 350/865, accuracy0.141, loss3.471, learning rate 0.0030000 \n",
      "Step: 355/865, accuracy0.172, loss3.296, learning rate 0.0030000 \n",
      "Step: 360/865, accuracy0.094, loss3.460, learning rate 0.0030000 \n",
      "Step: 365/865, accuracy0.172, loss3.286, learning rate 0.0030000 \n",
      "Step: 370/865, accuracy0.203, loss3.315, learning rate 0.0030000 \n",
      "Step: 375/865, accuracy0.188, loss3.455, learning rate 0.0030000 \n",
      "Step: 380/865, accuracy0.172, loss3.240, learning rate 0.0030000 \n",
      "Step: 385/865, accuracy0.125, loss3.231, learning rate 0.0030000 \n",
      "Step: 390/865, accuracy0.031, loss3.614, learning rate 0.0030000 \n",
      "Step: 395/865, accuracy0.078, loss3.554, learning rate 0.0030000 \n",
      "Step: 400/865, accuracy0.078, loss3.618, learning rate 0.0030000 \n",
      "Step: 405/865, accuracy0.141, loss3.488, learning rate 0.0030000 \n",
      "Step: 410/865, accuracy0.125, loss3.430, learning rate 0.0030000 \n",
      "Step: 415/865, accuracy0.047, loss3.448, learning rate 0.0030000 \n",
      "Step: 420/865, accuracy0.094, loss3.528, learning rate 0.0030000 \n",
      "Step: 425/865, accuracy0.234, loss3.220, learning rate 0.0030000 \n",
      "Step: 430/865, accuracy0.188, loss3.219, learning rate 0.0030000 \n",
      "Step: 435/865, accuracy0.156, loss3.389, learning rate 0.0030000 \n",
      "Step: 440/865, accuracy0.078, loss3.516, learning rate 0.0030000 \n",
      "Step: 445/865, accuracy0.219, loss3.246, learning rate 0.0030000 \n",
      "Step: 450/865, accuracy0.109, loss3.455, learning rate 0.0030000 \n",
      "Step: 455/865, accuracy0.125, loss3.391, learning rate 0.0030000 \n",
      "Step: 460/865, accuracy0.125, loss3.541, learning rate 0.0030000 \n",
      "Step: 465/865, accuracy0.172, loss3.369, learning rate 0.0030000 \n",
      "Step: 470/865, accuracy0.109, loss3.396, learning rate 0.0030000 \n",
      "Step: 475/865, accuracy0.156, loss3.435, learning rate 0.0030000 \n",
      "Step: 480/865, accuracy0.109, loss3.400, learning rate 0.0030000 \n",
      "Step: 485/865, accuracy0.109, loss3.391, learning rate 0.0030000 \n",
      "Step: 490/865, accuracy0.047, loss3.650, learning rate 0.0030000 \n",
      "Step: 495/865, accuracy0.172, loss3.368, learning rate 0.0030000 \n",
      "Step: 500/865, accuracy0.188, loss3.403, learning rate 0.0030000 \n",
      "Step: 505/865, accuracy0.141, loss3.453, learning rate 0.0030000 \n",
      "Step: 510/865, accuracy0.156, loss3.273, learning rate 0.0030000 \n",
      "Step: 515/865, accuracy0.094, loss3.561, learning rate 0.0030000 \n",
      "Step: 520/865, accuracy0.188, loss3.263, learning rate 0.0030000 \n",
      "Step: 525/865, accuracy0.109, loss3.438, learning rate 0.0030000 \n",
      "Step: 530/865, accuracy0.078, loss3.446, learning rate 0.0030000 \n",
      "Step: 535/865, accuracy0.094, loss3.333, learning rate 0.0030000 \n",
      "Step: 540/865, accuracy0.172, loss3.326, learning rate 0.0030000 \n",
      "Step: 545/865, accuracy0.203, loss3.257, learning rate 0.0030000 \n",
      "Step: 550/865, accuracy0.156, loss3.269, learning rate 0.0030000 \n",
      "Step: 555/865, accuracy0.125, loss3.450, learning rate 0.0030000 \n",
      "Step: 560/865, accuracy0.234, loss3.249, learning rate 0.0030000 \n",
      "Step: 565/865, accuracy0.109, loss3.384, learning rate 0.0030000 \n",
      "Step: 570/865, accuracy0.156, loss3.268, learning rate 0.0030000 \n",
      "Step: 575/865, accuracy0.141, loss3.394, learning rate 0.0030000 \n",
      "Step: 580/865, accuracy0.094, loss3.541, learning rate 0.0030000 \n",
      "Step: 585/865, accuracy0.141, loss3.415, learning rate 0.0030000 \n",
      "Step: 590/865, accuracy0.109, loss3.380, learning rate 0.0030000 \n",
      "Step: 595/865, accuracy0.141, loss3.397, learning rate 0.0030000 \n",
      "Step: 600/865, accuracy0.109, loss3.515, learning rate 0.0030000 \n",
      "Step: 605/865, accuracy0.078, loss3.530, learning rate 0.0030000 \n",
      "Step: 610/865, accuracy0.062, loss3.715, learning rate 0.0030000 \n",
      "Step: 615/865, accuracy0.156, loss3.495, learning rate 0.0030000 \n",
      "Step: 620/865, accuracy0.047, loss3.585, learning rate 0.0030000 \n",
      "Step: 625/865, accuracy0.125, loss3.489, learning rate 0.0030000 \n",
      "Step: 630/865, accuracy0.156, loss3.284, learning rate 0.0030000 \n",
      "Step: 635/865, accuracy0.125, loss3.325, learning rate 0.0030000 \n",
      "Step: 640/865, accuracy0.266, loss3.249, learning rate 0.0030000 \n",
      "Step: 645/865, accuracy0.094, loss3.395, learning rate 0.0030000 \n",
      "Step: 650/865, accuracy0.125, loss3.406, learning rate 0.0030000 \n",
      "Step: 655/865, accuracy0.172, loss3.246, learning rate 0.0030000 \n",
      "Step: 660/865, accuracy0.094, loss3.546, learning rate 0.0030000 \n",
      "Step: 665/865, accuracy0.172, loss3.201, learning rate 0.0030000 \n",
      "Step: 670/865, accuracy0.078, loss3.550, learning rate 0.0030000 \n",
      "Step: 675/865, accuracy0.094, loss3.454, learning rate 0.0030000 \n",
      "Step: 680/865, accuracy0.203, loss3.360, learning rate 0.0030000 \n",
      "Step: 685/865, accuracy0.109, loss3.507, learning rate 0.0030000 \n",
      "Step: 690/865, accuracy0.125, loss3.443, learning rate 0.0030000 \n",
      "Step: 695/865, accuracy0.203, loss3.221, learning rate 0.0030000 \n",
      "Step: 700/865, accuracy0.172, loss3.355, learning rate 0.0030000 \n",
      "Step: 705/865, accuracy0.109, loss3.273, learning rate 0.0030000 \n",
      "Step: 710/865, accuracy0.078, loss3.462, learning rate 0.0030000 \n",
      "Step: 715/865, accuracy0.109, loss3.535, learning rate 0.0030000 \n",
      "Step: 720/865, accuracy0.141, loss3.438, learning rate 0.0030000 \n",
      "Step: 725/865, accuracy0.094, loss3.541, learning rate 0.0030000 \n",
      "Step: 730/865, accuracy0.156, loss3.411, learning rate 0.0030000 \n",
      "Step: 735/865, accuracy0.172, loss3.342, learning rate 0.0030000 \n",
      "Step: 740/865, accuracy0.109, loss3.501, learning rate 0.0030000 \n",
      "Step: 745/865, accuracy0.078, loss3.525, learning rate 0.0030000 \n",
      "Step: 750/865, accuracy0.109, loss3.423, learning rate 0.0030000 \n",
      "Step: 755/865, accuracy0.109, loss3.541, learning rate 0.0030000 \n",
      "Step: 760/865, accuracy0.078, loss3.553, learning rate 0.0030000 \n",
      "Step: 765/865, accuracy0.094, loss3.549, learning rate 0.0030000 \n",
      "Step: 770/865, accuracy0.156, loss3.375, learning rate 0.0030000 \n",
      "Step: 775/865, accuracy0.094, loss3.640, learning rate 0.0030000 \n",
      "Step: 780/865, accuracy0.234, loss3.211, learning rate 0.0030000 \n",
      "Step: 785/865, accuracy0.125, loss3.369, learning rate 0.0030000 \n",
      "Step: 790/865, accuracy0.219, loss2.992, learning rate 0.0030000 \n",
      "Step: 795/865, accuracy0.125, loss3.384, learning rate 0.0030000 \n",
      "Step: 800/865, accuracy0.109, loss3.447, learning rate 0.0030000 \n",
      "Step: 805/865, accuracy0.078, loss3.428, learning rate 0.0030000 \n",
      "Step: 810/865, accuracy0.125, loss3.456, learning rate 0.0030000 \n",
      "Step: 815/865, accuracy0.156, loss3.332, learning rate 0.0030000 \n",
      "Step: 820/865, accuracy0.172, loss3.271, learning rate 0.0030000 \n",
      "Step: 825/865, accuracy0.156, loss3.414, learning rate 0.0030000 \n",
      "Step: 830/865, accuracy0.188, loss3.450, learning rate 0.0030000 \n",
      "Step: 835/865, accuracy0.109, loss3.502, learning rate 0.0030000 \n",
      "Step: 840/865, accuracy0.109, loss3.464, learning rate 0.0030000 \n",
      "Step: 845/865, accuracy0.141, loss3.393, learning rate 0.0030000 \n",
      "Step: 850/865, accuracy0.172, loss3.371, learning rate 0.0030000 \n",
      "Step: 855/865, accuracy0.109, loss3.495, learning rate 0.0030000 \n",
      "Step: 860/865, accuracy0.156, loss3.327, learning rate 0.0030000 \n",
      "Step: 864/865, accuracy0.226, loss3.301, learning rate 0.0030000 \n",
      "Epoch: 2/100, accuracy0.129, loss3.421, learning rate 0.003\n",
      "Estimated reamining runtime: 6 days, 23:42:33.157997\n",
      "--Validation--\n",
      "Validation : Accuracy: 0.120, Loss: 3.432\n",
      "=== Epoch: 3 ===\n",
      "Step: 0/865, accuracy0.188, loss3.394, learning rate 0.0030000 \n",
      "Step: 5/865, accuracy0.156, loss3.328, learning rate 0.0030000 \n",
      "Step: 10/865, accuracy0.125, loss3.456, learning rate 0.0030000 \n",
      "Step: 15/865, accuracy0.188, loss3.345, learning rate 0.0030000 \n",
      "Step: 20/865, accuracy0.094, loss3.516, learning rate 0.0030000 \n",
      "Step: 25/865, accuracy0.156, loss3.503, learning rate 0.0030000 \n",
      "Step: 30/865, accuracy0.125, loss3.497, learning rate 0.0030000 \n",
      "Step: 35/865, accuracy0.094, loss3.493, learning rate 0.0030000 \n",
      "Step: 40/865, accuracy0.219, loss3.151, learning rate 0.0030000 \n",
      "Step: 45/865, accuracy0.172, loss3.401, learning rate 0.0030000 \n",
      "Step: 50/865, accuracy0.156, loss3.487, learning rate 0.0030000 \n",
      "Step: 55/865, accuracy0.125, loss3.293, learning rate 0.0030000 \n",
      "Step: 60/865, accuracy0.125, loss3.414, learning rate 0.0030000 \n",
      "Step: 65/865, accuracy0.125, loss3.561, learning rate 0.0030000 \n",
      "Step: 70/865, accuracy0.203, loss3.159, learning rate 0.0030000 \n",
      "Step: 75/865, accuracy0.125, loss3.380, learning rate 0.0030000 \n",
      "Step: 80/865, accuracy0.094, loss3.576, learning rate 0.0030000 \n",
      "Step: 85/865, accuracy0.141, loss3.384, learning rate 0.0030000 \n",
      "Step: 90/865, accuracy0.125, loss3.464, learning rate 0.0030000 \n",
      "Step: 95/865, accuracy0.078, loss3.579, learning rate 0.0030000 \n",
      "Step: 100/865, accuracy0.094, loss3.498, learning rate 0.0030000 \n",
      "Step: 105/865, accuracy0.156, loss3.279, learning rate 0.0030000 \n",
      "Step: 110/865, accuracy0.156, loss3.388, learning rate 0.0030000 \n",
      "Step: 115/865, accuracy0.172, loss3.411, learning rate 0.0030000 \n",
      "Step: 120/865, accuracy0.125, loss3.467, learning rate 0.0030000 \n",
      "Step: 125/865, accuracy0.156, loss3.354, learning rate 0.0030000 \n",
      "Step: 130/865, accuracy0.156, loss3.434, learning rate 0.0030000 \n",
      "Step: 135/865, accuracy0.125, loss3.478, learning rate 0.0030000 \n",
      "Step: 140/865, accuracy0.141, loss3.338, learning rate 0.0030000 \n",
      "Step: 145/865, accuracy0.094, loss3.446, learning rate 0.0030000 \n",
      "Step: 150/865, accuracy0.188, loss3.322, learning rate 0.0030000 \n",
      "Step: 155/865, accuracy0.078, loss3.532, learning rate 0.0030000 \n",
      "Step: 160/865, accuracy0.062, loss3.655, learning rate 0.0030000 \n",
      "Step: 165/865, accuracy0.172, loss3.296, learning rate 0.0030000 \n",
      "Step: 170/865, accuracy0.172, loss3.259, learning rate 0.0030000 \n",
      "Step: 175/865, accuracy0.141, loss3.456, learning rate 0.0030000 \n",
      "Step: 180/865, accuracy0.141, loss3.448, learning rate 0.0030000 \n",
      "Step: 185/865, accuracy0.109, loss3.469, learning rate 0.0030000 \n",
      "Step: 190/865, accuracy0.172, loss3.376, learning rate 0.0030000 \n",
      "Step: 195/865, accuracy0.156, loss3.441, learning rate 0.0030000 \n",
      "Step: 200/865, accuracy0.109, loss3.498, learning rate 0.0030000 \n",
      "Step: 205/865, accuracy0.141, loss3.457, learning rate 0.0030000 \n",
      "Step: 210/865, accuracy0.125, loss3.586, learning rate 0.0030000 \n",
      "Step: 215/865, accuracy0.203, loss3.226, learning rate 0.0030000 \n",
      "Step: 220/865, accuracy0.109, loss3.494, learning rate 0.0030000 \n",
      "Step: 225/865, accuracy0.172, loss3.458, learning rate 0.0030000 \n",
      "Step: 230/865, accuracy0.078, loss3.500, learning rate 0.0030000 \n",
      "Step: 235/865, accuracy0.156, loss3.342, learning rate 0.0030000 \n",
      "Step: 240/865, accuracy0.156, loss3.314, learning rate 0.0030000 \n",
      "Step: 245/865, accuracy0.172, loss3.286, learning rate 0.0030000 \n",
      "Step: 250/865, accuracy0.188, loss3.419, learning rate 0.0030000 \n",
      "Step: 255/865, accuracy0.172, loss3.356, learning rate 0.0030000 \n",
      "Step: 260/865, accuracy0.203, loss3.391, learning rate 0.0030000 \n",
      "Step: 265/865, accuracy0.188, loss3.352, learning rate 0.0030000 \n",
      "Step: 270/865, accuracy0.125, loss3.430, learning rate 0.0030000 \n",
      "Step: 275/865, accuracy0.094, loss3.402, learning rate 0.0030000 \n",
      "Step: 280/865, accuracy0.094, loss3.628, learning rate 0.0030000 \n",
      "Step: 285/865, accuracy0.188, loss3.362, learning rate 0.0030000 \n",
      "Step: 290/865, accuracy0.125, loss3.483, learning rate 0.0030000 \n",
      "Step: 295/865, accuracy0.125, loss3.394, learning rate 0.0030000 \n",
      "Step: 300/865, accuracy0.141, loss3.410, learning rate 0.0030000 \n",
      "Step: 305/865, accuracy0.062, loss3.671, learning rate 0.0030000 \n",
      "Step: 310/865, accuracy0.156, loss3.330, learning rate 0.0030000 \n",
      "Step: 315/865, accuracy0.109, loss3.375, learning rate 0.0030000 \n",
      "Step: 320/865, accuracy0.125, loss3.575, learning rate 0.0030000 \n",
      "Step: 325/865, accuracy0.141, loss3.641, learning rate 0.0030000 \n",
      "Step: 330/865, accuracy0.109, loss3.544, learning rate 0.0030000 \n",
      "Step: 335/865, accuracy0.156, loss3.403, learning rate 0.0030000 \n",
      "Step: 340/865, accuracy0.141, loss3.296, learning rate 0.0030000 \n",
      "Step: 345/865, accuracy0.094, loss3.457, learning rate 0.0030000 \n",
      "Step: 350/865, accuracy0.109, loss3.393, learning rate 0.0030000 \n",
      "Step: 355/865, accuracy0.125, loss3.384, learning rate 0.0030000 \n",
      "Step: 360/865, accuracy0.078, loss3.408, learning rate 0.0030000 \n",
      "Step: 365/865, accuracy0.094, loss3.517, learning rate 0.0030000 \n",
      "Step: 370/865, accuracy0.062, loss3.563, learning rate 0.0030000 \n",
      "Step: 375/865, accuracy0.141, loss3.430, learning rate 0.0030000 \n",
      "Step: 380/865, accuracy0.094, loss3.458, learning rate 0.0030000 \n",
      "Step: 385/865, accuracy0.078, loss3.417, learning rate 0.0030000 \n",
      "Step: 390/865, accuracy0.125, loss3.413, learning rate 0.0030000 \n",
      "Step: 395/865, accuracy0.078, loss3.478, learning rate 0.0030000 \n",
      "Step: 400/865, accuracy0.125, loss3.474, learning rate 0.0030000 \n",
      "Step: 405/865, accuracy0.172, loss3.471, learning rate 0.0030000 \n",
      "Step: 410/865, accuracy0.078, loss3.512, learning rate 0.0030000 \n",
      "Step: 415/865, accuracy0.078, loss3.506, learning rate 0.0030000 \n",
      "Step: 420/865, accuracy0.094, loss3.568, learning rate 0.0030000 \n",
      "Step: 425/865, accuracy0.203, loss3.262, learning rate 0.0030000 \n",
      "Step: 430/865, accuracy0.156, loss3.352, learning rate 0.0030000 \n",
      "Step: 435/865, accuracy0.203, loss3.178, learning rate 0.0030000 \n",
      "Step: 440/865, accuracy0.109, loss3.396, learning rate 0.0030000 \n",
      "Step: 445/865, accuracy0.172, loss3.265, learning rate 0.0030000 \n",
      "Step: 450/865, accuracy0.109, loss3.436, learning rate 0.0030000 \n",
      "Step: 455/865, accuracy0.125, loss3.475, learning rate 0.0030000 \n",
      "Step: 460/865, accuracy0.156, loss3.413, learning rate 0.0030000 \n",
      "Step: 465/865, accuracy0.156, loss3.321, learning rate 0.0030000 \n",
      "Step: 470/865, accuracy0.094, loss3.454, learning rate 0.0030000 \n",
      "Step: 475/865, accuracy0.062, loss3.503, learning rate 0.0030000 \n",
      "Step: 480/865, accuracy0.109, loss3.512, learning rate 0.0030000 \n",
      "Step: 485/865, accuracy0.109, loss3.400, learning rate 0.0030000 \n",
      "Step: 490/865, accuracy0.141, loss3.434, learning rate 0.0030000 \n",
      "Step: 495/865, accuracy0.062, loss3.540, learning rate 0.0030000 \n",
      "Step: 500/865, accuracy0.156, loss3.372, learning rate 0.0030000 \n",
      "Step: 505/865, accuracy0.172, loss3.310, learning rate 0.0030000 \n",
      "Step: 510/865, accuracy0.141, loss3.342, learning rate 0.0030000 \n",
      "Step: 515/865, accuracy0.156, loss3.423, learning rate 0.0030000 \n",
      "Step: 520/865, accuracy0.109, loss3.421, learning rate 0.0030000 \n",
      "Step: 525/865, accuracy0.078, loss3.490, learning rate 0.0030000 \n",
      "Step: 530/865, accuracy0.094, loss3.574, learning rate 0.0030000 \n",
      "Step: 535/865, accuracy0.062, loss3.617, learning rate 0.0030000 \n",
      "Step: 540/865, accuracy0.078, loss3.320, learning rate 0.0030000 \n",
      "Step: 545/865, accuracy0.062, loss3.523, learning rate 0.0030000 \n",
      "Step: 550/865, accuracy0.156, loss3.400, learning rate 0.0030000 \n",
      "Step: 555/865, accuracy0.156, loss3.341, learning rate 0.0030000 \n",
      "Step: 560/865, accuracy0.125, loss3.378, learning rate 0.0030000 \n",
      "Step: 565/865, accuracy0.141, loss3.416, learning rate 0.0030000 \n",
      "Step: 570/865, accuracy0.125, loss3.510, learning rate 0.0030000 \n",
      "Step: 575/865, accuracy0.188, loss3.333, learning rate 0.0030000 \n",
      "Step: 580/865, accuracy0.047, loss3.525, learning rate 0.0030000 \n",
      "Step: 585/865, accuracy0.141, loss3.450, learning rate 0.0030000 \n",
      "Step: 590/865, accuracy0.094, loss3.362, learning rate 0.0030000 \n",
      "Step: 595/865, accuracy0.141, loss3.286, learning rate 0.0030000 \n",
      "Step: 600/865, accuracy0.141, loss3.410, learning rate 0.0030000 \n",
      "Step: 605/865, accuracy0.141, loss3.521, learning rate 0.0030000 \n",
      "Step: 610/865, accuracy0.094, loss3.397, learning rate 0.0030000 \n",
      "Step: 615/865, accuracy0.109, loss3.509, learning rate 0.0030000 \n",
      "Step: 620/865, accuracy0.125, loss3.516, learning rate 0.0030000 \n",
      "Step: 625/865, accuracy0.141, loss3.373, learning rate 0.0030000 \n",
      "Step: 630/865, accuracy0.078, loss3.451, learning rate 0.0030000 \n",
      "Step: 635/865, accuracy0.094, loss3.426, learning rate 0.0030000 \n",
      "Step: 640/865, accuracy0.094, loss3.553, learning rate 0.0030000 \n",
      "Step: 645/865, accuracy0.109, loss3.500, learning rate 0.0030000 \n",
      "Step: 650/865, accuracy0.078, loss3.570, learning rate 0.0030000 \n",
      "Step: 655/865, accuracy0.203, loss3.304, learning rate 0.0030000 \n",
      "Step: 660/865, accuracy0.109, loss3.378, learning rate 0.0030000 \n",
      "Step: 665/865, accuracy0.109, loss3.518, learning rate 0.0030000 \n",
      "Step: 670/865, accuracy0.172, loss3.282, learning rate 0.0030000 \n",
      "Step: 675/865, accuracy0.172, loss3.430, learning rate 0.0030000 \n",
      "Step: 680/865, accuracy0.125, loss3.417, learning rate 0.0030000 \n",
      "Step: 685/865, accuracy0.094, loss3.471, learning rate 0.0030000 \n",
      "Step: 690/865, accuracy0.125, loss3.557, learning rate 0.0030000 \n",
      "Step: 695/865, accuracy0.078, loss3.396, learning rate 0.0030000 \n",
      "Step: 700/865, accuracy0.141, loss3.383, learning rate 0.0030000 \n",
      "Step: 705/865, accuracy0.188, loss3.288, learning rate 0.0030000 \n",
      "Step: 710/865, accuracy0.125, loss3.404, learning rate 0.0030000 \n",
      "Step: 715/865, accuracy0.094, loss3.404, learning rate 0.0030000 \n",
      "Step: 720/865, accuracy0.047, loss3.609, learning rate 0.0030000 \n",
      "Step: 725/865, accuracy0.141, loss3.294, learning rate 0.0030000 \n",
      "Step: 730/865, accuracy0.094, loss3.553, learning rate 0.0030000 \n",
      "Step: 735/865, accuracy0.078, loss3.415, learning rate 0.0030000 \n",
      "Step: 740/865, accuracy0.188, loss3.222, learning rate 0.0030000 \n",
      "Step: 745/865, accuracy0.141, loss3.397, learning rate 0.0030000 \n",
      "Step: 750/865, accuracy0.109, loss3.501, learning rate 0.0030000 \n",
      "Step: 755/865, accuracy0.141, loss3.438, learning rate 0.0030000 \n",
      "Step: 760/865, accuracy0.109, loss3.452, learning rate 0.0030000 \n",
      "Step: 765/865, accuracy0.094, loss3.509, learning rate 0.0030000 \n",
      "Step: 770/865, accuracy0.156, loss3.204, learning rate 0.0030000 \n",
      "Step: 775/865, accuracy0.078, loss3.584, learning rate 0.0030000 \n",
      "Step: 780/865, accuracy0.156, loss3.295, learning rate 0.0030000 \n",
      "Step: 785/865, accuracy0.219, loss3.115, learning rate 0.0030000 \n",
      "Step: 790/865, accuracy0.188, loss3.483, learning rate 0.0030000 \n",
      "Step: 795/865, accuracy0.141, loss3.457, learning rate 0.0030000 \n",
      "Step: 800/865, accuracy0.062, loss3.557, learning rate 0.0030000 \n",
      "Step: 805/865, accuracy0.125, loss3.472, learning rate 0.0030000 \n",
      "Step: 810/865, accuracy0.141, loss3.385, learning rate 0.0030000 \n",
      "Step: 815/865, accuracy0.219, loss3.222, learning rate 0.0030000 \n",
      "Step: 820/865, accuracy0.156, loss3.419, learning rate 0.0030000 \n",
      "Step: 825/865, accuracy0.203, loss3.363, learning rate 0.0030000 \n",
      "Step: 830/865, accuracy0.109, loss3.484, learning rate 0.0030000 \n",
      "Step: 835/865, accuracy0.125, loss3.382, learning rate 0.0030000 \n",
      "Step: 840/865, accuracy0.188, loss3.348, learning rate 0.0030000 \n",
      "Step: 845/865, accuracy0.062, loss3.584, learning rate 0.0030000 \n",
      "Step: 850/865, accuracy0.109, loss3.479, learning rate 0.0030000 \n",
      "Step: 855/865, accuracy0.172, loss3.425, learning rate 0.0030000 \n",
      "Step: 860/865, accuracy0.125, loss3.410, learning rate 0.0030000 \n",
      "Step: 864/865, accuracy0.097, loss3.175, learning rate 0.0030000 \n",
      "Epoch: 3/100, accuracy0.127, loss3.424, learning rate 0.003\n",
      "Estimated reamining runtime: 6 days, 17:20:18.195852\n",
      "--Validation--\n",
      "Validation : Accuracy: 0.118, Loss: 3.436\n",
      "=== Epoch: 4 ===\n",
      "Step: 0/865, accuracy0.156, loss3.308, learning rate 0.0030000 \n",
      "Step: 5/865, accuracy0.203, loss3.270, learning rate 0.0030000 \n",
      "Step: 10/865, accuracy0.125, loss3.484, learning rate 0.0030000 \n",
      "Step: 15/865, accuracy0.078, loss3.507, learning rate 0.0030000 \n",
      "Step: 20/865, accuracy0.203, loss3.306, learning rate 0.0030000 \n",
      "Step: 25/865, accuracy0.125, loss3.515, learning rate 0.0030000 \n",
      "Step: 30/865, accuracy0.094, loss3.483, learning rate 0.0030000 \n",
      "Step: 35/865, accuracy0.062, loss3.540, learning rate 0.0030000 \n",
      "Step: 40/865, accuracy0.172, loss3.222, learning rate 0.0030000 \n",
      "Step: 45/865, accuracy0.125, loss3.394, learning rate 0.0030000 \n",
      "Step: 50/865, accuracy0.172, loss3.309, learning rate 0.0030000 \n",
      "Step: 55/865, accuracy0.125, loss3.357, learning rate 0.0030000 \n",
      "Step: 60/865, accuracy0.062, loss3.582, learning rate 0.0030000 \n",
      "Step: 65/865, accuracy0.109, loss3.336, learning rate 0.0030000 \n",
      "Step: 70/865, accuracy0.125, loss3.457, learning rate 0.0030000 \n",
      "Step: 75/865, accuracy0.109, loss3.469, learning rate 0.0030000 \n",
      "Step: 80/865, accuracy0.156, loss3.427, learning rate 0.0030000 \n",
      "Step: 85/865, accuracy0.172, loss3.281, learning rate 0.0030000 \n",
      "Step: 90/865, accuracy0.125, loss3.322, learning rate 0.0030000 \n",
      "Step: 95/865, accuracy0.109, loss3.503, learning rate 0.0030000 \n",
      "Step: 100/865, accuracy0.188, loss3.368, learning rate 0.0030000 \n",
      "Step: 105/865, accuracy0.125, loss3.290, learning rate 0.0030000 \n",
      "Step: 110/865, accuracy0.109, loss3.406, learning rate 0.0030000 \n",
      "Step: 115/865, accuracy0.109, loss3.411, learning rate 0.0030000 \n",
      "Step: 120/865, accuracy0.078, loss3.462, learning rate 0.0030000 \n",
      "Step: 125/865, accuracy0.078, loss3.469, learning rate 0.0030000 \n",
      "Step: 130/865, accuracy0.125, loss3.563, learning rate 0.0030000 \n",
      "Step: 135/865, accuracy0.078, loss3.630, learning rate 0.0030000 \n",
      "Step: 140/865, accuracy0.188, loss3.299, learning rate 0.0030000 \n",
      "Step: 145/865, accuracy0.141, loss3.485, learning rate 0.0030000 \n",
      "Step: 150/865, accuracy0.109, loss3.505, learning rate 0.0030000 \n",
      "Step: 155/865, accuracy0.125, loss3.426, learning rate 0.0030000 \n",
      "Step: 160/865, accuracy0.188, loss3.397, learning rate 0.0030000 \n",
      "Step: 165/865, accuracy0.109, loss3.352, learning rate 0.0030000 \n",
      "Step: 170/865, accuracy0.109, loss3.465, learning rate 0.0030000 \n",
      "Step: 175/865, accuracy0.109, loss3.442, learning rate 0.0030000 \n",
      "Step: 180/865, accuracy0.109, loss3.477, learning rate 0.0030000 \n",
      "Step: 185/865, accuracy0.141, loss3.319, learning rate 0.0030000 \n",
      "Step: 190/865, accuracy0.078, loss3.397, learning rate 0.0030000 \n",
      "Step: 195/865, accuracy0.141, loss3.408, learning rate 0.0030000 \n",
      "Step: 200/865, accuracy0.062, loss3.462, learning rate 0.0030000 \n",
      "Step: 205/865, accuracy0.109, loss3.606, learning rate 0.0030000 \n",
      "Step: 210/865, accuracy0.141, loss3.386, learning rate 0.0030000 \n",
      "Step: 215/865, accuracy0.188, loss3.226, learning rate 0.0030000 \n",
      "Step: 220/865, accuracy0.094, loss3.409, learning rate 0.0030000 \n",
      "Step: 225/865, accuracy0.141, loss3.411, learning rate 0.0030000 \n",
      "Step: 230/865, accuracy0.156, loss3.402, learning rate 0.0030000 \n",
      "Step: 235/865, accuracy0.141, loss3.347, learning rate 0.0030000 \n",
      "Step: 240/865, accuracy0.141, loss3.279, learning rate 0.0030000 \n",
      "Step: 245/865, accuracy0.188, loss3.347, learning rate 0.0030000 \n",
      "Step: 250/865, accuracy0.125, loss3.423, learning rate 0.0030000 \n",
      "Step: 255/865, accuracy0.094, loss3.493, learning rate 0.0030000 \n",
      "Step: 260/865, accuracy0.062, loss3.420, learning rate 0.0030000 \n",
      "Step: 265/865, accuracy0.078, loss3.635, learning rate 0.0030000 \n",
      "Step: 270/865, accuracy0.094, loss3.449, learning rate 0.0030000 \n",
      "Step: 275/865, accuracy0.078, loss3.594, learning rate 0.0030000 \n",
      "Step: 280/865, accuracy0.109, loss3.422, learning rate 0.0030000 \n",
      "Step: 285/865, accuracy0.109, loss3.514, learning rate 0.0030000 \n",
      "Step: 290/865, accuracy0.109, loss3.471, learning rate 0.0030000 \n",
      "Step: 295/865, accuracy0.156, loss3.362, learning rate 0.0030000 \n",
      "Step: 300/865, accuracy0.125, loss3.330, learning rate 0.0030000 \n",
      "Step: 305/865, accuracy0.125, loss3.320, learning rate 0.0030000 \n",
      "Step: 310/865, accuracy0.141, loss3.276, learning rate 0.0030000 \n",
      "Step: 315/865, accuracy0.078, loss3.449, learning rate 0.0030000 \n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"Plant Leaf Disease\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param('framework', 'homebrew')\n",
    "    mlflow.log_param('data_split', '90/10')\n",
    "    mlflow.log_param('type', 'Basic_CNN')\n",
    "    mlflow.log_params(config)\n",
    "    mdl.train_with_loader(train_loader, epochs=config['max_epochs'], validation_loader=validation_loader, cls_count=39, log_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae5f745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 7200)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl.layers[3].output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649471e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
