{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ad873e",
   "metadata": {},
   "source": [
    "# Dev notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef02f46",
   "metadata": {},
   "source": [
    "I use this notebook to develop my implementation of a simple and minimal neural network framework.\n",
    "\n",
    "Inspiration for this network is drawn from the book [Neural Networks from Scratch (NNFS)](https://nnfs.io/) & [Pytorch's implementation](https://pytorch.org/)\n",
    "\n",
    "Notes:\n",
    "- The work on experimenting with backpropagation has been moved to another notebook to keep this one minimal\n",
    "- I have chosen to use a similar naming convention to that used by pytorch (why reinvent the wheel), this has the benefit of ensuring that when we compare implementations the architecture of the networks is the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab9fbb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import for dev and testing\n",
    "import nnfs \n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466662af",
   "metadata": {},
   "source": [
    "## Defining Base Module\n",
    "\n",
    "This is a module that contains all the base attributes and function needed by every class in the framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52091eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(ABC):\n",
    "    \"\"\"Base class for all classes in frame work to ensure the same attributes and common function names.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        # Attributes to hold input and outputs\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85db83e2",
   "metadata": {},
   "source": [
    "##  Defining layers \n",
    "\n",
    "In this section We define and test the layers\n",
    "\n",
    "Notes:\n",
    "- since we intend to use ReLU as one of our activation functions we will use the He weight initialization method as described in https://arxiv.org/abs/1502.01852"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7fe0c9",
   "metadata": {},
   "source": [
    "### Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48b830b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(Module):\n",
    "    \"\"\"Linear transformation layer of the type o = ixW + b,\n",
    "    \n",
    "    where I is the incoming vector, W is the layers weight matrix, b is bias vector and o is the dot product of the \n",
    "    i and W plus the bias\n",
    "    \n",
    "    Args:\n",
    "        in_features (int): the size of the input features \n",
    "        out_features (int): the size of the output features\n",
    "        \n",
    "    Attributes:\n",
    "        weights (np_array) numpy array of in_features x n_neurons\n",
    "        biases  (np_array) numpy array of 1 x n_neurons\n",
    "        inputs  (np_array) numpy array of latest batch of inputs\n",
    "        inputs  (np_array) numpy array of latest batch of outputs\n",
    "        d_w     (np_array) The current gradients with respect to the weights \n",
    "        d_x     (np_array) The current gradients with respect to the inputs\n",
    "        d_b     (np_array) The current gradients with respect to the biases\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features) -> None:\n",
    "        super().__init__()\n",
    "        # initializing weights and biases \n",
    "        #self.weights = np.random.normal(0.0, np.sqrt(2/in_features), (in_features, out_features))\n",
    "        # Using a simpler initialization  for testing \n",
    "        self.weights = 0.01 * np.random.randn(in_features, out_features)\n",
    "        self.bias = np.zeros((1, out_features))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Saving inputs for backward step\n",
    "        self.input = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_vals):\n",
    "        \"\"\"Backpropagation  of the linear function\n",
    "\n",
    "        Args:\n",
    "            d_vals (np_array) array of derivatives from the previous layer/function.\n",
    "        \"\"\"\n",
    "        self.d_w = np.dot(self.input.T, d_vals)\n",
    "        self.d_x = np.dot(d_vals, self.weights.T)\n",
    "        self.d_b = np.sum(d_vals, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ea363f",
   "metadata": {},
   "source": [
    "#### Testing Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1ca4160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [-1.0475188e-04  1.1395361e-04 -4.7983500e-05]\n",
      " [-2.7414842e-04  3.1729150e-04 -8.6921798e-05]\n",
      " [-4.2188365e-04  5.2666257e-04 -5.5912682e-05]\n",
      " [-5.7707680e-04  7.1401405e-04 -8.9430439e-05]\n",
      " [-3.5430698e-04  3.5025488e-04 -2.3363481e-04]\n",
      " [-8.9267001e-04  1.0767876e-03 -1.9453237e-04]\n",
      " [-9.3350781e-04  1.0723802e-03 -3.1227397e-04]\n",
      " [-1.1243758e-03  1.3112801e-03 -3.3629674e-04]\n",
      " [-1.3386955e-03  1.6200906e-03 -2.8101794e-04]]\n"
     ]
    }
   ],
   "source": [
    "# The sample data is a list of coordinate, ie two points \n",
    "# The layer therefore will take 2 inputs \n",
    "# We have given it 3 out features (3 neurons) so we expect to see an output with the shape (n_samples*n_neurons, n_neurons)\n",
    "# In out case that should be (300, 3) \n",
    "X, _ = spiral_data(samples=100, classes=3)\n",
    "linear1 = LinearLayer(2, 3)\n",
    "output = linear1.forward(X)\n",
    "print(output[:10, :])\n",
    "\n",
    "assert output.shape == (300,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2017ad64",
   "metadata": {},
   "source": [
    "## Defining Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8da5f5",
   "metadata": {},
   "source": [
    "### ReLu\n",
    "\n",
    "$$y = \\begin{cases}\n",
    "   x &x> 0 \\\\\n",
    "   0 & otherwise\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a03d0ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\"Applies Rectified linear Unit function to vector.\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        # initializing attributes needed for backwards \n",
    "        super().__init__()\n",
    "        self.d_relu = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # storing inputs needed for backwards \n",
    "        self.inputs = x\n",
    "        self.output = np.maximum(x, 0)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, d_vals):\n",
    "        self.d_relu = d_vals.copy()\n",
    "        self.d_relu[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa60fd7a",
   "metadata": {},
   "source": [
    "#### Testing ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bea142e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 3. , 4. , 0. , 0.1, 0. ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = [-2, 3, 4, 0, 0.1, -44]\n",
    "test_relu = ReLU()\n",
    "\n",
    "# Checking values are as expected \n",
    "assert np.all(np.array([0., 3, 4, 0, 0.1, 0.]) == test_relu.forward(i))\n",
    "\n",
    "test_relu.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc75ac1",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "$$\\text{softmax}(x)_i = \\frac{exp(x_i)}{\\sum_{j}^{ }exp(x_j))}$$\n",
    "\n",
    "The soft max represents the confidence score for each output class and adds up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "966f1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Module):\n",
    "    \"\"\"Applies Softmax function to input matrix.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.confidence_scores = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # exponenets of each value\n",
    "        exp_vals = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        exp_sum = np.sum(exp_vals, axis=1, keepdims=True)\n",
    "        # Normalization to get the proabilities \n",
    "        self.output = exp_vals/exp_sum\n",
    "        return self.output\n",
    "\n",
    "    def _backward(self, d_vals):\n",
    "        # Initialize array for gradients wrt to inputs\n",
    "        self.d_soft = np.zeros_like(d_vals)\n",
    "        \n",
    "        _iter = enumerate(zip(self.output, d_vals))\n",
    "        for i, conf_score, d_val in _iter:\n",
    "            # Flatten confidence scores\n",
    "            cs = conf_score.reshape(-1, 1)\n",
    "            # Find the Jacobian matrix of the output \n",
    "            j_matrix = np.diagflat(cs) - np.dot(cs, cs.T)\n",
    "            # get the gradient \n",
    "            self.d_soft[i] = np.dot(j_matrix, d_val)\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"Does a the combined backward pass for CCE & Softmax as a single, faster step.\"\"\"\n",
    "        # Number of examples in the batch\n",
    "        n = len(y_pred)\n",
    "\n",
    "        # Getting descrete vals from one hot encoding \n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "        \n",
    "        self.d_soft = y_pred.copy()\n",
    "        self.d_soft[range(n), y_true] -= 1\n",
    "        self.d_soft = self.d_soft / n\n",
    "        return self.d_soft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e8095e",
   "metadata": {},
   "source": [
    "#### Testing Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5cd2c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.11513104e-19, 5.74952226e-19, 1.00000000e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = Softmax()\n",
    "softmax.forward([[1,2,44]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b31bb",
   "metadata": {},
   "source": [
    "## Defining Loss - Categorical Cross-Entropy\n",
    "\n",
    "$$ L_i = -\\sum_j y_{i,j}\\log(\\hat{y}_{i,j}) $$\n",
    "\n",
    "With taking one hot encoding into account we can simplify this down to:\n",
    "\n",
    "$$ L_i = -y_{i,k}\\log(\\hat{y}_{i,k}) $$\n",
    "\n",
    "where K is the index of the correct class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7645a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalCrossEntropyLoss:\n",
    "    \"\"\"Calculates the CCE loss for a given set of predictions.\n",
    "    This method expect a softmax output and one-hot encoded label mask\n",
    "    \n",
    "    y_pred (np_array): matrix of confidence scores of the prediction\n",
    "    y_true (np_array): matrix of one-hot encoded true lables of the classes\n",
    "    \"\"\"\n",
    "    def forward(y_pred, y_true):\n",
    "        # Clipping and applying one hot encoded labels as mask \n",
    "        # to zero out scores corresponding to incorrect classes\n",
    "        # We clip to make sure that none of the reaming classes are 0 or \n",
    "        # exactly 1 \n",
    "        clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        corrected = np.sum(clipped*y_true, axis=1)\n",
    "        # Taking the -ve log of the remaining confidence scores \n",
    "        negative_log = -np.log(corrected)\n",
    "        return np.mean(negative_log)\n",
    "\n",
    "    def backward(y_pred, y_true):\n",
    "        \"\"\"Backpropagation  of the CCE Loss\n",
    "\n",
    "        Args:\n",
    "            y_pred (np_array) array of predictions.\n",
    "            y_true (np_array) array of correct labels.\n",
    "        \"\"\"\n",
    "        return (-y_true/y_pred)/len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6704da26",
   "metadata": {},
   "source": [
    "#### Testing CCE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b0abd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38506088005216804"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.array([[0.7, 0.1, 0.2], [0.1,0.5,0.4],[0.02,0.9,0.08]])\n",
    "y_true = np.array([[1,0,0], [0,1,0], [0,1,0]])\n",
    "\n",
    "loss_function = CategoricalCrossEntropyLoss\n",
    "loss_function.forward(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b0d6f",
   "metadata": {},
   "source": [
    "## Defining Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb402b8",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Decent \n",
    "\n",
    "$$ \\text{Update} = -\\text{Learning Rate} \\cdot \\text{Gradient}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5427718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDG:\n",
    "    \"\"\"Stochastic Gradient Decent class used to update layer paramers\n",
    "    The update is the -ve learning rate multiplied by the gradient calculated in the backward step.\n",
    "\n",
    "    Attr:\n",
    "        lr (float) Learning rate to scale the gradients by for the update\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate, decay) -> None:\n",
    "        self.lr = learning_rate\n",
    "        self.decay = decay\n",
    "    \n",
    "    def update(self, layers):\n",
    "        \"\"\"Update a layers parameters.\n",
    "        \"\"\"\n",
    "        # pre update step\n",
    "\n",
    "        for layer in layers:\n",
    "            if type(layer) == LinearLayer:\n",
    "                layer.weights += -self.lr*layer.d_w\n",
    "                layer.bias += -self.lr*layer.d_b\n",
    "            else:\n",
    "                raise NotImplementedError(f'SDG does not support {layer.__class__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e97e5",
   "metadata": {},
   "source": [
    "## Defining Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee48592",
   "metadata": {},
   "source": [
    "### One-hot encoding function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "649a4d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_index(y, n):\n",
    "    return np.eye(n)[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cfee58",
   "metadata": {},
   "source": [
    "#### Testing one hot masker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da547098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=3\n",
    "y_test = np.array([0,1,2, 1, 2])\n",
    "\n",
    "one_hot_encode_index(y_test, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f0c34d",
   "metadata": {},
   "source": [
    "### Define Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb0b0f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    \"\"\"Calculates the accuracy of a batch of predictions\"\"\"\n",
    "    return np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_true, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b81ddc",
   "metadata": {},
   "source": [
    "## Integration Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aab2afd",
   "metadata": {},
   "source": [
    "### Data initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15fc0a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "y = one_hot_encode_index(y, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eb017c",
   "metadata": {},
   "source": [
    "### Network Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b157cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Loss:1.099, accuracy:0.300\n",
      "Epoch:100, Loss:1.078, accuracy:0.447\n",
      "Epoch:200, Loss:1.066, accuracy:0.457\n",
      "Epoch:300, Loss:1.062, accuracy:0.463\n",
      "Epoch:400, Loss:1.060, accuracy:0.463\n",
      "Epoch:500, Loss:1.060, accuracy:0.453\n",
      "Epoch:600, Loss:1.058, accuracy:0.460\n",
      "Epoch:700, Loss:1.057, accuracy:0.450\n",
      "Epoch:800, Loss:1.054, accuracy:0.460\n",
      "Epoch:900, Loss:1.049, accuracy:0.450\n",
      "Epoch:1000, Loss:1.040, accuracy:0.453\n",
      "Epoch:1100, Loss:1.047, accuracy:0.430\n",
      "Epoch:1200, Loss:1.042, accuracy:0.430\n",
      "Epoch:1300, Loss:1.037, accuracy:0.443\n",
      "Epoch:1400, Loss:1.030, accuracy:0.443\n",
      "Epoch:1500, Loss:1.026, accuracy:0.453\n",
      "Epoch:1600, Loss:1.023, accuracy:0.450\n",
      "Epoch:1700, Loss:1.020, accuracy:0.453\n",
      "Epoch:1800, Loss:1.018, accuracy:0.453\n",
      "Epoch:1900, Loss:1.017, accuracy:0.430\n",
      "Epoch:2000, Loss:1.029, accuracy:0.403\n",
      "Epoch:2100, Loss:1.050, accuracy:0.463\n",
      "Epoch:2200, Loss:1.019, accuracy:0.407\n",
      "Epoch:2300, Loss:1.002, accuracy:0.430\n",
      "Epoch:2400, Loss:1.022, accuracy:0.500\n",
      "Epoch:2500, Loss:1.022, accuracy:0.473\n",
      "Epoch:2600, Loss:1.011, accuracy:0.477\n",
      "Epoch:2700, Loss:1.020, accuracy:0.487\n",
      "Epoch:2800, Loss:0.992, accuracy:0.390\n",
      "Epoch:2900, Loss:0.977, accuracy:0.403\n",
      "Epoch:3000, Loss:1.036, accuracy:0.440\n",
      "Epoch:3100, Loss:0.969, accuracy:0.507\n",
      "Epoch:3200, Loss:0.962, accuracy:0.570\n",
      "Epoch:3300, Loss:0.951, accuracy:0.487\n",
      "Epoch:3400, Loss:0.936, accuracy:0.513\n",
      "Epoch:3500, Loss:0.939, accuracy:0.577\n",
      "Epoch:3600, Loss:0.921, accuracy:0.513\n",
      "Epoch:3700, Loss:0.920, accuracy:0.510\n",
      "Epoch:3800, Loss:0.903, accuracy:0.537\n",
      "Epoch:3900, Loss:0.928, accuracy:0.527\n",
      "Epoch:4000, Loss:0.923, accuracy:0.540\n",
      "Epoch:4100, Loss:0.923, accuracy:0.567\n",
      "Epoch:4200, Loss:0.908, accuracy:0.540\n",
      "Epoch:4300, Loss:0.904, accuracy:0.543\n",
      "Epoch:4400, Loss:0.901, accuracy:0.547\n",
      "Epoch:4500, Loss:0.900, accuracy:0.540\n",
      "Epoch:4600, Loss:0.896, accuracy:0.540\n",
      "Epoch:4700, Loss:0.895, accuracy:0.520\n",
      "Epoch:4800, Loss:0.894, accuracy:0.537\n",
      "Epoch:4900, Loss:0.898, accuracy:0.540\n",
      "Epoch:5000, Loss:0.906, accuracy:0.543\n",
      "Epoch:5100, Loss:0.907, accuracy:0.533\n",
      "Epoch:5200, Loss:0.893, accuracy:0.550\n",
      "Epoch:5300, Loss:0.880, accuracy:0.553\n",
      "Epoch:5400, Loss:0.897, accuracy:0.533\n",
      "Epoch:5500, Loss:0.974, accuracy:0.527\n",
      "Epoch:5600, Loss:0.875, accuracy:0.567\n",
      "Epoch:5700, Loss:0.884, accuracy:0.533\n",
      "Epoch:5800, Loss:0.882, accuracy:0.573\n",
      "Epoch:5900, Loss:0.888, accuracy:0.557\n",
      "Epoch:6000, Loss:0.895, accuracy:0.553\n",
      "Epoch:6100, Loss:0.886, accuracy:0.570\n",
      "Epoch:6200, Loss:0.941, accuracy:0.537\n",
      "Epoch:6300, Loss:0.858, accuracy:0.557\n",
      "Epoch:6400, Loss:0.872, accuracy:0.577\n",
      "Epoch:6500, Loss:0.941, accuracy:0.527\n",
      "Epoch:6600, Loss:0.848, accuracy:0.610\n",
      "Epoch:6700, Loss:0.845, accuracy:0.587\n",
      "Epoch:6800, Loss:0.850, accuracy:0.580\n",
      "Epoch:6900, Loss:0.822, accuracy:0.623\n",
      "Epoch:7000, Loss:0.873, accuracy:0.580\n",
      "Epoch:7100, Loss:0.811, accuracy:0.593\n",
      "Epoch:7200, Loss:0.834, accuracy:0.573\n",
      "Epoch:7300, Loss:0.847, accuracy:0.593\n",
      "Epoch:7400, Loss:0.811, accuracy:0.603\n",
      "Epoch:7500, Loss:0.812, accuracy:0.593\n",
      "Epoch:7600, Loss:0.799, accuracy:0.607\n",
      "Epoch:7700, Loss:0.818, accuracy:0.560\n",
      "Epoch:7800, Loss:0.845, accuracy:0.580\n",
      "Epoch:7900, Loss:0.784, accuracy:0.630\n",
      "Epoch:8000, Loss:0.779, accuracy:0.597\n",
      "Epoch:8100, Loss:0.753, accuracy:0.630\n",
      "Epoch:8200, Loss:0.760, accuracy:0.633\n",
      "Epoch:8300, Loss:0.768, accuracy:0.623\n",
      "Epoch:8400, Loss:0.773, accuracy:0.643\n",
      "Epoch:8500, Loss:0.755, accuracy:0.647\n",
      "Epoch:8600, Loss:0.772, accuracy:0.637\n",
      "Epoch:8700, Loss:0.762, accuracy:0.633\n",
      "Epoch:8800, Loss:0.752, accuracy:0.647\n",
      "Epoch:8900, Loss:0.746, accuracy:0.637\n",
      "Epoch:9000, Loss:0.750, accuracy:0.657\n",
      "Epoch:9100, Loss:0.740, accuracy:0.637\n",
      "Epoch:9200, Loss:0.756, accuracy:0.637\n",
      "Epoch:9300, Loss:0.739, accuracy:0.620\n",
      "Epoch:9400, Loss:1.554, accuracy:0.443\n",
      "Epoch:9500, Loss:0.697, accuracy:0.670\n",
      "Epoch:9600, Loss:0.759, accuracy:0.613\n",
      "Epoch:9700, Loss:0.766, accuracy:0.630\n",
      "Epoch:9800, Loss:0.740, accuracy:0.643\n",
      "Epoch:9900, Loss:0.740, accuracy:0.637\n",
      "Epoch:10000, Loss:0.968, accuracy:0.570\n"
     ]
    }
   ],
   "source": [
    "# Initializing Network Components \n",
    "relu = ReLU()\n",
    "softmax = Softmax()\n",
    "cce_loss = CategoricalCrossEntropyLoss\n",
    "optimizer = SDG(0.85, 1e-2)\n",
    "linear1 = LinearLayer(2, 64)\n",
    "linear2 = LinearLayer(64, 3)\n",
    "\n",
    "update_layers = [linear1, linear2]\n",
    "\n",
    "n_epochs = 10000\n",
    "\n",
    "for epoch in range(n_epochs + 1):\n",
    "    # Forward Pass\n",
    "    linear1.forward(X) \n",
    "    relu.forward(linear1.output)\n",
    "    linear2.forward(relu.output)\n",
    "    y_pred = softmax.forward(linear2.output)\n",
    "\n",
    "    # Calculating loss and accuracy \n",
    "    loss = cce_loss.forward(y_pred, y)\n",
    "    acc = accuracy(y_pred, y) \n",
    "\n",
    "    # Printing results\n",
    "    if not epoch % 100:\n",
    "        print(f\"Epoch:{epoch}, Loss:{loss:.3f}, accuracy:{acc:.3f}\")\n",
    "\n",
    "    # Backward pass\n",
    "    softmax.backward(y_pred, y)\n",
    "    linear2.backward(softmax.d_soft)\n",
    "    relu.backward(linear2.d_x)\n",
    "    linear1.backward(relu.d_relu)\n",
    "\n",
    "    # Optimization Step\n",
    "    optimizer.update(update_layers)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e554e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
