{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev Notebook - Backpropagation \n",
    "\n",
    "In this notebook I examine how backpropagation works referencing the examples provided in [NNFS](https://nnfs.io/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnfs \n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7691a0a1",
   "metadata": {},
   "source": [
    "#### Backpropagation - simplified \n",
    "\n",
    "Backpropagation through ReLU, based on the example in NNFS to ensure a solid understanding of the underlying math (partial diff and chain rule) and mechanisms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e5b368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulating a forward pass \n",
    "\n",
    "x = [1.0, -2.0, 3.0]\n",
    "w = [-3.0, -1.0, 2.0]\n",
    "b = 1.0\n",
    "\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0]* w[0]\n",
    "xw1 = x[1]* w[1]\n",
    "xw2 = x[2]* w[2]\n",
    "\n",
    "# Summing weights and bias\n",
    "z = xw0 + xw1 + xw2 +b\n",
    "\n",
    "# applying relu\n",
    "y = max(z, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9da502",
   "metadata": {},
   "source": [
    "If we represent the forward pass as a function we can say:\n",
    "\n",
    "$$\\text{ReLU}\\left(\\sum[\\text{inputs}\\cdotp\\text{weights}]+\\text{bias}\\right)$$\n",
    "\n",
    "We now need to find the partial derivatives of all the function for all the parameters. For example if we wanted to know the effect that w0 had on the outcome we woul need to know:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial x_0}\\left[\\text{ReLU}\\left(\\sum[\\text{inputs}\\cdotp\\text{weights}]+\\text{bias}\\right)\\right] = \\frac{d \\text{ReLU()}}{d \\text{sum()}}\\cdot\\frac{\\partial\\text{sum()}}{\\partial mul(x_0,w_0)}\\cdot\\frac{\\partial mul(x_0,w_0)}{\\partial x_0} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dee3a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The backward pass\n",
    "\n",
    "# derivative from previous layers\n",
    "d_val = 1.0\n",
    "\n",
    "# the derivative of relu wrt z \n",
    "d_relu_dz = d_val * (0,1)[z>0] # == i if z> 0, else 0 \n",
    "\n",
    "# Recall the derivative of a sum opperator os always 1 \n",
    "# derivative of the sum wrt x_n*w_n \n",
    "d_sum_dxwn = 1\n",
    "d_relu_dxw0 = d_relu_dz * d_sum_dxwn\n",
    "d_relu_dxw1 = d_relu_dz * d_sum_dxwn\n",
    "d_relu_dxw2 = d_relu_dz * d_sum_dxwn\n",
    "\n",
    "# derivative of the sum wrt b (bias) \n",
    "d_sum_db = 1\n",
    "d_relu_db = d_relu_dz * d_sum_db\n",
    "\n",
    "# Recall the derivative of a product is whateve input is being multiplied \n",
    "d_mul_dx0 = w[0]\n",
    "d_mul_dx1 = w[1]\n",
    "d_mul_dx2 = w[2]\n",
    "d_relu_dx0 = d_mul_dx0 * d_relu_dxw0\n",
    "d_relu_dx1 = d_mul_dx1 * d_relu_dxw2\n",
    "d_relu_dx2 = d_mul_dx2 * d_relu_dxw2\n",
    "\n",
    "d_mul_dw0 = x[0]\n",
    "d_mul_dw1 = x[1]\n",
    "d_mul_dw2 = x[2]\n",
    "d_relu_dw0 = d_mul_dw0 * d_relu_dxw0\n",
    "d_relu_dw1 = d_mul_dw1 * d_relu_dxw1\n",
    "d_relu_dw2 = d_mul_dw2 * d_relu_dxw2\n",
    "\n",
    "# Simplifying the above we can rewrite as:\n",
    "d_relu_dx0 = d_val * (0,1)[z>0] * w[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f97b9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized code for the backward pass \n",
    "# (yes, variables are being shadowed but it is okay this section is just for learning an not final code)\n",
    "d_val = 1.0\n",
    "\n",
    "d_x = [d_val*(0,1)[z>0]*_w for _w in w] # the derivative of the previous layer * d of relu * the corresponding weight for the input\n",
    "d_w = [d_val*(0,1)[z>0]*_x for _x in x] # the derivative of the previous layer * d of relu * the corresponding input for the weight\n",
    "d_b = d_val * (0,1)[z>0] # the derivative of the previous layer * d of relu (the derivative of the sum will always be 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79a1ecc",
   "metadata": {},
   "source": [
    "#### Backpropagation - A layer of neurons \n",
    "\n",
    "Considering multiple neurons in a layer rather than just one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8254d5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.44, -0.38, -0.07,  1.37])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy passed in grads from previous layer \n",
    "d_val = np.array([[1.,1.,1.]])\n",
    "\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "# gradient for first input  \n",
    "d_x0 = sum([weights[0][i] * d_val[0][i] for i in range(weights.shape[1])])\n",
    "d_x1 = sum([weights[1][i] * d_val[0][i] for i in range(weights.shape[1])])\n",
    "d_x2 = sum([weights[2][i] * d_val[0][i] for i in range(weights.shape[1])])\n",
    "d_x3 = sum([weights[3][i] * d_val[0][i] for i in range(weights.shape[1])])\n",
    "d_x = np.array([d_x0, d_x1, d_x2, d_x3])\n",
    "d_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42407a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44, -0.38, -0.07,  1.37],\n",
       "       [ 0.88, -0.76, -0.14,  2.74],\n",
       "       [ 1.32, -1.14, -0.21,  4.11]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizing the above code and accounting for batches of samples we get:\n",
    "\n",
    "d_val = np.array([[1., 1., 1.],\n",
    "                    [2., 2., 2.],\n",
    "                    [3., 3., 3.]])\n",
    "\n",
    "d_x = np.dot(d_val, weights.T)\n",
    "d_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fae954e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5,  0.5,  0.5],\n",
       "       [20.1, 20.1, 20.1],\n",
       "       [10.9, 10.9, 10.9],\n",
       "       [ 4.1,  4.1,  4.1]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To calculate the gradients wrt the weights we consider the input values \n",
    "inputs = np.array([[1, 2, 3, 2.5],\n",
    "                    [2., 5., -1., 2],\n",
    "                    [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "d_w = np.dot(inputs.T, d_val)\n",
    "d_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e04be64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6., 6., 6.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the derivative of the bias \n",
    "d_b = np.sum(d_val, axis=0, keepdims=True)\n",
    "d_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "336adb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  0.,  0.],\n",
       "       [ 5.,  0.,  0.,  8.],\n",
       "       [ 0., 10., 11.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output for the linear component \n",
    "z = np.array([[1,2,-3,-4], [2,-7,-1,3], [-1, 2,5,-1]])\n",
    "d_val = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "# Calcuting the derivative of Relu \n",
    "d_relu = np.zeros(z.shape)\n",
    "d_relu[z>0] = 1\n",
    "d_relu *= d_val\n",
    "d_relu "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a7372",
   "metadata": {},
   "source": [
    "At this point I will go back and update the Linear Layer and the Relu with backward code.\n",
    "\n",
    "#### Backpropagation of CCE Loss\n",
    "\n",
    "We find that the derivative of CCE loss to be:\n",
    "\n",
    "$$ \\frac{\\delta L_i}{\\hat{y_{i,j}}} = -\\frac{y_{i,j}}{\\hat{y_{i,j}}} $$\n",
    "\n",
    "I will now add this directly to the function\n",
    "\n",
    "#### Backpropagation of Softmax activation\n",
    "\n",
    "$$ \\frac{\\partial S_{i,j}}{\\partial Z_{i,k}} = S_{i,j} \\cdot (\\delta_{j,k} - S_{i,k})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d706b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7, 0. , 0. ],\n",
       "       [0. , 0.1, 0. ],\n",
       "       [0. , 0. , 0.2]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test implementation \n",
    "\n",
    "# Softmax output \n",
    "so = [0.7, 0.1, 0.2] \n",
    "so = np.array(so).reshape(-1, 1)\n",
    "np.diagflat(so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d2f46ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49, 0.07, 0.14],\n",
       "       [0.07, 0.01, 0.02],\n",
       "       [0.14, 0.02, 0.04]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(so, so.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b30ddf8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.20999999, -0.07      , -0.14      ],\n",
       "       [-0.07      ,  0.09      , -0.02      ],\n",
       "       [-0.14      , -0.02      ,  0.16      ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diagflat(so) - np.dot(so, so.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass for L1 and L2 \n",
    "\n",
    "$$ L_1'=\\frac{\\partial}{\\partial w_n} \\lambda\\sum_m|w_m| =  \\lambda\\frac{\\partial}{\\partial w_n} |w_m| =\n",
    "    \\begin{cases}\n",
    "        1 & w_m > 0 \\\\\n",
    "        -1 & w_m < 0\n",
    "    \\end{cases}\n",
    "\n",
    "$$ \n",
    "\n",
    "$$ L_2' = \\frac{\\partial}{\\partial w_n} \\lambda\\sum_m w_m^2 = \\lambda\\frac{\\partial}{\\partial w_n}w_m^2 = 2\\lambda w_m$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1., -1.,  1.],\n",
       "       [ 1., -1.,  1., -1.],\n",
       "       [-1., -1.,  1.,  1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]])\n",
    "\n",
    "dl1 = np.ones_like(weights)\n",
    "dl1[weights < 0 ] = -1\n",
    "\n",
    "dl1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation of Dropout\n",
    "\n",
    "When the output of the binomial function is 1, the function is the previous layers output $$z$$:\n",
    "\n",
    "$$ f'(z, p)  = \\frac{\\partial}{\\partial z}\\left[ \\frac{z}{1-p} \\right] = \\frac{1}{1-p}\\cdot\\frac{\\partial}{\\partial z} z = \\frac{1}{1-p} $$\n",
    "\n",
    "where p is the rate of neurons we intend to zero\n",
    "\n",
    "and when the output is 0 the functions output is 0 and so is the partial derivative. \n",
    "\n",
    "$$ f'(z, p) = 0 $$\n",
    "\n",
    "so we can denote the derivative of dropout as: \n",
    "\n",
    "$$ \\frac{\\partial}{\\partial Z_i}\\text{Dropout} = \\begin{cases} \n",
    "                                                    \\frac{1}{1-p} & r_i = 1 \\\\\n",
    "                                                    0 & r_i =0 \n",
    "                                                \\end{cases}\n",
    "                                                = \\frac{r_i}{1-p} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53ce40b7a308f72c177e55be18c06becd4df4a3e62d1e5504b718e819fa82c6d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3rc1 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
