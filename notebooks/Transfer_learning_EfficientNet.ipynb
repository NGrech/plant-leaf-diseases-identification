{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning with Resnet 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# setting up data loaders\n",
    "data_dir = os.path.join(os.pardir, 'data', 'Plant_leave_diseases_32')\n",
    "\n",
    "train_data = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=data_transforms)\n",
    "test_data = datasets.ImageFolder(os.path.join(data_dir, 'validation'), transform=data_transforms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = models.resnet50(pretrained=True)\n",
    "\n",
    "# Freezing the paramiters of the layers we do not want to train\n",
    "for parameters in mdl.parameters():\n",
    "    parameters.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating Classification layer \n",
    "_inputs = mdl.fc.in_features\n",
    "\n",
    "mdl.fc = nn.Sequential(OrderedDict([\n",
    "    ('fc1', nn.Linear(_inputs, 500)),\n",
    "    ('relu', nn.ReLU()),\n",
    "    ('fc2', nn.Linear(500, 39)),\n",
    "    ('output', nn.LogSoftmax(dim=1))\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs \n",
    "config = {\n",
    "    'max_epochs': 200,\n",
    "    'learning_rate': 0.002,\n",
    "    'resolution': 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, validation_loader, config, n_epochs=10, stopping_treshold=None):\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print('CUDA is available!  Training on GPU ...')\n",
    "        model.cuda()\n",
    "\n",
    "\n",
    "    # Loss and optimizer setup \n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    # Setting minimum validation loss to inf\n",
    "    validation_loss_minimum = np.Inf \n",
    "    train_loss_history = []\n",
    "    validation_loss_history = []\n",
    "\n",
    "    for epoch in range(1, n_epochs +1):\n",
    "\n",
    "        training_loss = 0.0\n",
    "        validation_loss = 0.0\n",
    "\n",
    "        # Training loop\n",
    "        training_accuracies = []\n",
    "        for X, y in train_loader:\n",
    "            \n",
    "            # Moving data to gpu if using \n",
    "            if torch.cuda.is_available():\n",
    "                X, y = X.cuda(), y.cuda()\n",
    "            \n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(X)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, y)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            training_loss += loss.item()*X.size(0)\n",
    "\n",
    "            # calculating accuracy\n",
    "            ps = torch.exp(output)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals = top_class == y.view(*top_class.shape)\n",
    "            training_accuracies.append(torch.mean(equals.type(torch.FloatTensor)).item())\n",
    "\n",
    "        # Validation Loop\n",
    "        with torch.no_grad():\n",
    "            accuracies = []\n",
    "            for X, y in validation_loader:\n",
    "\n",
    "                # Moving data to gpu if using \n",
    "                if torch.cuda.is_available():\n",
    "                    X, y = X.cuda(), y.cuda()\n",
    "                # forward pass: compute predicted outputs by passing inputs to the model\n",
    "                output = model(X)\n",
    "                # calculate the batch loss\n",
    "                loss = criterion(output, y)\n",
    "                # update validation loss\n",
    "                validation_loss += loss.item()*X.size(0)\n",
    "\n",
    "                # calculating accuracy\n",
    "                ps = torch.exp(output)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == y.view(*top_class.shape)\n",
    "                accuracies.append(torch.mean(equals.type(torch.FloatTensor)).item())\n",
    "                \n",
    "        # Mean loss \n",
    "        mean_training_loss = training_loss/len(train_loader.sampler)\n",
    "        mean_validation_loss = validation_loss/len(validation_loader.sampler)\n",
    "        mean_train_accuracy = sum(training_accuracies)/len(training_accuracies)\n",
    "        mean_accuracy = sum(accuracies)/len(accuracies)\n",
    "        train_loss_history.append(mean_training_loss)\n",
    "        validation_loss_history.append(mean_validation_loss)\n",
    "\n",
    "        # Printing epoch stats\n",
    "        print(f'Epoch: {epoch}/{n_epochs}, ' +\\\n",
    "              f'Training Loss: {mean_training_loss:.3f}, '+\\\n",
    "              f'Train accuracy {mean_train_accuracy:.3f} ' +\\\n",
    "              f'Validation Loss: {mean_validation_loss:.3f}, '+\\\n",
    "              f'Validation accuracy {mean_accuracy:.3f}')\n",
    "\n",
    "        # logging with mlflow \n",
    "        if mlflow.active_run():\n",
    "            mlflow.log_metric('loss', mean_training_loss, step=epoch)\n",
    "            mlflow.log_metric('accuracy', mean_train_accuracy, step=epoch)\n",
    "            mlflow.log_metric('validation_accuracy', mean_accuracy, step=epoch)\n",
    "            mlflow.log_metric('validation_loss', mean_validation_loss, step=epoch)\n",
    "\n",
    "        # Testing for early stopping\n",
    "        if stopping_treshold:\n",
    "            if mean_validation_loss < validation_loss_minimum:\n",
    "                validation_loss_minimum = mean_validation_loss\n",
    "            elif len([v for v in validation_loss_history[-stopping_treshold:] if v >= validation_loss_minimum]) >= stopping_treshold:\n",
    "                print(f\"Stopping early at epoch: {epoch}/{n_epochs}\")\n",
    "                break\n",
    "        \n",
    "\n",
    "    return train_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngrec\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\plant-leaf-diseases-identification-CBiGMIHu-py3.8\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200, Training Loss: 1.684, Train accuracy 0.527 Validation Loss: 5.315, Validation accuracy 0.067\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(test_data, batch_size=64)\n",
    "\n",
    "mlflow.set_experiment(\"Plant Leaf Disease\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param('framework', 'pytorch')\n",
    "    mlflow.log_param('data_split', '90/10')\n",
    "    mlflow.log_param('type', 'Resnet50')\n",
    "    mlflow.log_params(config)\n",
    "    tlh, vlh = train(mdl, train_loader, validation_loader, config, n_epochs=config['max_epochs'], stopping_treshold=50)\n",
    "    config['train_loss_history'] = tlh\n",
    "    config['validation_loss_history'] = vlh"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d53c06ab4573055b295bcdfb3b25bc2da21e5ae576a05184a9c53fc2168185ab"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('plant-leaf-diseases-identification-CBiGMIHu-py3.8': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
