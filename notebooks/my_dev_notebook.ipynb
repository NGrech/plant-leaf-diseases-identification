{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ad873e",
   "metadata": {},
   "source": [
    "# Dev notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef02f46",
   "metadata": {},
   "source": [
    "I use this notebook to develop my implementation of a simple and minimal neural network framework.\n",
    "\n",
    "Inspiration for this network is drawn from the book [Neural Networks from Scratch (NNFS)](https://nnfs.io/) & [Pytorch's implementation](https://pytorch.org/)\n",
    "\n",
    "Notes:\n",
    "- The work on experimenting with backpropagation has been moved to another notebook to keep this one minimal\n",
    "- I have chosen to use a similar naming convention to that used by pytorch (why reinvent the wheel), this has the benefit of ensuring that when we compare implementations the architecture of the networks is the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab9fbb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import for dev and testing\n",
    "import nnfs \n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from nnfs.datasets import spiral_data\n",
    "from typing import List\n",
    "\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466662af",
   "metadata": {},
   "source": [
    "## Defining Base Module\n",
    "\n",
    "This is a module that contains all the base attributes and function needed by every class in the framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52091eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(ABC):\n",
    "    \"\"\"Base class for all classes in frame work to ensure the same attributes and common function names.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        # Attributes to hold input and outputs\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85db83e2",
   "metadata": {},
   "source": [
    "##  Defining layers \n",
    "\n",
    "In this section We define and test the layers\n",
    "\n",
    "Notes:\n",
    "- since we intend to use ReLU as one of our activation functions we will use the He weight initialization method as described in https://arxiv.org/abs/1502.01852"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7fe0c9",
   "metadata": {},
   "source": [
    "### Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48b830b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(Module):\n",
    "    \"\"\"Linear transformation layer of the type o = ixW + b,\n",
    "    \n",
    "    where I is the incoming vector, W is the layers weight matrix, b is bias vector and o is the dot product of the \n",
    "    i and W plus the bias\n",
    "    \n",
    "    Args:\n",
    "        in_features (int): the size of the input features \n",
    "        out_features (int): the size of the output features\n",
    "        \n",
    "    Attributes:\n",
    "        weights (np_array) numpy array of in_features x n_neurons\n",
    "        biases  (np_array) numpy array of 1 x n_neurons\n",
    "        inputs  (np_array) numpy array of latest batch of inputs\n",
    "        inputs  (np_array) numpy array of latest batch of outputs\n",
    "        d_w     (np_array) The current gradients with respect to the weights \n",
    "        d_x     (np_array) The current gradients with respect to the inputs\n",
    "        d_b     (np_array) The current gradients with respect to the biases\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, \n",
    "                 lambda_l1_weight=0, lambda_l1_bias=0, \n",
    "                 lambda_l2_weight=0, lambda_l2_bias=0) -> None:\n",
    "        super().__init__()\n",
    "        # initializing weights and biases \n",
    "        #self.weights = np.random.normal(0.0, np.sqrt(2/in_features), (in_features, out_features))\n",
    "        # Using a simpler initialization  for testing \n",
    "        self.weights = 0.01 * np.random.randn(in_features, out_features)\n",
    "        self.bias = np.zeros((1, out_features))\n",
    "        # initializing regularization lambdas\n",
    "        self.lambda_l1_weight = lambda_l1_weight\n",
    "        self.lambda_l1_bias = lambda_l1_bias\n",
    "        self.lambda_l2_weight = lambda_l2_weight\n",
    "        self.lambda_l2_bias = lambda_l2_bias\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Saving inputs for backward step\n",
    "        self.input = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def l1_backward_w(self):\n",
    "        d_l1 = np.ones_like(self.weights) \n",
    "        d_l1[self.weights < 0] = -1\n",
    "        return d_l1 * self.lambda_l1_weight\n",
    "\n",
    "    def l1_backward_b(self):\n",
    "        d_l1 = np.ones_like(self.bias) \n",
    "        d_l1[self.bias < 0] = -1\n",
    "        return d_l1 * self.lambda_l1_bias\n",
    "\n",
    "    def l2_backward_w(self):\n",
    "        return 2 * self.lambda_l2_weight  * self.weights\n",
    "        \n",
    "    def l2_backward_b(self):\n",
    "        return 2 * self.lambda_l2_bias  * self.bias\n",
    "\n",
    "    def backward(self, d_vals):\n",
    "        \"\"\"Backpropagation  of the linear function\n",
    "\n",
    "        Args:\n",
    "            d_vals (np_array) array of derivatives from the previous layer/function.\n",
    "        \"\"\"\n",
    "        self.d_w = np.dot(self.input.T, d_vals) + self.l1_backward_w() + self.l2_backward_w()\n",
    "        self.d_x = np.dot(d_vals, self.weights.T)\n",
    "        self.d_b = np.sum(d_vals, axis=0, keepdims=True) + self.l1_backward_b() + self.l2_backward_b()\n",
    "\n",
    "    def regularization_loss(self):\n",
    "        loss = 0\n",
    "        # L1 weight \n",
    "        loss += self.lambda_l1_weight * np.sum(np.abs(self.weights))\n",
    "        # L1 bias\n",
    "        loss += self.lambda_l1_bias * np.sum(np.abs(self.bias))\n",
    "        # L2 weight \n",
    "        loss += self.lambda_l2_weight * np.sum(self.weights * self.weights)\n",
    "        # L1 bias\n",
    "        loss += self.lambda_l2_bias * np.sum(self.bias * self.bias)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ea363f",
   "metadata": {},
   "source": [
    "#### Testing Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1ca4160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [-1.0475188e-04  1.1395361e-04 -4.7983500e-05]\n",
      " [-2.7414842e-04  3.1729150e-04 -8.6921798e-05]\n",
      " [-4.2188365e-04  5.2666257e-04 -5.5912682e-05]\n",
      " [-5.7707680e-04  7.1401405e-04 -8.9430439e-05]\n",
      " [-3.5430698e-04  3.5025488e-04 -2.3363481e-04]\n",
      " [-8.9267001e-04  1.0767876e-03 -1.9453237e-04]\n",
      " [-9.3350781e-04  1.0723802e-03 -3.1227397e-04]\n",
      " [-1.1243758e-03  1.3112801e-03 -3.3629674e-04]\n",
      " [-1.3386955e-03  1.6200906e-03 -2.8101794e-04]]\n"
     ]
    }
   ],
   "source": [
    "# The sample data is a list of coordinate, ie two points \n",
    "# The layer therefore will take 2 inputs \n",
    "# We have given it 3 out features (3 neurons) so we expect to see an output with the shape (n_samples*n_neurons, n_neurons)\n",
    "# In out case that should be (300, 3) \n",
    "X, _ = spiral_data(samples=100, classes=3)\n",
    "linear1 = LinearLayer(2, 3)\n",
    "output = linear1.forward(X)\n",
    "print(output[:10, :])\n",
    "\n",
    "assert output.shape == (300,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2017ad64",
   "metadata": {},
   "source": [
    "## Defining Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8da5f5",
   "metadata": {},
   "source": [
    "### ReLu\n",
    "\n",
    "$$y = \\begin{cases}\n",
    "   x &x> 0 \\\\\n",
    "   0 & otherwise\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a03d0ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\"Applies Rectified linear Unit function to vector.\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        # initializing attributes needed for backwards \n",
    "        super().__init__()\n",
    "        self.d_relu = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # storing inputs needed for backwards \n",
    "        self.inputs = x\n",
    "        self.output = np.maximum(x, 0)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, d_vals):\n",
    "        self.d_relu = d_vals.copy()\n",
    "        self.d_relu[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa60fd7a",
   "metadata": {},
   "source": [
    "#### Testing ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bea142e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 3. , 4. , 0. , 0.1, 0. ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = [-2, 3, 4, 0, 0.1, -44]\n",
    "test_relu = ReLU()\n",
    "\n",
    "# Checking values are as expected \n",
    "assert np.all(np.array([0., 3, 4, 0, 0.1, 0.]) == test_relu.forward(i))\n",
    "\n",
    "test_relu.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc75ac1",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "$$\\text{softmax}(x)_i = \\frac{exp(x_i)}{\\sum_{j}^{ }exp(x_j))}$$\n",
    "\n",
    "The soft max represents the confidence score for each output class and adds up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "966f1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Module):\n",
    "    \"\"\"Applies Softmax function to input matrix.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.confidence_scores = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # exponenets of each value\n",
    "        exp_vals = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        exp_sum = np.sum(exp_vals, axis=1, keepdims=True)\n",
    "        # Normalization to get the proabilities \n",
    "        self.output = exp_vals/exp_sum\n",
    "        return self.output\n",
    "\n",
    "    def _backward(self, d_vals):\n",
    "        # Initialize array for gradients wrt to inputs\n",
    "        self.d_soft = np.zeros_like(d_vals)\n",
    "        \n",
    "        _iter = enumerate(zip(self.output, d_vals))\n",
    "        for i, conf_score, d_val in _iter:\n",
    "            # Flatten confidence scores\n",
    "            cs = conf_score.reshape(-1, 1)\n",
    "            # Find the Jacobian matrix of the output \n",
    "            j_matrix = np.diagflat(cs) - np.dot(cs, cs.T)\n",
    "            # get the gradient \n",
    "            self.d_soft[i] = np.dot(j_matrix, d_val)\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"Does a the combined backward pass for CCE & Softmax as a single, faster step.\"\"\"\n",
    "        # Number of examples in the batch\n",
    "        n = len(y_pred)\n",
    "\n",
    "        # Getting descrete vals from one hot encoding \n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "        \n",
    "        self.d_soft = y_pred.copy()\n",
    "        self.d_soft[range(n), y_true] -= 1\n",
    "        self.d_soft = self.d_soft / n\n",
    "        return self.d_soft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e8095e",
   "metadata": {},
   "source": [
    "#### Testing Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5cd2c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.11513104e-19, 5.74952226e-19, 1.00000000e+00]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = Softmax()\n",
    "softmax.forward([[1,2,44]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b31bb",
   "metadata": {},
   "source": [
    "## Defining Loss - Categorical Cross-Entropy\n",
    "\n",
    "$$ L_i = -\\sum_j y_{i,j}\\log(\\hat{y}_{i,j}) $$\n",
    "\n",
    "With taking one hot encoding into account we can simplify this down to:\n",
    "\n",
    "$$ L_i = -y_{i,k}\\log(\\hat{y}_{i,k}) $$\n",
    "\n",
    "where K is the index of the correct class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7645a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalCrossEntropyLoss:\n",
    "    \"\"\"Calculates the CCE loss for a given set of predictions.\n",
    "    This method expect a softmax output and one-hot encoded label mask\n",
    "    \n",
    "    y_pred (np_array): matrix of confidence scores of the prediction\n",
    "    y_true (np_array): matrix of one-hot encoded true lables of the classes\n",
    "    \"\"\"\n",
    "    def forward(y_pred, y_true):\n",
    "        # Clipping and applying one hot encoded labels as mask \n",
    "        # to zero out scores corresponding to incorrect classes\n",
    "        # We clip to make sure that none of the reaming classes are 0 or \n",
    "        # exactly 1 \n",
    "        clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        corrected = np.sum(clipped*y_true, axis=1)\n",
    "        # Taking the -ve log of the remaining confidence scores \n",
    "        negative_log = -np.log(corrected)\n",
    "        return np.mean(negative_log)\n",
    "\n",
    "    def backward(y_pred, y_true):\n",
    "        \"\"\"Backpropagation  of the CCE Loss\n",
    "\n",
    "        Args:\n",
    "            y_pred (np_array) array of predictions.\n",
    "            y_true (np_array) array of correct labels.\n",
    "        \"\"\"\n",
    "        return (-y_true/y_pred)/len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6704da26",
   "metadata": {},
   "source": [
    "#### Testing CCE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b0abd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38506088005216804"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.array([[0.7, 0.1, 0.2], [0.1,0.5,0.4],[0.02,0.9,0.08]])\n",
    "y_true = np.array([[1,0,0], [0,1,0], [0,1,0]])\n",
    "\n",
    "loss_function = CategoricalCrossEntropyLoss\n",
    "loss_function.forward(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b0d6f",
   "metadata": {},
   "source": [
    "## Defining Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb402b8",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Decent \n",
    "\n",
    "$$ \\text{Update} = -\\text{Learning Rate} \\cdot \\text{Gradient}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5427718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDG:\n",
    "    \"\"\"Stochastic Gradient Decent class used to update layer paramers\n",
    "    The update is the -ve learning rate multiplied by the gradient calculated in the backward step.\n",
    "\n",
    "    Attr:\n",
    "        lr (float) Learning rate to scale the gradients by for the update\n",
    "    \"\"\"\n",
    "    IMPLEMENTED = [LinearLayer]\n",
    "\n",
    "    def __init__(self, learning_rate=1, decay=0., momentum=0.) -> None:\n",
    "        self.lr = learning_rate\n",
    "        self.clr = learning_rate # current learning rate\n",
    "        self.decay = decay\n",
    "        self.momentum = momentum\n",
    "        self.iterations = 0\n",
    "\n",
    "    def init_momentum(self, layers):\n",
    "        for layer in layers:\n",
    "            if not hasattr(layer, 'momentum_w'):\n",
    "                layer.momentum_w = np.zeros_like(layer.weights)\n",
    "                layer.momentum_b = np.zeros_like(layer.bias)\n",
    "\n",
    "    def pre_update_step(self):\n",
    "        decay_rate = 1/(1 + self.decay * self.iterations)\n",
    "        self.clr = self.lr * decay_rate\n",
    "\n",
    "    def get_updates(self, layer):\n",
    "        return (\n",
    "            -self.clr*layer.d_w,\n",
    "            -self.clr*layer.d_b\n",
    "        )\n",
    "\n",
    "    def get_momentum_updates(self, layer):\n",
    "        wu = (self.momentum * layer.momentum_w) - (self.clr * layer.d_w) \n",
    "        bu = (self.momentum * layer.momentum_b) - (self.clr * layer.d_b) \n",
    "        layer.momentum_w = wu\n",
    "        layer.momentum_b = bu\n",
    "        return (wu, bu)\n",
    "\n",
    "    def update(self, layers):\n",
    "        \"\"\"Update a layers parameters.\n",
    "        \"\"\"\n",
    "        # Test to make sure all layers supported\n",
    "        if any(l for l in layers if type(l) not in self.IMPLEMENTED):\n",
    "            unsupported = next(l for l in layers if type(l) not in self.IMPLEMENTED)\n",
    "            raise NotImplementedError(f'SDG does not support {unsupported.__class__}')\n",
    "\n",
    "        # pre update step\n",
    "        if self.decay:\n",
    "            self.pre_update_step()\n",
    "\n",
    "        # On the first iteration using momentum initialize the layer momentums\n",
    "        if self.iterations == 0 and self.momentum:\n",
    "            self.init_momentum(layers)\n",
    "\n",
    "        # Update step\n",
    "        for layer in layers:\n",
    "\n",
    "            if self.momentum:\n",
    "                weight_u, bias_u = self.get_momentum_updates(layer)\n",
    "            else:\n",
    "                weight_u, bias_u = self.get_updates(layer)\n",
    "            \n",
    "            layer.weights += weight_u\n",
    "            layer.bias += bias_u\n",
    "\n",
    "        # post update\n",
    "        self.iterations += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\n",
    "Short for Adaptive Momentum.\n",
    "\n",
    "An extension to the Root mean square propagation (RSMprop) technique that adds in a bias correction mechanism used to correct the momentum and momentum caches.\n",
    "\n",
    "To find the update with Adam we need to take the following steps:\n",
    "\n",
    "1. Find momentum for the current step\n",
    "2. Get corrected the momentum \n",
    "3. Update the cache with the square of the gradient \n",
    "4. Get the corrected cache \n",
    "5. Update weights \n",
    "\n",
    "\n",
    "In the first step we calculate the layer weight and bias momentums by:\n",
    "\n",
    "$$ \\text{Layer Momentum} = (\\beta_1 \\cdot \\text{Layer Momentum}) + ((1 - \\beta_1) \\cdot gradient)$$ \n",
    "\n",
    "where $\\beta_1$ is a hyper-parameter that allows us to apply fractions of the momentum and gradient at each step. \n",
    "\n",
    "To correct this we then divide the momentum by bias correction mechanism: \n",
    "\n",
    "$$ \\text{Corrected Momentum} = \\frac{\\text{Layer Momentum}}{1 - \\beta_1^{n+1}} $$\n",
    "\n",
    "where $n$ is the number of the iteration/epoch and we add 1 to it to account for initializing it from 0\n",
    "\n",
    "Next we update the cache for the weights and biases:\n",
    "\n",
    "$$ \\text{Cache} = (\\beta_2 \\cdot \\text{Cache}) + ((1 - \\beta_2) * \\text{gradients}^2)$$\n",
    "\n",
    "We once again correct, this time the cache, with Adam's bias correction mechanism:\n",
    "\n",
    "$$ \\text{Corrected Cache} = \\frac{\\text{Cache}}{{1 - \\beta_2^{n+1}}}$$\n",
    "\n",
    "Finally to update the weights we do the following:\n",
    "\n",
    "$$ \\text{Update} = \\frac{\\text{Current Learning Rate} \\cdot \\text{Corrected Momentum}}{\\sqrt{\\text{Corrected Cache}} + \\epsilon} $$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9974c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    \"\"\"Adam Optimizer\"\"\"\n",
    "\n",
    "    IMPLEMENTED = [LinearLayer]\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999) -> None:\n",
    "        self.lr = learning_rate\n",
    "        self.clr = learning_rate # current learning rate\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.iterations = 0\n",
    "\n",
    "    def pre_update_step(self):\n",
    "        decay_rate = 1/(1 + self.decay * self.iterations)\n",
    "        self.clr = self.lr * decay_rate\n",
    "\n",
    "    def init_momentum(self, layers:List[LinearLayer]):\n",
    "        for layer in layers:\n",
    "            # Init momentum for weights\n",
    "            layer.momentums_w = np.zeros_like(layer.weights)\n",
    "            layer.cache_w = np.zeros_like(layer.weights)\n",
    "\n",
    "            # Init momentums for biases\n",
    "            layer.momentums_b = np.zeros_like(layer.bias)\n",
    "            layer.cache_b = np.zeros_like(layer.bias)\n",
    "            \n",
    "\n",
    "    def update(self, layers:List[LinearLayer]):\n",
    "        # pre update step\n",
    "        if self.decay:\n",
    "           self.pre_update_step()\n",
    "        \n",
    "        if self.iterations == 0:\n",
    "            self.init_momentum(layers)\n",
    "\n",
    "        # Update step\n",
    "        for layer in layers:     \n",
    "            ## Updating momentum \n",
    "            layer.momentums_w = self.beta_1 * layer.momentums_w + (1 - self.beta_1) * layer.d_w\n",
    "            layer.momentums_b = self.beta_1 * layer.momentums_b + (1 - self.beta_1) * layer.d_b\n",
    "\n",
    "            ## Correcting momentum \n",
    "            correction_bias_momentums = 1 - self.beta_1**(self.iterations +1)\n",
    "\n",
    "            corrected_weights = layer.momentums_w / correction_bias_momentums\n",
    "            corrected_bias    = layer.momentums_b / correction_bias_momentums\n",
    "\n",
    "            ## Updating cache\n",
    "            layer.cache_w = self.beta_2 * layer.cache_w + (1 - self.beta_2) * layer.d_w**2\n",
    "            layer.cache_b = self.beta_2 * layer.cache_b + (1 - self.beta_2) * layer.d_b**2\n",
    "\n",
    "            ## Correcting cache\n",
    "            correction_bias_cache = 1 - self.beta_2**(self.iterations +1)\n",
    "\n",
    "            corrected_cache_w = layer.cache_w / correction_bias_cache\n",
    "            corrected_cache_b = layer.cache_b / correction_bias_cache\n",
    "\n",
    "            ## Updating weights \n",
    "            layer.weights += -self.clr * corrected_weights / (np.sqrt(corrected_cache_w) + self.epsilon)\n",
    "\n",
    "            ## Updating bias\n",
    "            layer.bias    += -self.clr * corrected_bias / (np.sqrt(corrected_cache_b) + self.epsilon)\n",
    "        \n",
    "        # Post update step\n",
    "        self.iterations += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e97e5",
   "metadata": {},
   "source": [
    "## Defining Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee48592",
   "metadata": {},
   "source": [
    "### One-hot encoding function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "649a4d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_index(y, n):\n",
    "    return np.eye(n)[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cfee58",
   "metadata": {},
   "source": [
    "#### Testing one hot masker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da547098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=3\n",
    "y_test = np.array([0,1,2, 1, 2])\n",
    "\n",
    "one_hot_encode_index(y_test, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f0c34d",
   "metadata": {},
   "source": [
    "### Define Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb0b0f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    \"\"\"Calculates the accuracy of a batch of predictions\"\"\"\n",
    "    return np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_true, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9d170f",
   "metadata": {},
   "source": [
    "## Defining Regularization \n",
    "\n",
    "Regularization methods are used to reduce generalization errors, With L1 and L2 regularization we calculate a penalty that we add to the loss if weight and biases are large. We want to see many neurons contribute to the evaluation rather than a few having a large impact.\n",
    "\n",
    "L1 weight regularization: \n",
    "\n",
    "$$ L_{1w} = \\lambda\\sum_m|w_m| $$\n",
    "\n",
    "L1 bias regularization: \n",
    "\n",
    "$$ L_{1b} = \\lambda\\sum_n|b_n| $$\n",
    "\n",
    "L2 weight regularization: \n",
    "\n",
    "$$ L_{2w} = \\lambda\\sum w^2_m $$\n",
    "\n",
    "L2 bias regularization: \n",
    "\n",
    "$$ L_{2b} = \\lambda\\sum_n b^2_n $$\n",
    "\n",
    "Overall loss:\n",
    "\n",
    "$$ \\text{Loss} = \\text{DataLoss} + L_{1w} + L_{1b} + L_{2w} + L_{2b} $$\n",
    "\n",
    "\n",
    "To implement these changes we will need to modify the Linear Layer class.\n",
    "\n",
    "(note: for backpropagation see backpropagation notebook)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b81ddc",
   "metadata": {},
   "source": [
    "## Integration Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e599d501",
   "metadata": {},
   "source": [
    "### Test with SDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b157cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Loss:1.099, Reg loss: 1.099, (0.000), accuracy:0.360\n",
      "Epoch:100, Loss:1.052, Reg loss: 1.055, (0.003), accuracy:0.447\n",
      "Epoch:200, Loss:0.986, Reg loss: 1.009, (0.024), accuracy:0.500\n",
      "Epoch:300, Loss:0.870, Reg loss: 0.910, (0.040), accuracy:0.570\n",
      "Epoch:400, Loss:0.881, Reg loss: 0.964, (0.083), accuracy:0.597\n",
      "Epoch:500, Loss:0.718, Reg loss: 0.798, (0.080), accuracy:0.657\n",
      "Epoch:600, Loss:0.872, Reg loss: 0.957, (0.085), accuracy:0.567\n",
      "Epoch:700, Loss:0.620, Reg loss: 0.711, (0.090), accuracy:0.700\n",
      "Epoch:800, Loss:0.579, Reg loss: 0.673, (0.095), accuracy:0.743\n",
      "Epoch:900, Loss:0.590, Reg loss: 0.684, (0.094), accuracy:0.740\n",
      "Epoch:1000, Loss:0.646, Reg loss: 0.769, (0.123), accuracy:0.683\n",
      "Epoch:1100, Loss:0.536, Reg loss: 0.639, (0.103), accuracy:0.773\n",
      "Epoch:1200, Loss:0.556, Reg loss: 0.656, (0.100), accuracy:0.747\n",
      "Epoch:1300, Loss:0.537, Reg loss: 0.638, (0.102), accuracy:0.747\n",
      "Epoch:1400, Loss:0.519, Reg loss: 0.619, (0.100), accuracy:0.790\n",
      "Epoch:1500, Loss:0.502, Reg loss: 0.608, (0.106), accuracy:0.780\n",
      "Epoch:1600, Loss:0.504, Reg loss: 0.604, (0.099), accuracy:0.773\n",
      "Epoch:1700, Loss:0.491, Reg loss: 0.591, (0.100), accuracy:0.783\n",
      "Epoch:1800, Loss:0.490, Reg loss: 0.588, (0.098), accuracy:0.797\n",
      "Epoch:1900, Loss:0.488, Reg loss: 0.586, (0.099), accuracy:0.800\n",
      "Epoch:2000, Loss:0.473, Reg loss: 0.569, (0.096), accuracy:0.817\n",
      "Epoch:2100, Loss:0.489, Reg loss: 0.584, (0.094), accuracy:0.763\n",
      "Epoch:2200, Loss:0.478, Reg loss: 0.571, (0.093), accuracy:0.800\n",
      "Epoch:2300, Loss:0.478, Reg loss: 0.571, (0.092), accuracy:0.767\n",
      "Epoch:2400, Loss:0.489, Reg loss: 0.581, (0.093), accuracy:0.800\n",
      "Epoch:2500, Loss:0.457, Reg loss: 0.548, (0.092), accuracy:0.803\n",
      "Epoch:2600, Loss:0.452, Reg loss: 0.543, (0.092), accuracy:0.810\n",
      "Epoch:2700, Loss:0.440, Reg loss: 0.532, (0.092), accuracy:0.833\n",
      "Epoch:2800, Loss:0.442, Reg loss: 0.560, (0.118), accuracy:0.823\n",
      "Epoch:2900, Loss:0.410, Reg loss: 0.515, (0.105), accuracy:0.823\n",
      "Epoch:3000, Loss:0.407, Reg loss: 0.506, (0.099), accuracy:0.827\n",
      "Epoch:3100, Loss:0.403, Reg loss: 0.500, (0.097), accuracy:0.833\n",
      "Epoch:3200, Loss:0.400, Reg loss: 0.495, (0.095), accuracy:0.830\n",
      "Epoch:3300, Loss:0.397, Reg loss: 0.492, (0.095), accuracy:0.833\n",
      "Epoch:3400, Loss:0.394, Reg loss: 0.488, (0.094), accuracy:0.830\n",
      "Epoch:3500, Loss:0.391, Reg loss: 0.485, (0.094), accuracy:0.823\n",
      "Epoch:3600, Loss:0.385, Reg loss: 0.479, (0.094), accuracy:0.840\n",
      "Epoch:3700, Loss:0.389, Reg loss: 0.482, (0.094), accuracy:0.827\n",
      "Epoch:3800, Loss:0.380, Reg loss: 0.474, (0.094), accuracy:0.840\n",
      "Epoch:3900, Loss:0.392, Reg loss: 0.485, (0.093), accuracy:0.830\n",
      "Epoch:4000, Loss:0.386, Reg loss: 0.479, (0.092), accuracy:0.827\n",
      "Epoch:4100, Loss:0.380, Reg loss: 0.472, (0.092), accuracy:0.850\n",
      "Epoch:4200, Loss:0.373, Reg loss: 0.464, (0.092), accuracy:0.830\n",
      "Epoch:4300, Loss:0.382, Reg loss: 0.473, (0.091), accuracy:0.833\n",
      "Epoch:4400, Loss:0.370, Reg loss: 0.461, (0.091), accuracy:0.840\n",
      "Epoch:4500, Loss:0.370, Reg loss: 0.460, (0.090), accuracy:0.830\n",
      "Epoch:4600, Loss:0.370, Reg loss: 0.459, (0.090), accuracy:0.837\n",
      "Epoch:4700, Loss:0.367, Reg loss: 0.456, (0.089), accuracy:0.833\n",
      "Epoch:4800, Loss:0.367, Reg loss: 0.456, (0.089), accuracy:0.833\n",
      "Epoch:4900, Loss:0.372, Reg loss: 0.460, (0.088), accuracy:0.840\n",
      "Epoch:5000, Loss:0.364, Reg loss: 0.453, (0.088), accuracy:0.850\n",
      "Epoch:5100, Loss:0.361, Reg loss: 0.449, (0.088), accuracy:0.847\n",
      "Epoch:5200, Loss:0.364, Reg loss: 0.452, (0.087), accuracy:0.833\n",
      "Epoch:5300, Loss:0.360, Reg loss: 0.447, (0.087), accuracy:0.853\n",
      "Epoch:5400, Loss:0.360, Reg loss: 0.447, (0.087), accuracy:0.843\n",
      "Epoch:5500, Loss:0.360, Reg loss: 0.446, (0.086), accuracy:0.837\n",
      "Epoch:5600, Loss:0.359, Reg loss: 0.445, (0.086), accuracy:0.840\n",
      "Epoch:5700, Loss:0.357, Reg loss: 0.443, (0.086), accuracy:0.837\n",
      "Epoch:5800, Loss:0.357, Reg loss: 0.442, (0.085), accuracy:0.863\n",
      "Epoch:5900, Loss:0.355, Reg loss: 0.440, (0.085), accuracy:0.850\n",
      "Epoch:6000, Loss:0.357, Reg loss: 0.442, (0.085), accuracy:0.843\n",
      "Epoch:6100, Loss:0.356, Reg loss: 0.440, (0.084), accuracy:0.857\n",
      "Epoch:6200, Loss:0.353, Reg loss: 0.437, (0.084), accuracy:0.840\n",
      "Epoch:6300, Loss:0.352, Reg loss: 0.436, (0.084), accuracy:0.843\n",
      "Epoch:6400, Loss:0.352, Reg loss: 0.435, (0.084), accuracy:0.843\n",
      "Epoch:6500, Loss:0.351, Reg loss: 0.435, (0.083), accuracy:0.850\n",
      "Epoch:6600, Loss:0.350, Reg loss: 0.434, (0.083), accuracy:0.843\n",
      "Epoch:6700, Loss:0.350, Reg loss: 0.433, (0.083), accuracy:0.853\n",
      "Epoch:6800, Loss:0.350, Reg loss: 0.432, (0.083), accuracy:0.850\n",
      "Epoch:6900, Loss:0.350, Reg loss: 0.432, (0.083), accuracy:0.860\n",
      "Epoch:7000, Loss:0.348, Reg loss: 0.431, (0.082), accuracy:0.850\n",
      "Epoch:7100, Loss:0.348, Reg loss: 0.430, (0.082), accuracy:0.847\n",
      "Epoch:7200, Loss:0.347, Reg loss: 0.429, (0.082), accuracy:0.847\n",
      "Epoch:7300, Loss:0.347, Reg loss: 0.428, (0.082), accuracy:0.843\n",
      "Epoch:7400, Loss:0.346, Reg loss: 0.428, (0.082), accuracy:0.850\n",
      "Epoch:7500, Loss:0.346, Reg loss: 0.427, (0.081), accuracy:0.850\n",
      "Epoch:7600, Loss:0.346, Reg loss: 0.427, (0.081), accuracy:0.850\n",
      "Epoch:7700, Loss:0.345, Reg loss: 0.426, (0.081), accuracy:0.850\n",
      "Epoch:7800, Loss:0.345, Reg loss: 0.425, (0.081), accuracy:0.843\n",
      "Epoch:7900, Loss:0.343, Reg loss: 0.424, (0.081), accuracy:0.843\n",
      "Epoch:8000, Loss:0.343, Reg loss: 0.424, (0.081), accuracy:0.853\n",
      "Epoch:8100, Loss:0.343, Reg loss: 0.423, (0.081), accuracy:0.847\n",
      "Epoch:8200, Loss:0.342, Reg loss: 0.422, (0.080), accuracy:0.847\n",
      "Epoch:8300, Loss:0.342, Reg loss: 0.422, (0.080), accuracy:0.853\n",
      "Epoch:8400, Loss:0.341, Reg loss: 0.421, (0.080), accuracy:0.847\n",
      "Epoch:8500, Loss:0.341, Reg loss: 0.421, (0.080), accuracy:0.850\n",
      "Epoch:8600, Loss:0.341, Reg loss: 0.421, (0.080), accuracy:0.847\n",
      "Epoch:8700, Loss:0.340, Reg loss: 0.420, (0.080), accuracy:0.853\n",
      "Epoch:8800, Loss:0.340, Reg loss: 0.420, (0.079), accuracy:0.850\n",
      "Epoch:8900, Loss:0.339, Reg loss: 0.419, (0.079), accuracy:0.857\n",
      "Epoch:9000, Loss:0.339, Reg loss: 0.419, (0.079), accuracy:0.853\n",
      "Epoch:9100, Loss:0.339, Reg loss: 0.418, (0.079), accuracy:0.850\n",
      "Epoch:9200, Loss:0.338, Reg loss: 0.417, (0.079), accuracy:0.850\n",
      "Epoch:9300, Loss:0.338, Reg loss: 0.417, (0.079), accuracy:0.850\n",
      "Epoch:9400, Loss:0.338, Reg loss: 0.416, (0.079), accuracy:0.853\n",
      "Epoch:9500, Loss:0.338, Reg loss: 0.416, (0.079), accuracy:0.850\n",
      "Epoch:9600, Loss:0.337, Reg loss: 0.416, (0.079), accuracy:0.857\n",
      "Epoch:9700, Loss:0.337, Reg loss: 0.415, (0.078), accuracy:0.853\n",
      "Epoch:9800, Loss:0.337, Reg loss: 0.415, (0.078), accuracy:0.853\n",
      "Epoch:9900, Loss:0.336, Reg loss: 0.415, (0.078), accuracy:0.853\n",
      "Epoch:10000, Loss:0.336, Reg loss: 0.414, (0.078), accuracy:0.850\n"
     ]
    }
   ],
   "source": [
    "nnfs.init()\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "y = one_hot_encode_index(y, 3)\n",
    "\n",
    "# Initializing Network Components \n",
    "relu = ReLU()\n",
    "softmax = Softmax()\n",
    "cce_loss = CategoricalCrossEntropyLoss\n",
    "optimizer = SDG(decay=1e-3, momentum=0.9)\n",
    "linear1 = LinearLayer(2, 64, lambda_l2_weight=5e-4, lambda_l2_bias=5e-4)\n",
    "linear2 = LinearLayer(64, 3)\n",
    "\n",
    "update_layers = [linear1, linear2]\n",
    "\n",
    "n_epochs = 10000\n",
    "\n",
    "for epoch in range(n_epochs + 1):\n",
    "    # Forward Pass\n",
    "    linear1.forward(X) \n",
    "    relu.forward(linear1.output)\n",
    "    linear2.forward(relu.output)\n",
    "    y_pred = softmax.forward(linear2.output)\n",
    "\n",
    "    # Calculating loss and regularized loss  \n",
    "    loss = cce_loss.forward(y_pred, y)\n",
    "    # Regularised loss\n",
    "    rl = sum([l.regularization_loss() for l in update_layers if hasattr(l, 'regularization_loss')])\n",
    "    regularized_loss = loss + rl\n",
    "\n",
    "    #Calculating accuracy \n",
    "    acc = accuracy(y_pred, y) \n",
    "\n",
    "    # Printing results\n",
    "    if not epoch % 100:\n",
    "        print(f\"Epoch:{epoch}, Loss:{loss:.3f}, Reg loss: {regularized_loss:.3f}, ({rl:.3f}), accuracy:{acc:.3f}\")\n",
    "\n",
    "    # Backward pass\n",
    "    softmax.backward(y_pred, y)\n",
    "    linear2.backward(softmax.d_soft)\n",
    "    relu.backward(linear2.d_x)\n",
    "    linear1.backward(relu.d_relu)\n",
    "\n",
    "    # Optimization Step\n",
    "    optimizer.update(update_layers)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d282f129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vallidation: Accuracy: 0.790, loss: 0.689\n"
     ]
    }
   ],
   "source": [
    "# Validating the model \n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "y_test = one_hot_encode_index(y_test, 3)\n",
    "\n",
    "linear1.forward(X_test) \n",
    "relu.forward(linear1.output)\n",
    "linear2.forward(relu.output)\n",
    "y_pred = softmax.forward(linear2.output)\n",
    "\n",
    "# Calculating loss and accuracy \n",
    "loss = cce_loss.forward(y_pred, y_test)\n",
    "acc = accuracy(y_pred, y_test) \n",
    "\n",
    "print(f'Vallidation: Accuracy: {acc:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7e554e",
   "metadata": {},
   "source": [
    "### Test with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "13409958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Loss:1.099, Reg loss: 1.099, (0.000), accuracy:0.353\n",
      "Epoch:100, Loss:0.664, Reg loss: 0.721, (0.056), accuracy:0.740\n",
      "Epoch:200, Loss:0.435, Reg loss: 0.531, (0.097), accuracy:0.860\n",
      "Epoch:300, Loss:0.342, Reg loss: 0.449, (0.107), accuracy:0.873\n",
      "Epoch:400, Loss:0.296, Reg loss: 0.403, (0.107), accuracy:0.913\n",
      "Epoch:500, Loss:0.259, Reg loss: 0.364, (0.105), accuracy:0.930\n",
      "Epoch:600, Loss:0.236, Reg loss: 0.337, (0.101), accuracy:0.930\n",
      "Epoch:700, Loss:0.222, Reg loss: 0.320, (0.098), accuracy:0.937\n",
      "Epoch:800, Loss:0.209, Reg loss: 0.312, (0.103), accuracy:0.943\n",
      "Epoch:900, Loss:0.199, Reg loss: 0.298, (0.099), accuracy:0.947\n",
      "Epoch:1000, Loss:0.193, Reg loss: 0.290, (0.096), accuracy:0.947\n",
      "Epoch:1100, Loss:0.188, Reg loss: 0.282, (0.094), accuracy:0.947\n",
      "Epoch:1200, Loss:0.182, Reg loss: 0.274, (0.091), accuracy:0.953\n",
      "Epoch:1300, Loss:0.179, Reg loss: 0.268, (0.089), accuracy:0.953\n",
      "Epoch:1400, Loss:0.174, Reg loss: 0.261, (0.087), accuracy:0.953\n",
      "Epoch:1500, Loss:0.169, Reg loss: 0.255, (0.086), accuracy:0.953\n",
      "Epoch:1600, Loss:0.165, Reg loss: 0.249, (0.084), accuracy:0.960\n",
      "Epoch:1700, Loss:0.161, Reg loss: 0.244, (0.082), accuracy:0.957\n",
      "Epoch:1800, Loss:0.159, Reg loss: 0.239, (0.081), accuracy:0.957\n",
      "Epoch:1900, Loss:0.154, Reg loss: 0.233, (0.079), accuracy:0.967\n",
      "Epoch:2000, Loss:0.151, Reg loss: 0.229, (0.078), accuracy:0.957\n",
      "Epoch:2100, Loss:0.155, Reg loss: 0.231, (0.076), accuracy:0.947\n",
      "Epoch:2200, Loss:0.143, Reg loss: 0.218, (0.075), accuracy:0.967\n",
      "Epoch:2300, Loss:0.141, Reg loss: 0.215, (0.074), accuracy:0.963\n",
      "Epoch:2400, Loss:0.139, Reg loss: 0.212, (0.072), accuracy:0.967\n",
      "Epoch:2500, Loss:0.136, Reg loss: 0.207, (0.071), accuracy:0.967\n",
      "Epoch:2600, Loss:0.138, Reg loss: 0.208, (0.070), accuracy:0.967\n",
      "Epoch:2700, Loss:0.133, Reg loss: 0.203, (0.069), accuracy:0.967\n",
      "Epoch:2800, Loss:0.129, Reg loss: 0.197, (0.068), accuracy:0.963\n",
      "Epoch:2900, Loss:0.128, Reg loss: 0.195, (0.067), accuracy:0.967\n",
      "Epoch:3000, Loss:0.126, Reg loss: 0.192, (0.066), accuracy:0.963\n",
      "Epoch:3100, Loss:0.130, Reg loss: 0.195, (0.065), accuracy:0.960\n",
      "Epoch:3200, Loss:0.638, Reg loss: 0.712, (0.074), accuracy:0.757\n",
      "Epoch:3300, Loss:0.125, Reg loss: 0.201, (0.077), accuracy:0.963\n",
      "Epoch:3400, Loss:0.122, Reg loss: 0.196, (0.074), accuracy:0.967\n",
      "Epoch:3500, Loss:0.121, Reg loss: 0.193, (0.072), accuracy:0.967\n",
      "Epoch:3600, Loss:0.120, Reg loss: 0.191, (0.070), accuracy:0.967\n",
      "Epoch:3700, Loss:0.120, Reg loss: 0.189, (0.069), accuracy:0.967\n",
      "Epoch:3800, Loss:0.119, Reg loss: 0.187, (0.068), accuracy:0.967\n",
      "Epoch:3900, Loss:0.118, Reg loss: 0.185, (0.066), accuracy:0.967\n",
      "Epoch:4000, Loss:0.118, Reg loss: 0.183, (0.065), accuracy:0.967\n",
      "Epoch:4100, Loss:0.117, Reg loss: 0.181, (0.064), accuracy:0.967\n",
      "Epoch:4200, Loss:0.116, Reg loss: 0.180, (0.064), accuracy:0.967\n",
      "Epoch:4300, Loss:0.115, Reg loss: 0.178, (0.063), accuracy:0.967\n",
      "Epoch:4400, Loss:0.114, Reg loss: 0.176, (0.062), accuracy:0.963\n",
      "Epoch:4500, Loss:0.114, Reg loss: 0.175, (0.061), accuracy:0.963\n",
      "Epoch:4600, Loss:0.113, Reg loss: 0.173, (0.061), accuracy:0.967\n",
      "Epoch:4700, Loss:0.114, Reg loss: 0.174, (0.060), accuracy:0.963\n",
      "Epoch:4800, Loss:0.111, Reg loss: 0.170, (0.059), accuracy:0.963\n",
      "Epoch:4900, Loss:0.116, Reg loss: 0.175, (0.059), accuracy:0.953\n"
     ]
    }
   ],
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "y = one_hot_encode_index(y, 3)\n",
    "\n",
    "# Initializing Network Components \n",
    "relu = ReLU()\n",
    "softmax = Softmax()\n",
    "cce_loss = CategoricalCrossEntropyLoss\n",
    "optimizer = Adam(learning_rate=0.02, decay=5e-7)\n",
    "linear1 = LinearLayer(2, 256, lambda_l2_weight=5e-4, lambda_l2_bias=5e-4)\n",
    "linear2 = LinearLayer(256, 3)\n",
    "\n",
    "update_layers = [linear1, linear2]\n",
    "\n",
    "n_epochs = 10000\n",
    "\n",
    "for epoch in range(n_epochs + 1):\n",
    "    # Forward Pass\n",
    "    linear1.forward(X) \n",
    "    relu.forward(linear1.output)\n",
    "    linear2.forward(relu.output)\n",
    "    y_pred = softmax.forward(linear2.output)\n",
    "\n",
    "    # Calculating loss and regularized loss  \n",
    "    loss = cce_loss.forward(y_pred, y)\n",
    "    # Regularised loss\n",
    "    rl = sum([l.regularization_loss() for l in update_layers if hasattr(l, 'regularization_loss')])\n",
    "    regularized_loss = loss + rl\n",
    "\n",
    "    #Calculating accuracy \n",
    "    acc = accuracy(y_pred, y) \n",
    "\n",
    "    # Printing results\n",
    "    if not epoch % 100:\n",
    "        print(f\"Epoch:{epoch}, Loss:{loss:.3f}, Reg loss: {regularized_loss:.3f}, ({rl:.3f}), accuracy:{acc:.3f}\")\n",
    "\n",
    "    # Backward pass\n",
    "    softmax.backward(y_pred, y)\n",
    "    linear2.backward(softmax.d_soft)\n",
    "    relu.backward(linear2.d_x)\n",
    "    linear1.backward(relu.d_relu)\n",
    "\n",
    "    # Optimization Step\n",
    "    optimizer.update(update_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68645370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vallidation: Accuracy: 0.853, loss: 0.382\n"
     ]
    }
   ],
   "source": [
    "# Validating the model \n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "y_test = one_hot_encode_index(y_test, 3)\n",
    "\n",
    "linear1.forward(X_test) \n",
    "relu.forward(linear1.output)\n",
    "linear2.forward(relu.output)\n",
    "y_pred = softmax.forward(linear2.output)\n",
    "\n",
    "# Calculating loss and accuracy \n",
    "loss = cce_loss.forward(y_pred, y_test)\n",
    "acc = accuracy(y_pred, y_test) \n",
    "\n",
    "print(f'Vallidation: Accuracy: {acc:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120132a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
