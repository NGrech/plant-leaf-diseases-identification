{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ee38e8",
   "metadata": {},
   "source": [
    "# Implementation of 9-layer CNN Homebrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b2250c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model' from '..\\\\src\\\\model.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General imports \n",
    "import sys\n",
    "import os \n",
    "sys.path.insert(1, os.path.join(os.pardir, 'src'))\n",
    "from itertools import product\n",
    "\n",
    "# Data imports\n",
    "import torch\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Homebrew imports \n",
    "import model\n",
    "from utils import one_hot_encode_index\n",
    "from optimizers import Adam\n",
    "from activations import Softmax, ReLU\n",
    "from layers import Dropout, LinearLayer, ConvolutionLayer, PoolingLayer, FlattenLayer\n",
    "from loss import CategoricalCrossEntropyLoss\n",
    "\n",
    "## TESTING \n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dbb6cc",
   "metadata": {},
   "source": [
    "## Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d69b9a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(128),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "                                      ])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(129),\n",
    "                                      transforms.CenterCrop(128),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "                                    ])\n",
    "\n",
    "# setting up data loaders\n",
    "data_dir = os.path.join(os.pardir, 'data', 'Plant_leave_diseases_224')\n",
    "\n",
    "train_data = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=train_transforms)\n",
    "test_data = datasets.ImageFolder(os.path.join(data_dir, 'validation'), transform=test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51af43ca",
   "metadata": {},
   "source": [
    "### Train config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d59977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs \n",
    "config = {\n",
    "    'max_epochs': 100,\n",
    "    'learning_rate': 0.003,\n",
    "    'resolution': 128,\n",
    "    'name': 'CNN_9layer_homebrew'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce71e6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3b2d3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture: \n",
      "\t (0): ConvolutionLayer (Trainable: False)\n",
      "\t (1): ReLU (Trainable: False)\n",
      "\t (2): PoolingLayer (Trainable: False)\n",
      "\t (3): ConvolutionLayer (Trainable: False)\n",
      "\t (4): ReLU (Trainable: False)\n",
      "\t (5): PoolingLayer (Trainable: False)\n",
      "\t (6): ConvolutionLayer (Trainable: False)\n",
      "\t (7): ReLU (Trainable: False)\n",
      "\t (8): PoolingLayer (Trainable: False)\n",
      "\t (9): FlattenLayer (Trainable: False)\n",
      "\t (10): LinearLayer (Trainable: True)\n",
      "\t (11): ReLU (Trainable: False)\n",
      "\t (12): LinearLayer (Trainable: True)\n",
      "\t (13): ReLU (Trainable: False)\n",
      "\t (14): LinearLayer (Trainable: True)\n",
      "\t (15): Softmax (Trainable: False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mdl = model.Model(Adam(learning_rate=config['learning_rate']),\n",
    "                      CategoricalCrossEntropyLoss())\n",
    "\n",
    "# Config early stop \n",
    "mdl.add_early_stop(25)\n",
    "\n",
    "mdl.set_save_config(model_name=config['name'], save_path=os.path.join(os.pardir, 'model'))\n",
    "\n",
    "# Defining architecture \n",
    "\n",
    "mdl.set_sequence([\n",
    "                    ConvolutionLayer(3, 32, 3),\n",
    "                    ReLU(),\n",
    "                    PoolingLayer(32, 2),\n",
    "                    ConvolutionLayer(32, 16, 3),\n",
    "                    ReLU(),\n",
    "                    PoolingLayer(16, 2),\n",
    "                    ConvolutionLayer(16, 8, 3),\n",
    "                    ReLU(),\n",
    "                    PoolingLayer(8, 2),\n",
    "                    FlattenLayer(),\n",
    "                    LinearLayer(1800, 1568),\n",
    "                    ReLU(),\n",
    "                    LinearLayer(1568, 128),\n",
    "                    ReLU(),\n",
    "                    LinearLayer(128, 39),\n",
    "                    Softmax()\n",
    "                ])\n",
    "print(mdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb41934",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14df8745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch: 1 ===\n",
      "Step: 0/865, accuracy0.016, loss3.876, learning rate 0.0030000 \n",
      "Step: 5/865, accuracy0.016, loss3.643, learning rate 0.0030000 \n",
      "Step: 10/865, accuracy0.109, loss3.618, learning rate 0.0030000 \n",
      "Step: 15/865, accuracy0.062, loss3.557, learning rate 0.0030000 \n",
      "Step: 20/865, accuracy0.094, loss3.479, learning rate 0.0030000 \n",
      "Step: 25/865, accuracy0.016, loss3.547, learning rate 0.0030000 \n",
      "Step: 30/865, accuracy0.125, loss3.513, learning rate 0.0030000 \n",
      "Step: 35/865, accuracy0.094, loss3.574, learning rate 0.0030000 \n",
      "Step: 40/865, accuracy0.094, loss3.431, learning rate 0.0030000 \n",
      "Step: 45/865, accuracy0.156, loss3.395, learning rate 0.0030000 \n",
      "Step: 50/865, accuracy0.109, loss3.561, learning rate 0.0030000 \n",
      "Step: 55/865, accuracy0.078, loss3.518, learning rate 0.0030000 \n",
      "Step: 60/865, accuracy0.062, loss3.433, learning rate 0.0030000 \n",
      "Step: 65/865, accuracy0.031, loss3.457, learning rate 0.0030000 \n",
      "Step: 70/865, accuracy0.109, loss3.381, learning rate 0.0030000 \n",
      "Step: 75/865, accuracy0.062, loss3.656, learning rate 0.0030000 \n",
      "Step: 80/865, accuracy0.094, loss3.494, learning rate 0.0030000 \n",
      "Step: 85/865, accuracy0.062, loss3.532, learning rate 0.0030000 \n",
      "Step: 90/865, accuracy0.016, loss3.483, learning rate 0.0030000 \n",
      "Step: 95/865, accuracy0.094, loss3.457, learning rate 0.0030000 \n",
      "Step: 100/865, accuracy0.031, loss3.492, learning rate 0.0030000 \n",
      "Step: 105/865, accuracy0.094, loss3.537, learning rate 0.0030000 \n",
      "Step: 110/865, accuracy0.094, loss3.499, learning rate 0.0030000 \n",
      "Step: 115/865, accuracy0.109, loss3.563, learning rate 0.0030000 \n",
      "Step: 120/865, accuracy0.062, loss3.392, learning rate 0.0030000 \n",
      "Step: 125/865, accuracy0.078, loss3.544, learning rate 0.0030000 \n",
      "Step: 130/865, accuracy0.078, loss3.601, learning rate 0.0030000 \n",
      "Step: 135/865, accuracy0.203, loss3.404, learning rate 0.0030000 \n",
      "Step: 140/865, accuracy0.094, loss3.502, learning rate 0.0030000 \n",
      "Step: 145/865, accuracy0.078, loss3.497, learning rate 0.0030000 \n",
      "Step: 150/865, accuracy0.109, loss3.444, learning rate 0.0030000 \n",
      "Step: 155/865, accuracy0.078, loss3.298, learning rate 0.0030000 \n",
      "Step: 160/865, accuracy0.109, loss3.534, learning rate 0.0030000 \n",
      "Step: 165/865, accuracy0.031, loss3.609, learning rate 0.0030000 \n",
      "Step: 170/865, accuracy0.078, loss3.528, learning rate 0.0030000 \n",
      "Step: 175/865, accuracy0.094, loss3.521, learning rate 0.0030000 \n",
      "Step: 180/865, accuracy0.094, loss3.470, learning rate 0.0030000 \n",
      "Step: 185/865, accuracy0.047, loss3.601, learning rate 0.0030000 \n",
      "Step: 190/865, accuracy0.172, loss3.351, learning rate 0.0030000 \n",
      "Step: 195/865, accuracy0.078, loss3.522, learning rate 0.0030000 \n",
      "Step: 200/865, accuracy0.062, loss3.497, learning rate 0.0030000 \n",
      "Step: 205/865, accuracy0.078, loss3.531, learning rate 0.0030000 \n",
      "Step: 210/865, accuracy0.109, loss3.483, learning rate 0.0030000 \n",
      "Step: 215/865, accuracy0.062, loss3.527, learning rate 0.0030000 \n",
      "Step: 220/865, accuracy0.062, loss3.626, learning rate 0.0030000 \n",
      "Step: 225/865, accuracy0.078, loss3.654, learning rate 0.0030000 \n",
      "Step: 230/865, accuracy0.062, loss3.422, learning rate 0.0030000 \n",
      "Step: 235/865, accuracy0.094, loss3.599, learning rate 0.0030000 \n",
      "Step: 240/865, accuracy0.047, loss3.620, learning rate 0.0030000 \n",
      "Step: 245/865, accuracy0.000, loss3.574, learning rate 0.0030000 \n",
      "Step: 250/865, accuracy0.125, loss3.407, learning rate 0.0030000 \n",
      "Step: 255/865, accuracy0.078, loss3.584, learning rate 0.0030000 \n",
      "Step: 260/865, accuracy0.125, loss3.518, learning rate 0.0030000 \n",
      "Step: 265/865, accuracy0.078, loss3.527, learning rate 0.0030000 \n",
      "Step: 270/865, accuracy0.047, loss3.499, learning rate 0.0030000 \n",
      "Step: 275/865, accuracy0.031, loss3.535, learning rate 0.0030000 \n",
      "Step: 280/865, accuracy0.062, loss3.531, learning rate 0.0030000 \n",
      "Step: 285/865, accuracy0.078, loss3.411, learning rate 0.0030000 \n",
      "Step: 290/865, accuracy0.094, loss3.545, learning rate 0.0030000 \n",
      "Step: 295/865, accuracy0.062, loss3.488, learning rate 0.0030000 \n",
      "Step: 300/865, accuracy0.078, loss3.529, learning rate 0.0030000 \n",
      "Step: 305/865, accuracy0.156, loss3.464, learning rate 0.0030000 \n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"Plant Leaf Disease\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param('framework', 'homebrew')\n",
    "    mlflow.log_param('data_split', '90/10')\n",
    "    mlflow.log_param('type', '9_layer_CNN')\n",
    "    mlflow.log_params(config)\n",
    "    mdl.train_with_loader(train_loader, epochs=config['max_epochs'], validation_loader=validation_loader, cls_count=39, log_freq=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
