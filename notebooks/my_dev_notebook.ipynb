{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ad873e",
   "metadata": {},
   "source": [
    "# Dev notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef02f46",
   "metadata": {},
   "source": [
    "### Getting started on the feed forward neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9fbb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnfs \n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc42b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='brg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85db83e2",
   "metadata": {},
   "source": [
    "####  Defining layers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7fe0c9",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- I have chosen to use a similar naming convention to that used by pytorch (why reinvent the wheel), this has the benefit of ensuring that when we compar|e implementations the architecture of the networks is the same. \n",
    "- since we intend to use ReLU as one of our activation functions we will use the He weight initialization method as described in https://arxiv.org/abs/1502.01852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b830b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer():\n",
    "    \"\"\"Linear transformation layer of the type o = ixW + b,\n",
    "    \n",
    "    where I is the incoming vector, W is the layers weight matrix, b is bias vector and o is the dot product of the \n",
    "    i and W plus the bias\n",
    "    \n",
    "    Args:\n",
    "        in_features (int): the size of the input features \n",
    "        out_features (int): the size of the output features\n",
    "        \n",
    "    Attributes:\n",
    "        weights (np_array) numpy array of in_features x n_neurons\n",
    "        biases  (np_array) numpy array of 1 x n_neurons\n",
    "        inputs  (np_array) numpy array of latest batch of inputs\n",
    "        d_w     (np_array) The current gradients with respect to the weights \n",
    "        d_x     (np_array) The current gradients with respect to the inputs\n",
    "        d_b     (np_array) The current gradients with respect to the biases\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features) -> None:\n",
    "        # initializing weights and biases \n",
    "        self.weights = np.random.normal(0.0, np.sqrt(2/in_features), (in_features, out_features))\n",
    "        self.bias = np.zeros((1, out_features))\n",
    "        # initializing attributes needed for backwards \n",
    "        self.inputs = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Saving inputs for backward step\n",
    "        self.inputs = inputs\n",
    "        return np.dot(inputs, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, d_vals):\n",
    "        \"\"\"Backpropagation  of the linear function\n",
    "\n",
    "        Args:\n",
    "            d_vals (np_array) array of derivatives from the previous layer/function.\n",
    "        \"\"\"\n",
    "        self.d_w = np.dot(self.inputs.T, d_vals)\n",
    "        self.d_x = np.dot(d_vals, self.weights.T)\n",
    "        self.d_b = np.sum(d_vals, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ea363f",
   "metadata": {},
   "source": [
    "#### Testing Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca4160",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = LinearLayer(2, 3)\n",
    "linear1.forward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8da5f5",
   "metadata": {},
   "source": [
    "#### Activation Functions - ReLu\n",
    "\n",
    "$$y = \\begin{cases}\n",
    "   x &x> 0 \\\\\n",
    "   0 & otherwise\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d0ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"Applies Rectified linear Unit function to vector.\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        # initializing attributes needed for backwards \n",
    "        self.inputs = None\n",
    "        self.d_relu = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # storing inputs needed for backwards \n",
    "        self.inputs = x\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    def backward(self, d_vals):\n",
    "        self.d_relu = d_vals.copy()\n",
    "        self.d_relu[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa60fd7a",
   "metadata": {},
   "source": [
    "#### Testing ReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = [-2, 3, 4, 0, 0.1, -44]\n",
    "activator = ReLU()\n",
    "activator.forward(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc75ac1",
   "metadata": {},
   "source": [
    "#### Activation Functions -Softmax\n",
    "\n",
    "$$\\text{softmax}(x)_i = \\frac{exp(x_i)}{\\sum_{j}^{ }exp(x_j))}$$\n",
    "\n",
    "The soft max represents the confidence score for each output class and adds up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "966f1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"Applies Softmax function to input matrix.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.confidence_scores = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # exponenets of each value\n",
    "        exp_vals = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        exp_sum = np.sum(exp_vals, axis=1, keepdims=True)\n",
    "        # Normalization to get the proabilities \n",
    "        self.confidence_scores = exp_vals/exp_sum\n",
    "        return self.confidence_scores\n",
    "\n",
    "    def backward(self, d_vals):\n",
    "        # Initialize array for gradients wrt to inputs\n",
    "        self.d_soft = np.zeros_like(d_vals)\n",
    "        \n",
    "        _iter = enumerate(zip(self.confidence_scores, d_vals))\n",
    "        for i, conf_score, d_val in _iter:\n",
    "            # Flatten confidence scores\n",
    "            cs = conf_score.reshape(-1, 1)\n",
    "            # Find the Jacobian matrix of the output \n",
    "            j_matrix = np.diagflat(cs) - np.dot(cs, cs.T)\n",
    "            # get the gradient \n",
    "            self.d_soft[i] = np.dot(j_matrix, d_val)\n",
    "    \n",
    "    def combo_backward(self, y_pred, y_true):\n",
    "        \"\"\"Does a the combined backward pass for CCE & Softmax as a single, faster step.\"\"\"\n",
    "        n = len(y_pred)\n",
    "\n",
    "        # Getting descrete vals from one hot encoding \n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "        \n",
    "        self.d_soft = y_pred.copy()\n",
    "        self.d_soft[range(n), y_true] -= 1\n",
    "        self.d_soft = self.d_soft/n\n",
    "        return self.d_soft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e8095e",
   "metadata": {},
   "source": [
    "#### Testing Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c5cd2c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.11513104e-19, 5.74952226e-19, 1.00000000e+00]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = Softmax()\n",
    "softmax.forward([[1,2,44]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b31bb",
   "metadata": {},
   "source": [
    "#### Calculating Loss - Categorical Cross-Entropy\n",
    "\n",
    "$$ L_i = -\\sum_j y_{i,j}\\log(\\hat{y}_{i,j}) $$\n",
    "\n",
    "With taking one hot encoding into account we can simplify this down to:\n",
    "\n",
    "$$ L_i = -y_{i,k}\\log(\\hat{y}_{i,k}) $$\n",
    "\n",
    "where K is the index of the correct class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7645a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalCrossEntropyLoss:\n",
    "    \"\"\"Calculates the CCE loss for a given set of predictions.\n",
    "    This method expect a softmax output and one-hot encoded label mask\n",
    "    \n",
    "    y_pred (np_array): matrix of confidence scores of the prediction\n",
    "    y_true (np_array): matrix of one-hot encoded true lables of the classes\n",
    "    \"\"\"\n",
    "    def forward(y_pred, y_true):\n",
    "        # Clipping and applying one hot encoded labels as mask \n",
    "        # to zero out scores corresponding to incorrect classes\n",
    "        # We clip to make sure that none of the reaming classes are 0 or \n",
    "        # exactly 1 \n",
    "        corrected = np.sum(np.clip(y_pred, 1e-7, 1-1e-7)*y_true, axis=1)\n",
    "        # Taking the -ve log of the remaining confidence scores \n",
    "        negative_log = -np.log(corrected)\n",
    "        return np.mean(negative_log)\n",
    "    \n",
    "    def backward(y_pred, y_true):\n",
    "        \"\"\"Backpropagation  of the CCE Loss\n",
    "\n",
    "        Args:\n",
    "            y_pred (np_array) array of predictions.\n",
    "            y_true (np_array) array of correct labels.\n",
    "        \"\"\"\n",
    "        return (-y_true/y_pred)/len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6704da26",
   "metadata": {},
   "source": [
    "#### Testing CCE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0abd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([[0.7, 0.1, 0.2], [0.1,0.5,0.4],[0.02,0.9,0.08]])\n",
    "y_true = np.array([[1,0,0], [0,1,0], [0,1,0]])\n",
    "\n",
    "loss_function = CategoricalCrossEntropyLoss\n",
    "loss_function.forward(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee48592",
   "metadata": {},
   "source": [
    "#### One-hot encoding function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649a4d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_index(y, n):\n",
    "    return np.eye(n)[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cfee58",
   "metadata": {},
   "source": [
    "#### Testing one hot masker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da547098",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=3\n",
    "y_test = np.array([0,1,2, 1, 2])\n",
    "\n",
    "one_hot_encode_index(y_test, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b81ddc",
   "metadata": {},
   "source": [
    "#### Integration Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204856ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = ReLU\n",
    "softmax = Softmax\n",
    "cce_loss = CategoricalCrossEntropyLoss\n",
    "\n",
    "linear1 = LinearLayer(2, 3)\n",
    "linear2 = LinearLayer(3, 3)\n",
    "\n",
    "out1 = relu.forward(linear1.forward(X))\n",
    "out2 = softmax.forward(linear2.forward(out1))\n",
    "\n",
    "out2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22564cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cce_loss.forward(out2, one_hot_encode_index(y, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7691a0a1",
   "metadata": {},
   "source": [
    "#### Backpropagation - simplified \n",
    "\n",
    "Backpropagation through ReLU, based on the example in NNFS to ensure a solid understanding of the underlying math (partial diff and chain rule) and mechanisms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulating a forward pass \n",
    "\n",
    "x = [1.0, -2.0, 3.0]\n",
    "w = [-3.0, -1.0, 2.0]\n",
    "b = 1.0\n",
    "\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0]* w[0]\n",
    "xw1 = x[1]* w[1]\n",
    "xw2 = x[2]* w[2]\n",
    "\n",
    "# Summing weights and bias\n",
    "z = xw0 + xw1 + xw2 +b\n",
    "\n",
    "# applying relu\n",
    "y = max(z, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9da502",
   "metadata": {},
   "source": [
    "If we represent the forward pass as a function we can say:\n",
    "\n",
    "$$\\text{ReLU}\\left(\\sum[\\text{inputs}\\cdotp\\text{weights}]+\\text{bias}\\right)$$\n",
    "\n",
    "We now need to find the partial derivatives of all the function for all the parameters. For example if we wanted to know the effect that w0 had on the outcome we woul need to know:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial x_0}\\left[\\text{ReLU}\\left(\\sum[\\text{inputs}\\cdotp\\text{weights}]+\\text{bias}\\right)\\right] = \\frac{d \\text{ReLU()}}{d \\text{sum()}}\\cdot\\frac{\\partial\\text{sum()}}{\\partial mul(x_0,w_0)}\\cdot\\frac{\\partial mul(x_0,w_0)}{\\partial x_0} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee3a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The backward pass\n",
    "\n",
    "# derivative from previous layers\n",
    "d_val = 1.0\n",
    "\n",
    "# the derivative of relu wrt z \n",
    "d_relu_dz = d_val * (0,1)[z>0] # == i if z> 0, else 0 \n",
    "\n",
    "# Recall the derivative of a sum opperator os always 1 \n",
    "# derivative of the sum wrt x_n*w_n \n",
    "d_sum_dxwn = 1\n",
    "d_relu_dxw0 = d_relu_dz * d_sum_dxwn\n",
    "d_relu_dxw1 = d_relu_dz * d_sum_dxwn\n",
    "d_relu_dxw2 = d_relu_dz * d_sum_dxwn\n",
    "\n",
    "# derivative of the sum wrt b (bias) \n",
    "d_sum_db = 1\n",
    "d_relu_db = d_relu_dz * d_sum_db\n",
    "\n",
    "# Recall the derivative of a product is whateve input is being multiplied \n",
    "d_mul_dx0 = w[0]\n",
    "d_mul_dx1 = w[1]\n",
    "d_mul_dx2 = w[2]\n",
    "d_relu_dx0 = d_mul_dx0 * d_relu_dxw0\n",
    "d_relu_dx1 = d_mul_dx1 * d_relu_dxw2\n",
    "d_relu_dx2 = d_mul_dx2 * d_relu_dxw2\n",
    "\n",
    "d_mul_dw0 = x[0]\n",
    "d_mul_dw1 = x[1]\n",
    "d_mul_dw2 = x[2]\n",
    "d_relu_dw0 = d_mul_dw0 * d_relu_dxw0\n",
    "d_relu_dw1 = d_mul_dw1 * d_relu_dxw1\n",
    "d_relu_dw2 = d_mul_dw2 * d_relu_dxw2\n",
    "\n",
    "# Simplifying the above we can rewrite as:\n",
    "d_relu_dx0 = d_val * (0,1)[z>0] * w[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized code for the backward pass \n",
    "# (yes, variables are being shadowed but it is okay this section is just for learning an not final code)\n",
    "d_val = 1.0\n",
    "\n",
    "d_x = [d_val*(0,1)[z>0]*_w for _w in w] # the derivative of the previous layer * d of relu * the corresponding weight for the input\n",
    "d_w = [d_val*(0,1)[z>0]*_x for _x in x] # the derivative of the previous layer * d of relu * the corresponding input for the weight\n",
    "d_b = d_val * (0,1)[z>0] # the derivative of the previous layer * d of relu (the derivative of the sum will always be 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79a1ecc",
   "metadata": {},
   "source": [
    "#### Backpropagation - A layer of neurons \n",
    "\n",
    "Considering multiple neurons in a layer rather than just one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8254d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy passed in grads from previous layer \n",
    "d_val = np.array([[1.,1.,1.]])\n",
    "\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "# gradient for first input  \n",
    "d_x0 = sum([weights[0][i] * d_val[0][i] for i in range(weights.shape[1])])\n",
    "d_x1 = sum([weights[1][i] * d_val[0][i] for i in range(weights.shape[1])])\n",
    "d_x2 = sum([weights[2][i] * d_val[0][i] for i in range(weights.shape[1])])\n",
    "d_x3 = sum([weights[3][i] * d_val[0][i] for i in range(weights.shape[1])])\n",
    "d_x = np.array([d_x0, d_x1, d_x2, d_x3])\n",
    "d_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42407a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizing the above code and accounting for batches of samples we get:\n",
    "\n",
    "d_val = np.array([[1., 1., 1.],\n",
    "                    [2., 2., 2.],\n",
    "                    [3., 3., 3.]])\n",
    "\n",
    "d_x = np.dot(d_val, weights.T)\n",
    "d_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae954e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the gradients wrt the weights we consider the input values \n",
    "inputs = np.array([[1, 2, 3, 2.5],\n",
    "                    [2., 5., -1., 2],\n",
    "                    [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "d_w = np.dot(inputs.T, d_val)\n",
    "d_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e04be64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the derivative of the bias \n",
    "d_b = np.sum(d_val, axis=0, keepdims=True)\n",
    "d_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336adb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output for the linear component \n",
    "z = np.array([[1,2,-3,-4], [2,-7,-1,3], [-1, 2,5,-1]])\n",
    "d_val = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "# Calcuting the derivative of Relu \n",
    "d_relu = np.zeros(z.shape)\n",
    "d_relu[z>0] = 1\n",
    "d_relu *= d_val\n",
    "d_relu "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a7372",
   "metadata": {},
   "source": [
    "At this point I will go back and update the Linear Layer and the Relu with backward code.\n",
    "\n",
    "#### Backpropagation of CCE Loss\n",
    "\n",
    "We find that the derivative of CCE loss to be:\n",
    "\n",
    "$$ \\frac{\\delta L_i}{\\hat{y_{i,j}}} = -\\frac{y_{i,j}}{\\hat{y_{i,j}}} $$\n",
    "\n",
    "I will now add this directly to the function\n",
    "\n",
    "#### Backpropagation of Softmax activation\n",
    "\n",
    "$$ \\frac{\\partial S_{i,j}}{\\partial Z_{i,k}} = S_{i,j} \\cdot (\\delta_{j,k} - S_{i,k})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d706b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test implementation \n",
    "\n",
    "# Softmax output \n",
    "so = [0.7, 0.1, 0.2] \n",
    "so = np.array(so).reshape(-1, 1)\n",
    "np.diagflat(so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f46ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(so, so.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ddf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diagflat(so) - np.dot(so, so.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd76ba1",
   "metadata": {},
   "source": [
    "#### CCE and Softmax combined derivatives \n",
    "\n",
    "Combining the derivations of CCE and SOftmax together will alow is to solve them in a simpler and faster way "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39630749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
