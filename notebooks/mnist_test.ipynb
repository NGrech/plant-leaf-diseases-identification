{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mnist Fashion Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import urllib.request\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import sys\n",
    "sys.path.insert(1, r'../src')\n",
    "\n",
    "\n",
    "import model\n",
    "\n",
    "from utils import one_hot_encode_index\n",
    "\n",
    "from optimizers import Adam\n",
    "from activations import Softmax, ReLU\n",
    "from layers import Dropout, LinearLayer\n",
    "from loss import CategoricalCrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://nnfs.io/datasets/fashion_mnist_images.zip and saving as fashion_mnist_images.zip...\n",
      "Unzipping images...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "URL = 'https://nnfs.io/datasets/fashion_mnist_images.zip'\n",
    "FILE = 'fashion_mnist_images.zip'\n",
    "FOLDER = 'fashion_mnist_images'\n",
    "\n",
    "if not os.path.isfile(FILE):\n",
    "    print(f'Downloading {URL} and saving as {FILE}...')\n",
    "    urllib.request.urlretrieve(URL, FILE)\n",
    "\n",
    "print('Unzipping images...')\n",
    "with ZipFile(FILE) as zip_images:\n",
    "    zip_images.extractall(FOLDER)\n",
    "\n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a MNIST dataset\n",
    "def load_mnist_dataset(dataset, path):\n",
    "    # Scan all the directories and create a list of labels\n",
    "    labels = os.listdir(os.path.join(path, dataset))\n",
    "    # Create lists for samples and labels\n",
    "    X = []\n",
    "    y = []\n",
    "    # For each label folder\n",
    "    for label in labels:\n",
    "        # And for each image in given folder\n",
    "        for file in os.listdir(os.path.join(path, dataset, label)):\n",
    "            # Read the image\n",
    "            image = cv2.imread(os.path.join(path, dataset, label, file), cv2.IMREAD_UNCHANGED)\n",
    "            # And append it and a label to the lists\n",
    "            X.append(image)\n",
    "            y.append(label)\n",
    "    # Convert the data to proper numpy arrays and return\n",
    "    return np.array(X), np.array(y).astype('uint8')\n",
    "\n",
    "\n",
    "def create_data_mnist(path):\n",
    "    # Load both sets separately\n",
    "    X, y = load_mnist_dataset('train', path)\n",
    "    X_test, y_test = load_mnist_dataset('test', path)\n",
    "    y = one_hot_encode_index(y, 10)\n",
    "    y_test = one_hot_encode_index(y_test, 10)\n",
    "    \n",
    "    # And return all the data\n",
    "    return X, y, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_test, y_test = create_data_mnist('fashion_mnist_images')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling between -1 & 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_img(v):\n",
    "    return (v - 127.5) /127.5\n",
    "\n",
    "X = scale_img(X)\n",
    "X_test = scale_img(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(v):\n",
    "    return v.reshape(v.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorize(X)\n",
    "X_test = vectorize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle Training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = np.array(range(X.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "X = X[keys]\n",
    "y = y[keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch: 1 ===\n",
      "Step: 0/469, accuracy0.062, loss3.100, learning rate 0.0010000 \n",
      "Step: 100/469, accuracy0.711, loss0.723, learning rate 0.0009950 \n",
      "Step: 200/469, accuracy0.742, loss0.600, learning rate 0.0009901 \n",
      "Step: 300/469, accuracy0.766, loss0.609, learning rate 0.0009852 \n",
      "Step: 400/469, accuracy0.789, loss0.500, learning rate 0.0009804 \n",
      "Step: 468/469, accuracy0.833, loss0.454, learning rate 0.0009771 \n",
      "Epoch: 0/10, accuracy0.750, loss0.694, learning rate 0.001\n",
      "--Validation--\n",
      "Validation : Accuracy: 0.828, Loss: 0.473\n",
      "=== Epoch: 2 ===\n",
      "Step: 0/469, accuracy0.812, loss0.578, learning rate 0.0009771 \n",
      "Step: 100/469, accuracy0.766, loss0.502, learning rate 0.0009723 \n",
      "Step: 200/469, accuracy0.867, loss0.424, learning rate 0.0009676 \n",
      "Step: 300/469, accuracy0.820, loss0.464, learning rate 0.0009630 \n",
      "Step: 400/469, accuracy0.812, loss0.417, learning rate 0.0009584 \n",
      "Step: 468/469, accuracy0.885, loss0.332, learning rate 0.0009552 \n",
      "Epoch: 1/10, accuracy0.815, loss0.512, learning rate 0.001\n",
      "--Validation--\n",
      "Validation : Accuracy: 0.841, Loss: 0.434\n",
      "=== Epoch: 3 ===\n",
      "Step: 0/469, accuracy0.758, loss0.571, learning rate 0.0009552 \n",
      "Step: 100/469, accuracy0.812, loss0.477, learning rate 0.0009507 \n",
      "Step: 200/469, accuracy0.844, loss0.395, learning rate 0.0009462 \n",
      "Step: 300/469, accuracy0.797, loss0.551, learning rate 0.0009417 \n",
      "Step: 400/469, accuracy0.812, loss0.403, learning rate 0.0009373 \n",
      "Step: 468/469, accuracy0.865, loss0.379, learning rate 0.0009343 \n",
      "Epoch: 2/10, accuracy0.829, loss0.472, learning rate 0.001\n",
      "--Validation--\n",
      "Validation : Accuracy: 0.852, Loss: 0.411\n",
      "=== Epoch: 4 ===\n",
      "Step: 0/469, accuracy0.828, loss0.486, learning rate 0.0009343 \n",
      "Step: 100/469, accuracy0.828, loss0.478, learning rate 0.0009299 \n",
      "Step: 200/469, accuracy0.898, loss0.343, learning rate 0.0009256 \n",
      "Step: 300/469, accuracy0.812, loss0.458, learning rate 0.0009214 \n",
      "Step: 400/469, accuracy0.836, loss0.437, learning rate 0.0009171 \n",
      "Step: 468/469, accuracy0.823, loss0.374, learning rate 0.0009143 \n",
      "Epoch: 3/10, accuracy0.840, loss0.445, learning rate 0.001\n",
      "--Validation--\n",
      "Validation : Accuracy: 0.856, Loss: 0.397\n",
      "=== Epoch: 5 ===\n",
      "Step: 0/469, accuracy0.836, loss0.477, learning rate 0.0009142 \n",
      "Step: 100/469, accuracy0.773, loss0.454, learning rate 0.0009101 \n",
      "Step: 200/469, accuracy0.906, loss0.268, learning rate 0.0009060 \n",
      "Step: 300/469, accuracy0.836, loss0.506, learning rate 0.0009019 \n",
      "Step: 400/469, accuracy0.844, loss0.375, learning rate 0.0008978 \n",
      "Step: 468/469, accuracy0.896, loss0.310, learning rate 0.0008951 \n",
      "Epoch: 4/10, accuracy0.845, loss0.430, learning rate 0.001\n",
      "--Validation--\n",
      "Validation : Accuracy: 0.856, Loss: 0.399\n",
      "=== Epoch: 6 ===\n",
      "Step: 0/469, accuracy0.812, loss0.486, learning rate 0.0008951 \n",
      "Step: 100/469, accuracy0.812, loss0.446, learning rate 0.0008911 \n",
      "Step: 200/469, accuracy0.859, loss0.413, learning rate 0.0008871 \n",
      "Step: 300/469, accuracy0.820, loss0.476, learning rate 0.0008832 \n",
      "Step: 400/469, accuracy0.883, loss0.281, learning rate 0.0008793 \n",
      "Step: 468/469, accuracy0.865, loss0.389, learning rate 0.0008767 \n",
      "Epoch: 5/10, accuracy0.849, loss0.417, learning rate 0.001\n",
      "--Validation--\n",
      "Validation : Accuracy: 0.860, Loss: 0.382\n",
      "=== Epoch: 7 ===\n",
      "Step: 0/469, accuracy0.820, loss0.461, learning rate 0.0008767 \n",
      "Step: 100/469, accuracy0.836, loss0.452, learning rate 0.0008728 \n",
      "Step: 200/469, accuracy0.891, loss0.299, learning rate 0.0008690 \n",
      "Step: 300/469, accuracy0.828, loss0.410, learning rate 0.0008653 \n",
      "Step: 400/469, accuracy0.820, loss0.388, learning rate 0.0008615 \n",
      "Step: 468/469, accuracy0.833, loss0.328, learning rate 0.0008590 \n",
      "Epoch: 6/10, accuracy0.853, loss0.404, learning rate 0.001\n",
      "--Validation--\n",
      "Validation : Accuracy: 0.862, Loss: 0.375\n",
      "=== Epoch: 8 ===\n",
      "Step: 0/469, accuracy0.828, loss0.345, learning rate 0.0008590 \n",
      "Step: 100/469, accuracy0.852, loss0.389, learning rate 0.0008553 \n",
      "Step: 200/469, accuracy0.891, loss0.308, learning rate 0.0008517 \n",
      "Step: 300/469, accuracy0.852, loss0.401, learning rate 0.0008481 \n",
      "Step: 400/469, accuracy0.859, loss0.378, learning rate 0.0008445 \n",
      "Step: 468/469, accuracy0.844, loss0.308, learning rate 0.0008421 \n",
      "Epoch: 7/10, accuracy0.857, loss0.393, learning rate 0.001\n",
      "--Validation--\n",
      "Validation : Accuracy: 0.866, Loss: 0.366\n",
      "=== Epoch: 9 ===\n",
      "Step: 0/469, accuracy0.844, loss0.486, learning rate 0.0008420 \n",
      "Step: 100/469, accuracy0.812, loss0.467, learning rate 0.0008385 \n",
      "Step: 200/469, accuracy0.914, loss0.254, learning rate 0.0008350 \n",
      "Step: 300/469, accuracy0.844, loss0.407, learning rate 0.0008315 \n",
      "Step: 400/469, accuracy0.898, loss0.254, learning rate 0.0008281 \n",
      "Step: 468/469, accuracy0.833, loss0.311, learning rate 0.0008258 \n",
      "Epoch: 8/10, accuracy0.858, loss0.387, learning rate 0.001\n",
      "--Validation--\n",
      "Validation : Accuracy: 0.860, Loss: 0.376\n",
      "=== Epoch: 10 ===\n",
      "Step: 0/469, accuracy0.875, loss0.394, learning rate 0.0008257 \n",
      "Step: 100/469, accuracy0.836, loss0.401, learning rate 0.0008223 \n",
      "Step: 200/469, accuracy0.891, loss0.275, learning rate 0.0008190 \n",
      "Step: 300/469, accuracy0.828, loss0.379, learning rate 0.0008156 \n",
      "Step: 400/469, accuracy0.883, loss0.330, learning rate 0.0008123 \n",
      "Step: 468/469, accuracy0.833, loss0.298, learning rate 0.0008101 \n",
      "Epoch: 9/10, accuracy0.860, loss0.383, learning rate 0.001\n",
      "--Validation--\n",
      "Validation : Accuracy: 0.863, Loss: 0.371\n",
      "=== Epoch: 11 ===\n",
      "Step: 0/469, accuracy0.836, loss0.516, learning rate 0.0008100 \n",
      "Step: 100/469, accuracy0.852, loss0.351, learning rate 0.0008068 \n",
      "Step: 200/469, accuracy0.906, loss0.257, learning rate 0.0008035 \n",
      "Step: 300/469, accuracy0.875, loss0.394, learning rate 0.0008003 \n",
      "Step: 400/469, accuracy0.844, loss0.355, learning rate 0.0007971 \n",
      "Step: 468/469, accuracy0.875, loss0.285, learning rate 0.0007950 \n",
      "Epoch: 10/10, accuracy0.863, loss0.373, learning rate 0.001\n",
      "--Validation--\n",
      "Validation : Accuracy: 0.867, Loss: 0.359\n"
     ]
    }
   ],
   "source": [
    "## TESTING \n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "##\n",
    "\n",
    "cce_loss = CategoricalCrossEntropyLoss()\n",
    "optimizer = Adam(decay=5e-5)\n",
    "\n",
    "my_model = model.Model(optimizer, cce_loss)\n",
    "\n",
    "my_model.set_sequence([\n",
    "    LinearLayer(X.shape[1], 128),\n",
    "    ReLU(),\n",
    "    Dropout(0.5),\n",
    "    LinearLayer(128, 128),\n",
    "    ReLU(),\n",
    "    LinearLayer(128, 10),\n",
    "    Softmax()\n",
    "])\n",
    "my_model.train(X, y, epochs=10, batch_size=128, validation=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation : Accuracy: 0.867, Loss: 0.359\n"
     ]
    }
   ],
   "source": [
    "my_model.evaluate(X_test, y_test, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5734eafda3611ac4cda01bcae68efeb7a0fca20a99bcb218ad2b0b92f206bf5c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('plantLeafDiseases': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
