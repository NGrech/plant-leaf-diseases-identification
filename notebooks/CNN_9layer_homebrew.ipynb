{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ee38e8",
   "metadata": {},
   "source": [
    "# Implementation of 9-layer CNN Homebrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b2250c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model' from '..\\\\src\\\\model.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General imports \n",
    "import sys\n",
    "import os \n",
    "sys.path.insert(1, os.path.join(os.pardir, 'src'))\n",
    "from itertools import product\n",
    "\n",
    "# Data imports\n",
    "import torch\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Homebrew imports \n",
    "import model\n",
    "from utils import one_hot_encode_index\n",
    "from optimizers import Adam\n",
    "from activations import Softmax, ReLU\n",
    "from layers import Dropout, LinearLayer, ConvolutionLayer, PoolingLayer, FlattenLayer\n",
    "from loss import CategoricalCrossEntropyLoss\n",
    "\n",
    "## TESTING \n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dbb6cc",
   "metadata": {},
   "source": [
    "## Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d69b9a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(128),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "                                      ])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(129),\n",
    "                                      transforms.CenterCrop(128),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "                                    ])\n",
    "\n",
    "# setting up data loaders\n",
    "data_dir = os.path.join(os.pardir, 'data', 'Plant_leave_diseases_224')\n",
    "\n",
    "train_data = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=train_transforms)\n",
    "test_data = datasets.ImageFolder(os.path.join(data_dir, 'validation'), transform=test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51af43ca",
   "metadata": {},
   "source": [
    "### Train config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d59977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs \n",
    "config = {\n",
    "    'max_epochs': 100,\n",
    "    'learning_rate': 0.003,\n",
    "    'resolution': 128,\n",
    "    'name': 'CNN_9layer_homebrew'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce71e6",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3b2d3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture: \n",
      "\t (0): ConvolutionLayer (Trainable: False)\n",
      "\t (1): ReLU (Trainable: False)\n",
      "\t (2): PoolingLayer (Trainable: False)\n",
      "\t (3): ConvolutionLayer (Trainable: False)\n",
      "\t (4): ReLU (Trainable: False)\n",
      "\t (5): PoolingLayer (Trainable: False)\n",
      "\t (6): ConvolutionLayer (Trainable: False)\n",
      "\t (7): ReLU (Trainable: False)\n",
      "\t (8): PoolingLayer (Trainable: False)\n",
      "\t (9): FlattenLayer (Trainable: False)\n",
      "\t (10): LinearLayer (Trainable: True)\n",
      "\t (11): ReLU (Trainable: False)\n",
      "\t (12): LinearLayer (Trainable: True)\n",
      "\t (13): ReLU (Trainable: False)\n",
      "\t (14): LinearLayer (Trainable: True)\n",
      "\t (15): Softmax (Trainable: False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mdl = model.Model(Adam(learning_rate=config['learning_rate']),\n",
    "                      CategoricalCrossEntropyLoss())\n",
    "\n",
    "# Config early stop \n",
    "mdl.add_early_stop(25)\n",
    "\n",
    "mdl.set_save_config(model_name=config['name'], save_path=os.path.join(os.pardir, 'model'))\n",
    "\n",
    "# Defining architecture \n",
    "\n",
    "mdl.set_sequence([\n",
    "                    ConvolutionLayer(3, 32, 3),\n",
    "                    ReLU(),\n",
    "                    PoolingLayer(32, 2),\n",
    "                    ConvolutionLayer(32, 16, 3),\n",
    "                    ReLU(),\n",
    "                    PoolingLayer(16, 2),\n",
    "                    ConvolutionLayer(16, 8, 3),\n",
    "                    ReLU(),\n",
    "                    PoolingLayer(8, 2),\n",
    "                    FlattenLayer(),\n",
    "                    LinearLayer(1800, 1568),\n",
    "                    ReLU(),\n",
    "                    LinearLayer(1568, 128),\n",
    "                    ReLU(),\n",
    "                    LinearLayer(128, 39),\n",
    "                    Softmax()\n",
    "                ])\n",
    "print(mdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb41934",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14df8745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch: 1 ===\n",
      "Step: 0/865, accuracy0.016, loss3.876, learning rate 0.0030000 \n",
      "Step: 5/865, accuracy0.016, loss3.643, learning rate 0.0030000 \n",
      "Step: 10/865, accuracy0.109, loss3.618, learning rate 0.0030000 \n",
      "Step: 15/865, accuracy0.062, loss3.557, learning rate 0.0030000 \n",
      "Step: 20/865, accuracy0.094, loss3.479, learning rate 0.0030000 \n",
      "Step: 25/865, accuracy0.016, loss3.547, learning rate 0.0030000 \n",
      "Step: 30/865, accuracy0.125, loss3.513, learning rate 0.0030000 \n",
      "Step: 35/865, accuracy0.094, loss3.574, learning rate 0.0030000 \n",
      "Step: 40/865, accuracy0.094, loss3.431, learning rate 0.0030000 \n",
      "Step: 45/865, accuracy0.156, loss3.395, learning rate 0.0030000 \n",
      "Step: 50/865, accuracy0.109, loss3.561, learning rate 0.0030000 \n",
      "Step: 55/865, accuracy0.078, loss3.518, learning rate 0.0030000 \n",
      "Step: 60/865, accuracy0.062, loss3.433, learning rate 0.0030000 \n",
      "Step: 65/865, accuracy0.031, loss3.457, learning rate 0.0030000 \n",
      "Step: 70/865, accuracy0.109, loss3.381, learning rate 0.0030000 \n",
      "Step: 75/865, accuracy0.062, loss3.656, learning rate 0.0030000 \n",
      "Step: 80/865, accuracy0.094, loss3.494, learning rate 0.0030000 \n",
      "Step: 85/865, accuracy0.062, loss3.532, learning rate 0.0030000 \n",
      "Step: 90/865, accuracy0.016, loss3.483, learning rate 0.0030000 \n",
      "Step: 95/865, accuracy0.094, loss3.457, learning rate 0.0030000 \n",
      "Step: 100/865, accuracy0.031, loss3.492, learning rate 0.0030000 \n",
      "Step: 105/865, accuracy0.094, loss3.537, learning rate 0.0030000 \n",
      "Step: 110/865, accuracy0.094, loss3.499, learning rate 0.0030000 \n",
      "Step: 115/865, accuracy0.109, loss3.563, learning rate 0.0030000 \n",
      "Step: 120/865, accuracy0.062, loss3.392, learning rate 0.0030000 \n",
      "Step: 125/865, accuracy0.078, loss3.544, learning rate 0.0030000 \n",
      "Step: 130/865, accuracy0.078, loss3.601, learning rate 0.0030000 \n",
      "Step: 135/865, accuracy0.203, loss3.404, learning rate 0.0030000 \n",
      "Step: 140/865, accuracy0.094, loss3.502, learning rate 0.0030000 \n",
      "Step: 145/865, accuracy0.078, loss3.497, learning rate 0.0030000 \n",
      "Step: 150/865, accuracy0.109, loss3.444, learning rate 0.0030000 \n",
      "Step: 155/865, accuracy0.078, loss3.298, learning rate 0.0030000 \n",
      "Step: 160/865, accuracy0.109, loss3.534, learning rate 0.0030000 \n",
      "Step: 165/865, accuracy0.031, loss3.609, learning rate 0.0030000 \n",
      "Step: 170/865, accuracy0.078, loss3.528, learning rate 0.0030000 \n",
      "Step: 175/865, accuracy0.094, loss3.521, learning rate 0.0030000 \n",
      "Step: 180/865, accuracy0.094, loss3.470, learning rate 0.0030000 \n",
      "Step: 185/865, accuracy0.047, loss3.601, learning rate 0.0030000 \n",
      "Step: 190/865, accuracy0.172, loss3.351, learning rate 0.0030000 \n",
      "Step: 195/865, accuracy0.078, loss3.522, learning rate 0.0030000 \n",
      "Step: 200/865, accuracy0.062, loss3.497, learning rate 0.0030000 \n",
      "Step: 205/865, accuracy0.078, loss3.531, learning rate 0.0030000 \n",
      "Step: 210/865, accuracy0.109, loss3.483, learning rate 0.0030000 \n",
      "Step: 215/865, accuracy0.062, loss3.527, learning rate 0.0030000 \n",
      "Step: 220/865, accuracy0.062, loss3.626, learning rate 0.0030000 \n",
      "Step: 225/865, accuracy0.078, loss3.654, learning rate 0.0030000 \n",
      "Step: 230/865, accuracy0.062, loss3.422, learning rate 0.0030000 \n",
      "Step: 235/865, accuracy0.094, loss3.599, learning rate 0.0030000 \n",
      "Step: 240/865, accuracy0.047, loss3.620, learning rate 0.0030000 \n",
      "Step: 245/865, accuracy0.000, loss3.574, learning rate 0.0030000 \n",
      "Step: 250/865, accuracy0.125, loss3.407, learning rate 0.0030000 \n",
      "Step: 255/865, accuracy0.078, loss3.584, learning rate 0.0030000 \n",
      "Step: 260/865, accuracy0.125, loss3.518, learning rate 0.0030000 \n",
      "Step: 265/865, accuracy0.078, loss3.527, learning rate 0.0030000 \n",
      "Step: 270/865, accuracy0.047, loss3.499, learning rate 0.0030000 \n",
      "Step: 275/865, accuracy0.031, loss3.535, learning rate 0.0030000 \n",
      "Step: 280/865, accuracy0.062, loss3.531, learning rate 0.0030000 \n",
      "Step: 285/865, accuracy0.078, loss3.411, learning rate 0.0030000 \n",
      "Step: 290/865, accuracy0.094, loss3.545, learning rate 0.0030000 \n",
      "Step: 295/865, accuracy0.062, loss3.488, learning rate 0.0030000 \n",
      "Step: 300/865, accuracy0.078, loss3.529, learning rate 0.0030000 \n",
      "Step: 305/865, accuracy0.156, loss3.464, learning rate 0.0030000 \n",
      "Step: 310/865, accuracy0.125, loss3.362, learning rate 0.0030000 \n",
      "Step: 315/865, accuracy0.062, loss3.482, learning rate 0.0030000 \n",
      "Step: 320/865, accuracy0.141, loss3.429, learning rate 0.0030000 \n",
      "Step: 325/865, accuracy0.047, loss3.540, learning rate 0.0030000 \n",
      "Step: 330/865, accuracy0.062, loss3.539, learning rate 0.0030000 \n",
      "Step: 335/865, accuracy0.141, loss3.341, learning rate 0.0030000 \n",
      "Step: 340/865, accuracy0.078, loss3.507, learning rate 0.0030000 \n",
      "Step: 345/865, accuracy0.125, loss3.436, learning rate 0.0030000 \n",
      "Step: 350/865, accuracy0.047, loss3.361, learning rate 0.0030000 \n",
      "Step: 355/865, accuracy0.062, loss3.434, learning rate 0.0030000 \n",
      "Step: 360/865, accuracy0.094, loss3.471, learning rate 0.0030000 \n",
      "Step: 365/865, accuracy0.125, loss3.492, learning rate 0.0030000 \n",
      "Step: 370/865, accuracy0.125, loss3.427, learning rate 0.0030000 \n",
      "Step: 375/865, accuracy0.094, loss3.390, learning rate 0.0030000 \n",
      "Step: 380/865, accuracy0.094, loss3.327, learning rate 0.0030000 \n",
      "Step: 385/865, accuracy0.062, loss3.544, learning rate 0.0030000 \n",
      "Step: 390/865, accuracy0.125, loss3.527, learning rate 0.0030000 \n",
      "Step: 395/865, accuracy0.156, loss3.507, learning rate 0.0030000 \n",
      "Step: 400/865, accuracy0.156, loss3.414, learning rate 0.0030000 \n",
      "Step: 405/865, accuracy0.062, loss3.406, learning rate 0.0030000 \n",
      "Step: 410/865, accuracy0.094, loss3.593, learning rate 0.0030000 \n",
      "Step: 415/865, accuracy0.062, loss3.317, learning rate 0.0030000 \n",
      "Step: 420/865, accuracy0.094, loss3.547, learning rate 0.0030000 \n",
      "Step: 425/865, accuracy0.125, loss3.558, learning rate 0.0030000 \n",
      "Step: 430/865, accuracy0.047, loss3.627, learning rate 0.0030000 \n",
      "Step: 435/865, accuracy0.094, loss3.458, learning rate 0.0030000 \n",
      "Step: 440/865, accuracy0.047, loss3.410, learning rate 0.0030000 \n",
      "Step: 445/865, accuracy0.109, loss3.498, learning rate 0.0030000 \n",
      "Step: 450/865, accuracy0.078, loss3.436, learning rate 0.0030000 \n",
      "Step: 455/865, accuracy0.062, loss3.408, learning rate 0.0030000 \n",
      "Step: 460/865, accuracy0.156, loss3.481, learning rate 0.0030000 \n",
      "Step: 465/865, accuracy0.109, loss3.560, learning rate 0.0030000 \n",
      "Step: 470/865, accuracy0.125, loss3.486, learning rate 0.0030000 \n",
      "Step: 475/865, accuracy0.094, loss3.457, learning rate 0.0030000 \n",
      "Step: 480/865, accuracy0.078, loss3.451, learning rate 0.0030000 \n",
      "Step: 485/865, accuracy0.094, loss3.351, learning rate 0.0030000 \n",
      "Step: 490/865, accuracy0.094, loss3.475, learning rate 0.0030000 \n",
      "Step: 495/865, accuracy0.062, loss3.600, learning rate 0.0030000 \n",
      "Step: 500/865, accuracy0.031, loss3.495, learning rate 0.0030000 \n",
      "Step: 505/865, accuracy0.125, loss3.300, learning rate 0.0030000 \n",
      "Step: 510/865, accuracy0.062, loss3.556, learning rate 0.0030000 \n",
      "Step: 515/865, accuracy0.141, loss3.500, learning rate 0.0030000 \n",
      "Step: 520/865, accuracy0.109, loss3.600, learning rate 0.0030000 \n",
      "Step: 525/865, accuracy0.109, loss3.417, learning rate 0.0030000 \n",
      "Step: 530/865, accuracy0.078, loss3.618, learning rate 0.0030000 \n",
      "Step: 535/865, accuracy0.109, loss3.575, learning rate 0.0030000 \n",
      "Step: 540/865, accuracy0.047, loss3.396, learning rate 0.0030000 \n",
      "Step: 545/865, accuracy0.109, loss3.478, learning rate 0.0030000 \n",
      "Step: 550/865, accuracy0.109, loss3.507, learning rate 0.0030000 \n",
      "Step: 555/865, accuracy0.047, loss3.529, learning rate 0.0030000 \n",
      "Step: 560/865, accuracy0.094, loss3.615, learning rate 0.0030000 \n",
      "Step: 565/865, accuracy0.125, loss3.422, learning rate 0.0030000 \n",
      "Step: 570/865, accuracy0.125, loss3.543, learning rate 0.0030000 \n",
      "Step: 575/865, accuracy0.031, loss3.544, learning rate 0.0030000 \n",
      "Step: 580/865, accuracy0.125, loss3.509, learning rate 0.0030000 \n",
      "Step: 585/865, accuracy0.125, loss3.497, learning rate 0.0030000 \n",
      "Step: 590/865, accuracy0.188, loss3.451, learning rate 0.0030000 \n",
      "Step: 595/865, accuracy0.078, loss3.464, learning rate 0.0030000 \n",
      "Step: 600/865, accuracy0.125, loss3.471, learning rate 0.0030000 \n",
      "Step: 605/865, accuracy0.078, loss3.499, learning rate 0.0030000 \n",
      "Step: 610/865, accuracy0.062, loss3.522, learning rate 0.0030000 \n",
      "Step: 615/865, accuracy0.016, loss3.508, learning rate 0.0030000 \n",
      "Step: 620/865, accuracy0.094, loss3.521, learning rate 0.0030000 \n",
      "Step: 625/865, accuracy0.062, loss3.415, learning rate 0.0030000 \n",
      "Step: 630/865, accuracy0.047, loss3.591, learning rate 0.0030000 \n",
      "Step: 635/865, accuracy0.203, loss3.377, learning rate 0.0030000 \n",
      "Step: 640/865, accuracy0.141, loss3.410, learning rate 0.0030000 \n",
      "Step: 645/865, accuracy0.078, loss3.500, learning rate 0.0030000 \n",
      "Step: 650/865, accuracy0.141, loss3.500, learning rate 0.0030000 \n",
      "Step: 655/865, accuracy0.062, loss3.484, learning rate 0.0030000 \n",
      "Step: 660/865, accuracy0.125, loss3.444, learning rate 0.0030000 \n",
      "Step: 665/865, accuracy0.109, loss3.431, learning rate 0.0030000 \n",
      "Step: 670/865, accuracy0.094, loss3.546, learning rate 0.0030000 \n",
      "Step: 675/865, accuracy0.062, loss3.467, learning rate 0.0030000 \n",
      "Step: 680/865, accuracy0.125, loss3.564, learning rate 0.0030000 \n",
      "Step: 685/865, accuracy0.078, loss3.549, learning rate 0.0030000 \n",
      "Step: 690/865, accuracy0.047, loss3.389, learning rate 0.0030000 \n",
      "Step: 695/865, accuracy0.047, loss3.721, learning rate 0.0030000 \n",
      "Step: 700/865, accuracy0.031, loss3.574, learning rate 0.0030000 \n",
      "Step: 705/865, accuracy0.062, loss3.545, learning rate 0.0030000 \n",
      "Step: 710/865, accuracy0.109, loss3.491, learning rate 0.0030000 \n",
      "Step: 715/865, accuracy0.078, loss3.384, learning rate 0.0030000 \n",
      "Step: 720/865, accuracy0.062, loss3.486, learning rate 0.0030000 \n",
      "Step: 725/865, accuracy0.078, loss3.525, learning rate 0.0030000 \n",
      "Step: 730/865, accuracy0.188, loss3.356, learning rate 0.0030000 \n",
      "Step: 735/865, accuracy0.047, loss3.607, learning rate 0.0030000 \n",
      "Step: 740/865, accuracy0.094, loss3.535, learning rate 0.0030000 \n",
      "Step: 745/865, accuracy0.125, loss3.447, learning rate 0.0030000 \n",
      "Step: 750/865, accuracy0.047, loss3.611, learning rate 0.0030000 \n",
      "Step: 755/865, accuracy0.156, loss3.414, learning rate 0.0030000 \n",
      "Step: 760/865, accuracy0.047, loss3.619, learning rate 0.0030000 \n",
      "Step: 765/865, accuracy0.125, loss3.470, learning rate 0.0030000 \n",
      "Step: 770/865, accuracy0.078, loss3.550, learning rate 0.0030000 \n",
      "Step: 775/865, accuracy0.109, loss3.440, learning rate 0.0030000 \n",
      "Step: 780/865, accuracy0.125, loss3.373, learning rate 0.0030000 \n",
      "Step: 785/865, accuracy0.094, loss3.478, learning rate 0.0030000 \n",
      "Step: 790/865, accuracy0.031, loss3.538, learning rate 0.0030000 \n",
      "Step: 795/865, accuracy0.094, loss3.564, learning rate 0.0030000 \n",
      "Step: 800/865, accuracy0.078, loss3.676, learning rate 0.0030000 \n",
      "Step: 805/865, accuracy0.078, loss3.488, learning rate 0.0030000 \n",
      "Step: 810/865, accuracy0.031, loss3.500, learning rate 0.0030000 \n",
      "Step: 815/865, accuracy0.094, loss3.478, learning rate 0.0030000 \n",
      "Step: 820/865, accuracy0.109, loss3.474, learning rate 0.0030000 \n",
      "Step: 825/865, accuracy0.078, loss3.529, learning rate 0.0030000 \n",
      "Step: 830/865, accuracy0.109, loss3.342, learning rate 0.0030000 \n",
      "Step: 835/865, accuracy0.109, loss3.357, learning rate 0.0030000 \n",
      "Step: 840/865, accuracy0.109, loss3.480, learning rate 0.0030000 \n",
      "Step: 845/865, accuracy0.094, loss3.304, learning rate 0.0030000 \n",
      "Step: 850/865, accuracy0.109, loss3.566, learning rate 0.0030000 \n",
      "Step: 855/865, accuracy0.031, loss3.676, learning rate 0.0030000 \n",
      "Step: 860/865, accuracy0.031, loss3.596, learning rate 0.0030000 \n",
      "Step: 864/865, accuracy0.065, loss3.549, learning rate 0.0030000 \n",
      "Epoch: 1/100, accuracy0.087, loss3.497, learning rate 0.003\n",
      "Estimated reamining runtime: 118 days, 20:26:55.747452\n",
      "--Validation--\n",
      "Validation : Accuracy: 0.087, Loss: 3.490\n",
      "=== Epoch: 2 ===\n",
      "Step: 0/865, accuracy0.125, loss3.405, learning rate 0.0030000 \n",
      "Step: 5/865, accuracy0.078, loss3.544, learning rate 0.0030000 \n",
      "Step: 10/865, accuracy0.062, loss3.649, learning rate 0.0030000 \n",
      "Step: 15/865, accuracy0.062, loss3.530, learning rate 0.0030000 \n",
      "Step: 20/865, accuracy0.047, loss3.504, learning rate 0.0030000 \n",
      "Step: 25/865, accuracy0.125, loss3.506, learning rate 0.0030000 \n",
      "Step: 30/865, accuracy0.094, loss3.474, learning rate 0.0030000 \n",
      "Step: 35/865, accuracy0.094, loss3.436, learning rate 0.0030000 \n",
      "Step: 40/865, accuracy0.094, loss3.492, learning rate 0.0030000 \n",
      "Step: 45/865, accuracy0.047, loss3.552, learning rate 0.0030000 \n",
      "Step: 50/865, accuracy0.062, loss3.446, learning rate 0.0030000 \n",
      "Step: 55/865, accuracy0.109, loss3.549, learning rate 0.0030000 \n",
      "Step: 60/865, accuracy0.156, loss3.356, learning rate 0.0030000 \n",
      "Step: 65/865, accuracy0.109, loss3.507, learning rate 0.0030000 \n",
      "Step: 70/865, accuracy0.094, loss3.577, learning rate 0.0030000 \n",
      "Step: 75/865, accuracy0.109, loss3.549, learning rate 0.0030000 \n",
      "Step: 80/865, accuracy0.141, loss3.473, learning rate 0.0030000 \n",
      "Step: 85/865, accuracy0.125, loss3.522, learning rate 0.0030000 \n",
      "Step: 90/865, accuracy0.125, loss3.472, learning rate 0.0030000 \n",
      "Step: 95/865, accuracy0.078, loss3.528, learning rate 0.0030000 \n",
      "Step: 100/865, accuracy0.094, loss3.424, learning rate 0.0030000 \n",
      "Step: 105/865, accuracy0.062, loss3.644, learning rate 0.0030000 \n",
      "Step: 110/865, accuracy0.062, loss3.542, learning rate 0.0030000 \n",
      "Step: 115/865, accuracy0.156, loss3.433, learning rate 0.0030000 \n",
      "Step: 120/865, accuracy0.062, loss3.570, learning rate 0.0030000 \n",
      "Step: 125/865, accuracy0.078, loss3.519, learning rate 0.0030000 \n",
      "Step: 130/865, accuracy0.047, loss3.393, learning rate 0.0030000 \n",
      "Step: 135/865, accuracy0.031, loss3.592, learning rate 0.0030000 \n",
      "Step: 140/865, accuracy0.094, loss3.451, learning rate 0.0030000 \n",
      "Step: 145/865, accuracy0.047, loss3.504, learning rate 0.0030000 \n",
      "Step: 150/865, accuracy0.078, loss3.483, learning rate 0.0030000 \n",
      "Step: 155/865, accuracy0.062, loss3.375, learning rate 0.0030000 \n",
      "Step: 160/865, accuracy0.078, loss3.426, learning rate 0.0030000 \n",
      "Step: 165/865, accuracy0.125, loss3.346, learning rate 0.0030000 \n",
      "Step: 170/865, accuracy0.031, loss3.432, learning rate 0.0030000 \n",
      "Step: 175/865, accuracy0.172, loss3.441, learning rate 0.0030000 \n",
      "Step: 180/865, accuracy0.109, loss3.431, learning rate 0.0030000 \n",
      "Step: 185/865, accuracy0.062, loss3.631, learning rate 0.0030000 \n",
      "Step: 190/865, accuracy0.047, loss3.409, learning rate 0.0030000 \n",
      "Step: 195/865, accuracy0.125, loss3.306, learning rate 0.0030000 \n",
      "Step: 200/865, accuracy0.062, loss3.482, learning rate 0.0030000 \n",
      "Step: 205/865, accuracy0.047, loss3.586, learning rate 0.0030000 \n",
      "Step: 210/865, accuracy0.078, loss3.513, learning rate 0.0030000 \n",
      "Step: 215/865, accuracy0.078, loss3.637, learning rate 0.0030000 \n",
      "Step: 220/865, accuracy0.047, loss3.547, learning rate 0.0030000 \n",
      "Step: 225/865, accuracy0.047, loss3.502, learning rate 0.0030000 \n",
      "Step: 230/865, accuracy0.047, loss3.582, learning rate 0.0030000 \n",
      "Step: 235/865, accuracy0.047, loss3.537, learning rate 0.0030000 \n",
      "Step: 240/865, accuracy0.078, loss3.565, learning rate 0.0030000 \n",
      "Step: 245/865, accuracy0.078, loss3.533, learning rate 0.0030000 \n",
      "Step: 250/865, accuracy0.156, loss3.454, learning rate 0.0030000 \n",
      "Step: 255/865, accuracy0.125, loss3.504, learning rate 0.0030000 \n",
      "Step: 260/865, accuracy0.078, loss3.504, learning rate 0.0030000 \n",
      "Step: 265/865, accuracy0.125, loss3.453, learning rate 0.0030000 \n",
      "Step: 270/865, accuracy0.156, loss3.331, learning rate 0.0030000 \n",
      "Step: 275/865, accuracy0.047, loss3.563, learning rate 0.0030000 \n",
      "Step: 280/865, accuracy0.062, loss3.494, learning rate 0.0030000 \n",
      "Step: 285/865, accuracy0.031, loss3.484, learning rate 0.0030000 \n",
      "Step: 290/865, accuracy0.078, loss3.482, learning rate 0.0030000 \n",
      "Step: 295/865, accuracy0.125, loss3.335, learning rate 0.0030000 \n",
      "Step: 300/865, accuracy0.062, loss3.619, learning rate 0.0030000 \n",
      "Step: 305/865, accuracy0.094, loss3.355, learning rate 0.0030000 \n",
      "Step: 310/865, accuracy0.031, loss3.589, learning rate 0.0030000 \n",
      "Step: 315/865, accuracy0.047, loss3.574, learning rate 0.0030000 \n",
      "Step: 320/865, accuracy0.141, loss3.488, learning rate 0.0030000 \n",
      "Step: 325/865, accuracy0.094, loss3.480, learning rate 0.0030000 \n",
      "Step: 330/865, accuracy0.094, loss3.479, learning rate 0.0030000 \n",
      "Step: 335/865, accuracy0.047, loss3.555, learning rate 0.0030000 \n",
      "Step: 340/865, accuracy0.109, loss3.516, learning rate 0.0030000 \n",
      "Step: 345/865, accuracy0.078, loss3.375, learning rate 0.0030000 \n",
      "Step: 350/865, accuracy0.078, loss3.467, learning rate 0.0030000 \n",
      "Step: 355/865, accuracy0.062, loss3.554, learning rate 0.0030000 \n",
      "Step: 360/865, accuracy0.000, loss3.533, learning rate 0.0030000 \n",
      "Step: 365/865, accuracy0.062, loss3.618, learning rate 0.0030000 \n",
      "Step: 370/865, accuracy0.094, loss3.423, learning rate 0.0030000 \n",
      "Step: 375/865, accuracy0.078, loss3.601, learning rate 0.0030000 \n",
      "Step: 380/865, accuracy0.109, loss3.511, learning rate 0.0030000 \n",
      "Step: 385/865, accuracy0.094, loss3.372, learning rate 0.0030000 \n",
      "Step: 390/865, accuracy0.188, loss3.326, learning rate 0.0030000 \n",
      "Step: 395/865, accuracy0.109, loss3.481, learning rate 0.0030000 \n",
      "Step: 400/865, accuracy0.094, loss3.625, learning rate 0.0030000 \n",
      "Step: 405/865, accuracy0.047, loss3.418, learning rate 0.0030000 \n",
      "Step: 410/865, accuracy0.078, loss3.434, learning rate 0.0030000 \n",
      "Step: 415/865, accuracy0.078, loss3.487, learning rate 0.0030000 \n",
      "Step: 420/865, accuracy0.156, loss3.468, learning rate 0.0030000 \n",
      "Step: 425/865, accuracy0.109, loss3.484, learning rate 0.0030000 \n",
      "Step: 430/865, accuracy0.125, loss3.535, learning rate 0.0030000 \n",
      "Step: 435/865, accuracy0.047, loss3.547, learning rate 0.0030000 \n",
      "Step: 440/865, accuracy0.047, loss3.598, learning rate 0.0030000 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27172/721022812.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mmlflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'9_layer_CNN'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mmlflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mmdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_with_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_epochs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m39\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\ngrec\\Documents\\Learning\\Masters\\AI for data science\\Assignment\\plant-leaf-diseases-identification\\src\\model.py\u001b[0m in \u001b[0;36mtrain_with_loader\u001b[1;34m(self, train_loader, epochs, log, log_freq, validation_loader, cls_count)\u001b[0m\n\u001b[0;32m    300\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_logger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch_logger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--Validation--'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ngrec\\Documents\\Learning\\Masters\\AI for data science\\Assignment\\plant-leaf-diseases-identification\\src\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"Model Architecture: \\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlayer_str\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;34m\"\"\"Appends a single layer to the end of the network\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ngrec\\Documents\\Learning\\Masters\\AI for data science\\Assignment\\plant-leaf-diseases-identification\\src\\layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__convolve2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ngrec\\Documents\\Learning\\Masters\\AI for data science\\Assignment\\plant-leaf-diseases-identification\\src\\layers.py\u001b[0m in \u001b[0;36m__convolve2d\u001b[1;34m(self, pad, strides)\u001b[0m\n\u001b[0;32m    356\u001b[0m                 \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m                 \u001b[0mt2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mch_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m                 \u001b[0msum_ch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvolve2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'valid'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mch_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mch_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum_ch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"Plant Leaf Disease\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param('framework', 'homebrew')\n",
    "    mlflow.log_param('data_split', '90/10')\n",
    "    mlflow.log_param('type', '9_layer_CNN')\n",
    "    mlflow.log_params(config)\n",
    "    mdl.train_with_loader(train_loader, epochs=config['max_epochs'], validation_loader=validation_loader, cls_count=39, log_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1beaad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl.set_save_config(model_name=config['name'], save_path=os.path.join(os.getcwd(), 'models'))\n",
    "mdl.save(mdl.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab691005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
