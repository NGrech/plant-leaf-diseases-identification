{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ad873e",
   "metadata": {},
   "source": [
    "# Dev notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef02f46",
   "metadata": {},
   "source": [
    "I use this notebook to develop my implementation of a simple and minimal neural network framework.\n",
    "\n",
    "Inspiration for this network is drawn from the book [Neural Networks from Scratch (NNFS)](https://nnfs.io/) & [Pytorch's implementation](https://pytorch.org/)\n",
    "\n",
    "Notes:\n",
    "- The work on experimenting with backpropagation has been moved to another notebook to keep this one minimal\n",
    "- I have chosen to use a similar naming convention to that used by pytorch (why reinvent the wheel), this has the benefit of ensuring that when we compare implementations the architecture of the networks is the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab9fbb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import for dev and testing\n",
    "import nnfs \n",
    "import numpy as np\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from nnfs.datasets import spiral_data\n",
    "from typing import List\n",
    "\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466662af",
   "metadata": {},
   "source": [
    "## Defining Base Module\n",
    "\n",
    "This is a module that contains all the base attributes and function needed by every class in the framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52091eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(ABC):\n",
    "    \"\"\"Base class for all classes in frame work to ensure the same attributes and common function names.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        # Attributes to hold input and outputs\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85db83e2",
   "metadata": {},
   "source": [
    "##  Defining layers \n",
    "\n",
    "In this section We define and test the layers\n",
    "\n",
    "Notes:\n",
    "- since we intend to use ReLU as one of our activation functions we will use the He weight initialization method as described in https://arxiv.org/abs/1502.01852"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7fe0c9",
   "metadata": {},
   "source": [
    "### Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48b830b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(Module):\n",
    "    \"\"\"Linear transformation layer of the type o = ixW + b,\n",
    "    \n",
    "    where I is the incoming vector, W is the layers weight matrix, b is bias vector and o is the dot product of the \n",
    "    i and W plus the bias\n",
    "    \n",
    "    Args:\n",
    "        in_features      (int):   The size of the input features \n",
    "        out_features     (int):   The size of the output features\n",
    "        lambda_l1_weight (float): Hyperperamiter lambda for L1 regularization for the weights \n",
    "        lambda_l1_bias   (float): Hyperperamiter lambda for L1 regularization for the bias\n",
    "        lambda_l2_weight (float): Hyperperamiter lambda for L2 regularization for the weights\n",
    "        lambda_l2_bias   (float): Hyperperamiter lambda for L2 regularization for the bias\n",
    "        \n",
    "    Attributes:\n",
    "        weights          (np_array): numpy array of in_features x n_neurons\n",
    "        biases           (np_array): numpy array of 1 x n_neurons\n",
    "        inputs           (np_array): numpy array of latest batch of inputs\n",
    "        outputs          (np_array): numpy array of latest batch of outputs\n",
    "        d_w              (np_array): The current gradients with respect to the weights \n",
    "        d_b              (np_array): The current gradients with respect to the biases\n",
    "        grad             (np_array): The current gradients with respect to the inputs\n",
    "        lambda_l1_weight (float):    Hyperperamiter lambda for L1 regularization for the weights \n",
    "        lambda_l1_bias   (float):    Hyperperamiter lambda for L1 regularization for the bias\n",
    "        lambda_l2_weight (float):    Hyperperamiter lambda for L2 regularization for the weights\n",
    "        lambda_l2_bias   (float):    Hyperperamiter lambda for L2 regularization for the bias\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, \n",
    "                 lambda_l1_weight=0, lambda_l1_bias=0, \n",
    "                 lambda_l2_weight=0, lambda_l2_bias=0) -> None:\n",
    "        super().__init__()\n",
    "        # initializing weights and biases \n",
    "        self.weights = np.random.normal(0.0, np.sqrt(2/in_features), (in_features, out_features))\n",
    "        # Using a simpler initialization  for testing \n",
    "        #self.weights = 0.01 * np.random.randn(in_features, out_features)\n",
    "        self.bias = np.zeros((1, out_features))\n",
    "        # initializing regularization lambdas\n",
    "        if (lambda_l1_bias > 0) | (lambda_l1_weight > 0):\n",
    "            self.lambda_l1_weight = lambda_l1_weight\n",
    "            self.lambda_l1_bias = lambda_l1_bias\n",
    "        else:\n",
    "            self.lambda_l1_weight = None\n",
    "            self.lambda_l1_bias = None\n",
    "        if (lambda_l2_bias > 0) | (lambda_l2_weight > 0):  \n",
    "            self.lambda_l2_weight = lambda_l2_weight\n",
    "            self.lambda_l2_bias = lambda_l2_bias\n",
    "        else: \n",
    "            self.lambda_l2_weight = None\n",
    "            self.lambda_l2_bias = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass through the layer.\n",
    "        \n",
    "        Args:\n",
    "        inputs (np_array): Inputs to the layer must be the same size as the weights.\n",
    "        \"\"\"\n",
    "        self.input = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def l1_backward_w(self):\n",
    "        \"\"\"Backpropagation of L1 regularization function wrt weights.\"\"\"\n",
    "        if self.lambda_l1_weight:\n",
    "            d_l1 = np.ones_like(self.weights) \n",
    "            d_l1[self.weights < 0] = -1\n",
    "            return d_l1 * self.lambda_l1_weight\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def l1_backward_b(self):\n",
    "        \"\"\"Backpropagation of L1 regularization function wrt bias.\"\"\"\n",
    "        if self.lambda_l1_bias:\n",
    "            d_l1 = np.ones_like(self.bias) \n",
    "            d_l1[self.bias < 0] = -1\n",
    "            return d_l1 * self.lambda_l1_bias\n",
    "        else:\n",
    "            return 0  \n",
    "\n",
    "    def l2_backward_w(self):\n",
    "        \"\"\"Backpropagation of L2 regularization function wrt weights.\"\"\"\n",
    "        if self.lambda_l2_weight:\n",
    "            return 2 * self.lambda_l2_weight  * self.weights\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def l2_backward_b(self):\n",
    "        \"\"\"Backpropagation of L1 regularization function wrt bias.\"\"\"\n",
    "        if self.lambda_l2_bias:\n",
    "            return 2 * self.lambda_l2_bias  * self.bias\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def backward(self, d_vals):\n",
    "        \"\"\"Backpropagation  of the linear layer function\n",
    "\n",
    "        Args:\n",
    "            d_vals (np_array): derivatives from the previous layer/function.\n",
    "        \"\"\"\n",
    "        self.d_w = np.dot(self.input.T, d_vals) + self.l1_backward_w() + self.l2_backward_w()\n",
    "        self.d_b = np.sum(d_vals, axis=0, keepdims=True) + self.l1_backward_b() + self.l2_backward_b()\n",
    "\n",
    "        self.grad = np.dot(d_vals, self.weights.T)\n",
    "\n",
    "    def regularization_loss(self):\n",
    "        \"\"\"Calculates the regularization loss of the layer. \n",
    "        It will only do the calculation if the respective lambda for the loss type is > 0\"\"\"\n",
    "        loss = 0\n",
    "        # L1 weight \n",
    "        if self.lambda_l1_weight:\n",
    "            loss += self.lambda_l1_weight * np.sum(np.abs(self.weights))\n",
    "        # L1 bias\n",
    "        if self.lambda_l1_bias:\n",
    "            loss += self.lambda_l1_bias * np.sum(np.abs(self.bias))\n",
    "        # L2 weight\n",
    "        if self.lambda_l2_weight:\n",
    "            loss += self.lambda_l2_weight * np.sum(self.weights * self.weights)\n",
    "        # L2 bias\n",
    "        if self.lambda_l2_bias:\n",
    "            loss += self.lambda_l2_bias * np.sum(self.bias * self.bias)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ea363f",
   "metadata": {},
   "source": [
    "#### Testing Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1ca4160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.        ]\n",
      " [-0.01047519  0.01139536 -0.00479835]\n",
      " [-0.02741484  0.03172915 -0.00869218]\n",
      " [-0.04218837  0.05266625 -0.00559127]\n",
      " [-0.05770768  0.0714014  -0.00894304]\n",
      " [-0.0354307   0.03502549 -0.02336348]\n",
      " [-0.089267    0.10767876 -0.01945324]\n",
      " [-0.09335078  0.10723802 -0.0312274 ]\n",
      " [-0.11243759  0.13112801 -0.03362967]\n",
      " [-0.13386956  0.16200906 -0.02810179]]\n"
     ]
    }
   ],
   "source": [
    "# The sample data is a list of coordinate, ie two points \n",
    "# The layer therefore will take 2 inputs \n",
    "# We have given it 3 out features (3 neurons) so we expect to see an output with the shape (n_samples*n_neurons, n_neurons)\n",
    "# In out case that should be (300, 3) \n",
    "X, _ = spiral_data(samples=100, classes=3)\n",
    "linear1 = LinearLayer(2, 3)\n",
    "output = linear1.forward(X)\n",
    "print(output[:10, :])\n",
    "\n",
    "assert output.shape == (300,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ef04ca",
   "metadata": {},
   "source": [
    "## Defining Dropout\n",
    "\n",
    "The drop out layer will take one hyperparameter, rate,  which will represent the percentage of neurons that will be deactivated with each forward pass. \n",
    "\n",
    "To achieve this will will make a mask the size of the output of the previous layer (number of neurons) and apply a binomial distribution. The distribution will be used to generate an array of zero or ones that will be multiplied against the output of the previous layer. This will deactivate or zero out those outputs that correspond to the zeros generated to the binomial dist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b43a61e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    \"\"\" Dropout Layer, intended to be used in traning to deactivate a random portion of the neurons from \n",
    "    a pervious layer to based on the work https://arxiv.org/abs/1207.0580\n",
    "\n",
    "    Args:\n",
    "        p (float): probability of an element to be set to zero\n",
    "\n",
    "    Attributes:\n",
    "        p            (float):   probability of an element to be set to zero\n",
    "        mask         (ndarray): Latest scaled binary mask used to zero out input elements \n",
    "        traning_mode (binary):  Binary flag to control behaviour betwen traning and eval modes\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p:float) -> None:\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "        self.training_mode = True\n",
    "\n",
    "    def forward(self, input:np.ndarray) -> np.ndarray:\n",
    "        \"\"\"During training it will randomly zero out a number of inputs according to a binomial distribution\n",
    "        and it will also scale the inputs by 1/1-p to account for the lack of dropout in evaluation mode.\n",
    "        During evaluation it returns the input.\n",
    "        \n",
    "        Args:\n",
    "            input (ndarray): Output from a previous layer\n",
    "        \"\"\"\n",
    "        if not self.training_mode:\n",
    "            # Eval operation mode -> NO DROPOUT\n",
    "            self.output = input\n",
    "            return self.output\n",
    "\n",
    "        # Training operation mode -> Dropout \n",
    "        self.input = input\n",
    "        self.mask = np.random.binomial(1, self.p, size=input.shape)/ (1-self.p)\n",
    "        self.output = input * self.mask \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grads):\n",
    "        \"\"\"Backpropagation of the dropout function.\n",
    "        \n",
    "        Args:\n",
    "            grads (ndarray): gradients from the next layer.\n",
    "        \"\"\"\n",
    "        self.grad = grads * self.mask\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdfdc98",
   "metadata": {},
   "source": [
    "### Testing Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f92588f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ],\n",
       "       [-0.05237594,  0.05697681, -0.02399175],\n",
       "       [-0.13707422,  0.        , -0.0434609 ],\n",
       "       [-0.        ,  0.        , -0.02795634],\n",
       "       [-0.28853839,  0.35700701, -0.04471522],\n",
       "       [-0.17715348,  0.17512744, -0.1168174 ],\n",
       "       [-0.446335  ,  0.53839382, -0.09726618],\n",
       "       [-0.46675388,  0.53619012, -0.15613699],\n",
       "       [-0.56218795,  0.65564007, -0.        ],\n",
       "       [-0.        ,  0.8100453 , -0.14050897]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the output of the previous test\n",
    "input = output[:10, :]\n",
    "p = 0.8 # deactivate 10% of the neurons\n",
    "\n",
    "dropout = Dropout(p)\n",
    "dropout.forward(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2017ad64",
   "metadata": {},
   "source": [
    "## Defining Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8da5f5",
   "metadata": {},
   "source": [
    "### ReLu\n",
    "\n",
    "$$y = \\begin{cases}\n",
    "   x &x> 0 \\\\\n",
    "   0 & otherwise\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a03d0ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\"Applies Rectified linear Unit function to vector.\n",
    "    \n",
    "    Attributes:\n",
    "        inputs            (ndarray): numpy array of latest batch of inputs\n",
    "        outputs           (ndarray): numpy array of latest batch of outputs\n",
    "        grad              (ndarray): The current gradients with respect to the inputs\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        # initializing attributes needed for backwards \n",
    "        super().__init__()\n",
    "        self.grad = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # storing inputs needed for backwards \n",
    "        self.inputs = x\n",
    "        self.output = np.maximum(x, 0)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, d_vals):\n",
    "        self.grad = d_vals.copy()\n",
    "        self.grad[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa60fd7a",
   "metadata": {},
   "source": [
    "#### Testing ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bea142e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 3. , 4. , 0. , 0.1, 0. ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = [-2, 3, 4, 0, 0.1, -44]\n",
    "test_relu = ReLU()\n",
    "\n",
    "# Checking values are as expected \n",
    "assert np.all(np.array([0., 3, 4, 0, 0.1, 0.]) == test_relu.forward(i))\n",
    "\n",
    "test_relu.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc75ac1",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "$$\\text{softmax}(x)_i = \\frac{exp(x_i)}{\\sum_{j}^{ }exp(x_j))}$$\n",
    "\n",
    "The soft max represents the confidence score for each output class and adds up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "966f1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Module):\n",
    "    \"\"\"Applies Softmax function to input.\n",
    "    \n",
    "    Attributes:\n",
    "        inputs            (ndarray): numpy array of latest batch of inputs\n",
    "        outputs           (ndarray): numpy array of latest batch of outputs\n",
    "        grad              (ndarray): The current gradients with respect to the inputs\n",
    "        confidence_scores (ndarray): Latest batch of classification probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.confidence_scores = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\n",
    "        Args:\n",
    "            x (ndarray): Input from the pervious layer\n",
    "        \"\"\"\n",
    "        # exponenets of each value\n",
    "        exp_vals = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        exp_sum = np.sum(exp_vals, axis=1, keepdims=True)\n",
    "        # Normalization to get the proabilities \n",
    "        self.output = exp_vals/exp_sum\n",
    "        return self.output\n",
    "\n",
    "    def _backward(self, d_vals):\n",
    "        \"\"\"Backward pass which calculates the gradient wrt the inputs \n",
    "\n",
    "        Args:\n",
    "            d_vals (ndarray): gradients from the loss calculation\n",
    "        \"\"\"\n",
    "        # Initialize array for gradients wrt to inputs\n",
    "        self.grad = np.zeros_like(d_vals)\n",
    "        \n",
    "        _iter = enumerate(zip(self.output, d_vals))\n",
    "        for i, conf_score, d_val in _iter:\n",
    "            # Flatten confidence scores\n",
    "            cs = conf_score.reshape(-1, 1)\n",
    "            # Find the Jacobian matrix of the output \n",
    "            j_matrix = np.diagflat(cs) - np.dot(cs, cs.T)\n",
    "            # get the gradient \n",
    "            self.grad[i] = np.dot(j_matrix, d_val)\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"Combined backward pass for CCE & Softmax as a single which is\n",
    "           faster to compute.\n",
    "\n",
    "        Args:\n",
    "            y_pred (ndarray): predicted classes for the current batch\n",
    "            y_true (ndarray): One hot encoded true values for y\n",
    "        \"\"\"\n",
    "        # Number of examples in the batch\n",
    "        n = len(y_pred)\n",
    "\n",
    "        # Getting descrete vals from one hot encoding \n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "        \n",
    "        self.grad = y_pred.copy()\n",
    "        self.grad[range(n), y_true] -= 1\n",
    "        self.grad = self.grad / n\n",
    "        return self.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e8095e",
   "metadata": {},
   "source": [
    "#### Testing Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5cd2c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.11513104e-19, 5.74952226e-19, 1.00000000e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = Softmax()\n",
    "softmax.forward([[1,2,44]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b31bb",
   "metadata": {},
   "source": [
    "## Defining Loss - Categorical Cross-Entropy\n",
    "\n",
    "$$ L_i = -\\sum_j y_{i,j}\\log(\\hat{y}_{i,j}) $$\n",
    "\n",
    "With taking one hot encoding into account we can simplify this down to:\n",
    "\n",
    "$$ L_i = -y_{i,k}\\log(\\hat{y}_{i,k}) $$\n",
    "\n",
    "where K is the index of the correct class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7645a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalCrossEntropyLoss:\n",
    "    \"\"\"Calculates the CCE loss for a given set of predictions.\n",
    "    This method expect a softmax output and one-hot encoded label mask\n",
    "    \n",
    "    y_pred (np_array): matrix of confidence scores of the prediction\n",
    "    y_true (np_array): matrix of one-hot encoded true lables of the classes\n",
    "    \"\"\"\n",
    "    def forward(y_pred, y_true):\n",
    "        # Clipping and applying one hot encoded labels as mask \n",
    "        # to zero out scores corresponding to incorrect classes\n",
    "        # We clip to make sure that none of the reaming classes are 0 or \n",
    "        # exactly 1 \n",
    "        clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        corrected = np.sum(clipped*y_true, axis=1)\n",
    "        # Taking the -ve log of the remaining confidence scores \n",
    "        negative_log = -np.log(corrected)\n",
    "        return np.mean(negative_log)\n",
    "\n",
    "    def backward(y_pred, y_true):\n",
    "        \"\"\"Backpropagation  of the CCE Loss\n",
    "\n",
    "        Args:\n",
    "            y_pred (np_array) array of predictions.\n",
    "            y_true (np_array) array of correct labels.\n",
    "        \"\"\"\n",
    "        return (-y_true/y_pred)/len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6704da26",
   "metadata": {},
   "source": [
    "#### Testing CCE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b0abd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38506088005216804"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.array([[0.7, 0.1, 0.2], [0.1,0.5,0.4],[0.02,0.9,0.08]])\n",
    "y_true = np.array([[1,0,0], [0,1,0], [0,1,0]])\n",
    "\n",
    "loss_function = CategoricalCrossEntropyLoss\n",
    "loss_function.forward(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b0d6f",
   "metadata": {},
   "source": [
    "## Defining Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb402b8",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Decent \n",
    "\n",
    "$$ \\text{Update} = -\\text{Learning Rate} \\cdot \\text{Gradient}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5427718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDG:\n",
    "    \"\"\"Stochastic Gradient Decent class used to update layer paramers\n",
    "    The update is the -ve learning rate multiplied by the gradient calculated \n",
    "    in the backward step. Optionally it will also apply momentum and decay.\n",
    "\n",
    "    Args:\n",
    "        lr       (float): Learning rate to scale the gradients by for the update\n",
    "        decay    (float): Decay rate used to scale learning rate\n",
    "        momentum (float): momentum factor used to scale updates to avoid local minima\n",
    "    \n",
    "    Attributes:\n",
    "        lr         (float): Learning rate to scale the gradients by for the update\n",
    "        clr        (float): Learning rate at the current step\n",
    "        decay      (float): Decay rate used to scale learning rate\n",
    "        momentum   (float): momentum factor used to scale updates to avoid local minima\n",
    "        iterations (int):   Number of times optimizer has completed a step\n",
    "    \"\"\"\n",
    "    IMPLEMENTED = [LinearLayer]\n",
    "\n",
    "    def __init__(self, learning_rate=1, decay=0., momentum=0.) -> None:\n",
    "        self.lr = learning_rate\n",
    "        self.clr = learning_rate # current learning rate\n",
    "        self.decay = decay\n",
    "        self.momentum = momentum\n",
    "        self.iterations = 0\n",
    "\n",
    "    def init_momentum(self, layers):\n",
    "        \"\"\"Initializes momentum arttribute for layer objects.\n",
    "        Args:\n",
    "            Layers (list): A list of layers that need to be updated with momentum.\n",
    "        \"\"\"\n",
    "        for layer in layers:\n",
    "            if not hasattr(layer, 'momentum_w'):\n",
    "                layer.momentum_w = np.zeros_like(layer.weights)\n",
    "                layer.momentum_b = np.zeros_like(layer.bias)\n",
    "\n",
    "    def pre_update_step(self):\n",
    "        \"\"\"Update the current learning rate according to the decay and iterations\"\"\"\n",
    "        decay_rate = 1/(1 + self.decay * self.iterations)\n",
    "        self.clr = self.lr * decay_rate\n",
    "\n",
    "    def get_updates(self, layer):\n",
    "        \"\"\"Get the update values for a layer's weights and biases\n",
    "        Args:\n",
    "            Layers (list): A list of layers that need to be updated with momentum.\"\"\"\n",
    "        return (\n",
    "            -self.clr*layer.d_w,\n",
    "            -self.clr*layer.d_b\n",
    "        )\n",
    "\n",
    "    def get_momentum_updates(self, layer):\n",
    "        \"\"\"Updates a layers momentum.\"\"\"\n",
    "        wu = (self.momentum * layer.momentum_w) - (self.clr * layer.d_w) \n",
    "        bu = (self.momentum * layer.momentum_b) - (self.clr * layer.d_b) \n",
    "        layer.momentum_w = wu\n",
    "        layer.momentum_b = bu\n",
    "        return (wu, bu)\n",
    "\n",
    "    def update(self, layers):\n",
    "        \"\"\"Update a layers parameters\n",
    "        Args:\n",
    "            Layers (list): A list of layers that need to be updated.\n",
    "        \"\"\"\n",
    "        # Test to make sure all layers supported\n",
    "        if any(l for l in layers if type(l) not in self.IMPLEMENTED):\n",
    "            unsupported = next(l for l in layers if type(l) not in self.IMPLEMENTED)\n",
    "            raise NotImplementedError(f'SDG does not support {unsupported.__class__}')\n",
    "\n",
    "        # pre update step\n",
    "        if self.decay:\n",
    "            self.pre_update_step()\n",
    "\n",
    "        # On the first iteration using momentum initialize the layer momentums\n",
    "        if self.iterations == 0 and self.momentum:\n",
    "            self.init_momentum(layers)\n",
    "\n",
    "        # Update step\n",
    "        for layer in layers:\n",
    "\n",
    "            if self.momentum:\n",
    "                weight_u, bias_u = self.get_momentum_updates(layer)\n",
    "            else:\n",
    "                weight_u, bias_u = self.get_updates(layer)\n",
    "            \n",
    "            layer.weights += weight_u\n",
    "            layer.bias += bias_u\n",
    "\n",
    "        # post update\n",
    "        self.iterations += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\n",
    "Short for Adaptive Momentum.\n",
    "\n",
    "An extension to the Root mean square propagation (RSMprop) technique that adds in a bias correction mechanism used to correct the momentum and momentum caches.\n",
    "\n",
    "To find the update with Adam we need to take the following steps:\n",
    "\n",
    "1. Find momentum for the current step\n",
    "2. Get corrected the momentum \n",
    "3. Update the cache with the square of the gradient \n",
    "4. Get the corrected cache \n",
    "5. Update weights \n",
    "\n",
    "\n",
    "In the first step we calculate the layer weight and bias momentums by:\n",
    "\n",
    "$$ \\text{Layer Momentum} = (\\beta_1 \\cdot \\text{Layer Momentum}) + ((1 - \\beta_1) \\cdot gradient)$$ \n",
    "\n",
    "where $\\beta_1$ is a hyper-parameter that allows us to apply fractions of the momentum and gradient at each step. \n",
    "\n",
    "To correct this we then divide the momentum by bias correction mechanism: \n",
    "\n",
    "$$ \\text{Corrected Momentum} = \\frac{\\text{Layer Momentum}}{1 - \\beta_1^{n+1}} $$\n",
    "\n",
    "where $n$ is the number of the iteration/epoch and we add 1 to it to account for initializing it from 0\n",
    "\n",
    "Next we update the cache for the weights and biases:\n",
    "\n",
    "$$ \\text{Cache} = (\\beta_2 \\cdot \\text{Cache}) + ((1 - \\beta_2) * \\text{gradients}^2)$$\n",
    "\n",
    "We once again correct, this time the cache, with Adam's bias correction mechanism:\n",
    "\n",
    "$$ \\text{Corrected Cache} = \\frac{\\text{Cache}}{{1 - \\beta_2^{n+1}}}$$\n",
    "\n",
    "Finally to update the weights we do the following:\n",
    "\n",
    "$$ \\text{Update} = \\frac{\\text{Current Learning Rate} \\cdot \\text{Corrected Momentum}}{\\sqrt{\\text{Corrected Cache}} + \\epsilon} $$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9974c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    \"\"\"Adam Optimizer Short for Adaptive Momentum.\n",
    "    An extension to the Root mean square propagation (RSMprop) technique that adds in a bias correction mechanism used to correct the momentum and momentum caches.\n",
    "    To find the update with Adam we need to take the following steps:\n",
    "        1. Find momentum for the current step\n",
    "        2. Get corrected the momentum \n",
    "        3. Update the cache with the square of the gradient \n",
    "        4. Get the corrected cache \n",
    "        5. Update weights \n",
    "    \n",
    "    Args:\n",
    "        learning_rate (float): Learning rate to scale the gradients by for the update\n",
    "        decay         (float): Decay rate used to scale learning rate\n",
    "        epsilon       (float): Hyperparmeter for tuning update\n",
    "        beta_1        (float): Hyperparameter for calculating momentum \n",
    "        beta_2        (float): Hyperparameter for calculating cache\n",
    "\n",
    "    Attributes:\n",
    "        lr          (float): Learning rate to scale the gradients by for the update\n",
    "        clr         (float): L:earning rate at current step\n",
    "        decay       (float): Decay rate used to scale learning rate\n",
    "        epsilon     (float): Hyperparmeter for tuning update\n",
    "        beta_1      (float): Hyperparameter for calculating momentum \n",
    "        beta_2      (float): Hyperparameter for calculating cache\n",
    "        iterations  (int):   Number of times optimizer has completed a step\n",
    "    \"\"\"\n",
    "\n",
    "    IMPLEMENTED = [LinearLayer]\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999) -> None:\n",
    "        self.lr = learning_rate\n",
    "        self.clr = learning_rate # current learning rate\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.iterations = 0\n",
    "\n",
    "    def pre_update_step(self):\n",
    "        \"\"\"Update the current learning rate according to the decay and iterations\"\"\"\n",
    "        decay_rate = 1/(1 + self.decay * self.iterations)\n",
    "        self.clr = self.lr * decay_rate\n",
    "\n",
    "    def init_momentum(self, layers):\n",
    "        \"\"\"Initializes momentum arttribute for layer objects.\n",
    "        Args:\n",
    "            Layers (list): A list of layers that need to be updated with momentum.\n",
    "        \"\"\"\n",
    "        for layer in layers:\n",
    "            # Init momentum for weights\n",
    "            layer.momentums_w = np.zeros_like(layer.weights)\n",
    "            layer.cache_w = np.zeros_like(layer.weights)\n",
    "\n",
    "            # Init momentums for biases\n",
    "            layer.momentums_b = np.zeros_like(layer.bias)\n",
    "            layer.cache_b = np.zeros_like(layer.bias)\n",
    "            \n",
    "    def update(self, layers):\n",
    "        \"\"\"Update a layers parameters\n",
    "        Args:\n",
    "            Layers (list): A list of layers that need to be updated.\n",
    "        \"\"\"\n",
    "        # pre update step\n",
    "        if self.decay:\n",
    "           self.pre_update_step()\n",
    "        \n",
    "        if self.iterations == 0:\n",
    "            self.init_momentum(layers)\n",
    "\n",
    "        # Update step\n",
    "        for layer in layers:     \n",
    "            ## Updating momentum \n",
    "            layer.momentums_w = self.beta_1 * layer.momentums_w + (1 - self.beta_1) * layer.d_w\n",
    "            layer.momentums_b = self.beta_1 * layer.momentums_b + (1 - self.beta_1) * layer.d_b\n",
    "\n",
    "            ## Correcting momentum \n",
    "            correction_bias_momentums = 1 - self.beta_1**(self.iterations +1)\n",
    "\n",
    "            corrected_weights = layer.momentums_w / correction_bias_momentums\n",
    "            corrected_bias    = layer.momentums_b / correction_bias_momentums\n",
    "\n",
    "            ## Updating cache\n",
    "            layer.cache_w = self.beta_2 * layer.cache_w + (1 - self.beta_2) * layer.d_w**2\n",
    "            layer.cache_b = self.beta_2 * layer.cache_b + (1 - self.beta_2) * layer.d_b**2\n",
    "\n",
    "            ## Correcting cache\n",
    "            correction_bias_cache = 1 - self.beta_2**(self.iterations +1)\n",
    "\n",
    "            corrected_cache_w = layer.cache_w / correction_bias_cache\n",
    "            corrected_cache_b = layer.cache_b / correction_bias_cache\n",
    "\n",
    "            ## Updating weights \n",
    "            layer.weights += -self.clr * corrected_weights / (np.sqrt(corrected_cache_w) + self.epsilon)\n",
    "\n",
    "            ## Updating bias\n",
    "            layer.bias    += -self.clr * corrected_bias / (np.sqrt(corrected_cache_b) + self.epsilon)\n",
    "        \n",
    "        # Post update step\n",
    "        self.iterations += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e97e5",
   "metadata": {},
   "source": [
    "## Defining Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee48592",
   "metadata": {},
   "source": [
    "### One-hot encoding function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "649a4d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_index(y, n):\n",
    "    return np.eye(n)[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cfee58",
   "metadata": {},
   "source": [
    "#### Testing one hot masker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da547098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=3\n",
    "y_test = np.array([0,1,2, 1, 2])\n",
    "\n",
    "one_hot_encode_index(y_test, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f0c34d",
   "metadata": {},
   "source": [
    "### Define Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb0b0f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    \"\"\"Calculates the accuracy of a batch of predictions\"\"\"\n",
    "    return np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_true, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9d170f",
   "metadata": {},
   "source": [
    "## Defining Regularization \n",
    "\n",
    "Regularization methods are used to reduce generalization errors, With L1 and L2 regularization we calculate a penalty that we add to the loss if weight and biases are large. We want to see many neurons contribute to the evaluation rather than a few having a large impact.\n",
    "\n",
    "L1 weight regularization: \n",
    "\n",
    "$$ L_{1w} = \\lambda\\sum_m|w_m| $$\n",
    "\n",
    "L1 bias regularization: \n",
    "\n",
    "$$ L_{1b} = \\lambda\\sum_n|b_n| $$\n",
    "\n",
    "L2 weight regularization: \n",
    "\n",
    "$$ L_{2w} = \\lambda\\sum w^2_m $$\n",
    "\n",
    "L2 bias regularization: \n",
    "\n",
    "$$ L_{2b} = \\lambda\\sum_n b^2_n $$\n",
    "\n",
    "Overall loss:\n",
    "\n",
    "$$ \\text{Loss} = \\text{DataLoss} + L_{1w} + L_{1b} + L_{2w} + L_{2b} $$\n",
    "\n",
    "\n",
    "To implement these changes we will need to modify the Linear Layer class.\n",
    "\n",
    "(note: for backpropagation see backpropagation notebook)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b81ddc",
   "metadata": {},
   "source": [
    "## Integration Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e599d501",
   "metadata": {},
   "source": [
    "### Test with SDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b157cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Loss:1.113, Reg loss: 1.174, (0.061), accuracy:0.360\n",
      "Epoch:100, Loss:1.042, Reg loss: 1.068, (0.026), accuracy:0.407\n",
      "Epoch:200, Loss:0.777, Reg loss: 0.839, (0.062), accuracy:0.650\n",
      "Epoch:300, Loss:0.703, Reg loss: 0.793, (0.089), accuracy:0.680\n",
      "Epoch:400, Loss:0.607, Reg loss: 0.724, (0.117), accuracy:0.753\n",
      "Epoch:500, Loss:0.557, Reg loss: 0.679, (0.121), accuracy:0.797\n",
      "Epoch:600, Loss:0.522, Reg loss: 0.679, (0.157), accuracy:0.790\n",
      "Epoch:700, Loss:1.524, Reg loss: 1.665, (0.141), accuracy:0.480\n",
      "Epoch:800, Loss:0.523, Reg loss: 0.661, (0.138), accuracy:0.807\n",
      "Epoch:900, Loss:0.523, Reg loss: 0.652, (0.129), accuracy:0.790\n",
      "Epoch:1000, Loss:0.600, Reg loss: 0.738, (0.138), accuracy:0.757\n",
      "Epoch:1100, Loss:0.441, Reg loss: 0.577, (0.136), accuracy:0.813\n",
      "Epoch:1200, Loss:0.430, Reg loss: 0.566, (0.136), accuracy:0.827\n",
      "Epoch:1300, Loss:1.594, Reg loss: 1.739, (0.146), accuracy:0.487\n",
      "Epoch:1400, Loss:0.379, Reg loss: 0.527, (0.148), accuracy:0.870\n",
      "Epoch:1500, Loss:0.380, Reg loss: 0.510, (0.130), accuracy:0.887\n",
      "Epoch:1600, Loss:0.419, Reg loss: 0.545, (0.126), accuracy:0.837\n",
      "Epoch:1700, Loss:0.372, Reg loss: 0.497, (0.125), accuracy:0.853\n",
      "Epoch:1800, Loss:0.352, Reg loss: 0.477, (0.125), accuracy:0.877\n",
      "Epoch:1900, Loss:0.407, Reg loss: 0.531, (0.124), accuracy:0.843\n",
      "Epoch:2000, Loss:0.335, Reg loss: 0.458, (0.123), accuracy:0.890\n",
      "Epoch:2100, Loss:0.346, Reg loss: 0.468, (0.122), accuracy:0.890\n",
      "Epoch:2200, Loss:0.348, Reg loss: 0.469, (0.121), accuracy:0.870\n",
      "Epoch:2300, Loss:0.348, Reg loss: 0.469, (0.121), accuracy:0.867\n",
      "Epoch:2400, Loss:0.318, Reg loss: 0.437, (0.119), accuracy:0.897\n",
      "Epoch:2500, Loss:0.316, Reg loss: 0.434, (0.118), accuracy:0.873\n",
      "Epoch:2600, Loss:0.318, Reg loss: 0.435, (0.117), accuracy:0.890\n",
      "Epoch:2700, Loss:0.366, Reg loss: 0.482, (0.116), accuracy:0.847\n",
      "Epoch:2800, Loss:0.365, Reg loss: 0.480, (0.115), accuracy:0.850\n",
      "Epoch:2900, Loss:0.362, Reg loss: 0.476, (0.114), accuracy:0.857\n",
      "Epoch:3000, Loss:0.311, Reg loss: 0.425, (0.114), accuracy:0.880\n",
      "Epoch:3100, Loss:0.312, Reg loss: 0.425, (0.113), accuracy:0.883\n",
      "Epoch:3200, Loss:0.302, Reg loss: 0.414, (0.112), accuracy:0.890\n",
      "Epoch:3300, Loss:0.296, Reg loss: 0.407, (0.111), accuracy:0.900\n",
      "Epoch:3400, Loss:0.306, Reg loss: 0.417, (0.111), accuracy:0.890\n",
      "Epoch:3500, Loss:0.295, Reg loss: 0.405, (0.110), accuracy:0.887\n",
      "Epoch:3600, Loss:0.311, Reg loss: 0.420, (0.109), accuracy:0.880\n",
      "Epoch:3700, Loss:0.295, Reg loss: 0.404, (0.109), accuracy:0.890\n",
      "Epoch:3800, Loss:0.296, Reg loss: 0.404, (0.108), accuracy:0.883\n",
      "Epoch:3900, Loss:0.291, Reg loss: 0.399, (0.108), accuracy:0.893\n",
      "Epoch:4000, Loss:0.338, Reg loss: 0.445, (0.107), accuracy:0.870\n",
      "Epoch:4100, Loss:0.286, Reg loss: 0.392, (0.106), accuracy:0.903\n",
      "Epoch:4200, Loss:0.302, Reg loss: 0.408, (0.106), accuracy:0.880\n",
      "Epoch:4300, Loss:0.323, Reg loss: 0.429, (0.105), accuracy:0.873\n",
      "Epoch:4400, Loss:0.301, Reg loss: 0.406, (0.105), accuracy:0.883\n",
      "Epoch:4500, Loss:0.283, Reg loss: 0.387, (0.105), accuracy:0.907\n",
      "Epoch:4600, Loss:0.307, Reg loss: 0.411, (0.104), accuracy:0.880\n",
      "Epoch:4700, Loss:0.274, Reg loss: 0.378, (0.104), accuracy:0.910\n",
      "Epoch:4800, Loss:0.274, Reg loss: 0.377, (0.103), accuracy:0.910\n",
      "Epoch:4900, Loss:0.302, Reg loss: 0.404, (0.102), accuracy:0.883\n",
      "Epoch:5000, Loss:0.298, Reg loss: 0.400, (0.102), accuracy:0.887\n",
      "Epoch:5100, Loss:0.270, Reg loss: 0.372, (0.102), accuracy:0.913\n",
      "Epoch:5200, Loss:0.269, Reg loss: 0.370, (0.101), accuracy:0.913\n",
      "Epoch:5300, Loss:0.282, Reg loss: 0.383, (0.101), accuracy:0.887\n",
      "Epoch:5400, Loss:0.287, Reg loss: 0.388, (0.100), accuracy:0.887\n",
      "Epoch:5500, Loss:0.280, Reg loss: 0.380, (0.100), accuracy:0.903\n",
      "Epoch:5600, Loss:0.275, Reg loss: 0.375, (0.100), accuracy:0.900\n",
      "Epoch:5700, Loss:0.267, Reg loss: 0.366, (0.099), accuracy:0.910\n",
      "Epoch:5800, Loss:0.265, Reg loss: 0.364, (0.099), accuracy:0.913\n",
      "Epoch:5900, Loss:0.270, Reg loss: 0.368, (0.099), accuracy:0.903\n",
      "Epoch:6000, Loss:0.271, Reg loss: 0.370, (0.098), accuracy:0.903\n",
      "Epoch:6100, Loss:0.268, Reg loss: 0.366, (0.098), accuracy:0.913\n",
      "Epoch:6200, Loss:0.278, Reg loss: 0.376, (0.098), accuracy:0.900\n",
      "Epoch:6300, Loss:0.273, Reg loss: 0.370, (0.097), accuracy:0.900\n",
      "Epoch:6400, Loss:0.281, Reg loss: 0.378, (0.097), accuracy:0.893\n",
      "Epoch:6500, Loss:0.268, Reg loss: 0.365, (0.097), accuracy:0.910\n",
      "Epoch:6600, Loss:0.259, Reg loss: 0.356, (0.096), accuracy:0.917\n",
      "Epoch:6700, Loss:0.268, Reg loss: 0.364, (0.096), accuracy:0.907\n",
      "Epoch:6800, Loss:0.262, Reg loss: 0.358, (0.096), accuracy:0.910\n",
      "Epoch:6900, Loss:0.270, Reg loss: 0.365, (0.096), accuracy:0.910\n",
      "Epoch:7000, Loss:0.264, Reg loss: 0.359, (0.095), accuracy:0.910\n",
      "Epoch:7100, Loss:0.260, Reg loss: 0.355, (0.095), accuracy:0.907\n",
      "Epoch:7200, Loss:0.258, Reg loss: 0.353, (0.095), accuracy:0.910\n",
      "Epoch:7300, Loss:0.262, Reg loss: 0.357, (0.095), accuracy:0.913\n",
      "Epoch:7400, Loss:0.257, Reg loss: 0.351, (0.094), accuracy:0.917\n",
      "Epoch:7500, Loss:0.254, Reg loss: 0.348, (0.094), accuracy:0.913\n",
      "Epoch:7600, Loss:0.255, Reg loss: 0.349, (0.094), accuracy:0.923\n",
      "Epoch:7700, Loss:0.257, Reg loss: 0.351, (0.093), accuracy:0.910\n",
      "Epoch:7800, Loss:0.254, Reg loss: 0.347, (0.093), accuracy:0.913\n",
      "Epoch:7900, Loss:0.258, Reg loss: 0.351, (0.093), accuracy:0.913\n",
      "Epoch:8000, Loss:0.252, Reg loss: 0.345, (0.093), accuracy:0.913\n",
      "Epoch:8100, Loss:0.257, Reg loss: 0.350, (0.093), accuracy:0.910\n",
      "Epoch:8200, Loss:0.255, Reg loss: 0.348, (0.092), accuracy:0.920\n",
      "Epoch:8300, Loss:0.250, Reg loss: 0.342, (0.092), accuracy:0.927\n",
      "Epoch:8400, Loss:0.250, Reg loss: 0.342, (0.092), accuracy:0.920\n",
      "Epoch:8500, Loss:0.250, Reg loss: 0.342, (0.092), accuracy:0.927\n",
      "Epoch:8600, Loss:0.250, Reg loss: 0.341, (0.092), accuracy:0.913\n",
      "Epoch:8700, Loss:0.249, Reg loss: 0.340, (0.091), accuracy:0.917\n",
      "Epoch:8800, Loss:0.252, Reg loss: 0.343, (0.091), accuracy:0.913\n",
      "Epoch:8900, Loss:0.248, Reg loss: 0.339, (0.091), accuracy:0.927\n",
      "Epoch:9000, Loss:0.250, Reg loss: 0.341, (0.091), accuracy:0.920\n",
      "Epoch:9100, Loss:0.249, Reg loss: 0.340, (0.091), accuracy:0.923\n",
      "Epoch:9200, Loss:0.248, Reg loss: 0.338, (0.090), accuracy:0.927\n",
      "Epoch:9300, Loss:0.248, Reg loss: 0.338, (0.090), accuracy:0.927\n",
      "Epoch:9400, Loss:0.246, Reg loss: 0.336, (0.090), accuracy:0.930\n",
      "Epoch:9500, Loss:0.246, Reg loss: 0.336, (0.090), accuracy:0.927\n",
      "Epoch:9600, Loss:0.245, Reg loss: 0.335, (0.090), accuracy:0.920\n",
      "Epoch:9700, Loss:0.245, Reg loss: 0.335, (0.090), accuracy:0.917\n",
      "Epoch:9800, Loss:0.245, Reg loss: 0.334, (0.089), accuracy:0.920\n",
      "Epoch:9900, Loss:0.245, Reg loss: 0.334, (0.089), accuracy:0.927\n",
      "Epoch:10000, Loss:0.245, Reg loss: 0.334, (0.089), accuracy:0.923\n"
     ]
    }
   ],
   "source": [
    "nnfs.init()\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "y = one_hot_encode_index(y, 3)\n",
    "\n",
    "# Initializing Network Components \n",
    "relu = ReLU()\n",
    "softmax = Softmax()\n",
    "cce_loss = CategoricalCrossEntropyLoss\n",
    "optimizer = SDG(decay=1e-3, momentum=0.9)\n",
    "linear1 = LinearLayer(2, 64, lambda_l2_weight=5e-4, lambda_l2_bias=5e-4)\n",
    "linear2 = LinearLayer(64, 3)\n",
    "\n",
    "update_layers = [linear1, linear2]\n",
    "\n",
    "n_epochs = 10000\n",
    "\n",
    "for epoch in range(n_epochs + 1):\n",
    "    # Forward Pass\n",
    "    linear1.forward(X) \n",
    "    relu.forward(linear1.output)\n",
    "    linear2.forward(relu.output)\n",
    "    y_pred = softmax.forward(linear2.output)\n",
    "\n",
    "    # Calculating loss and regularized loss  \n",
    "    loss = cce_loss.forward(y_pred, y)\n",
    "    # Regularised loss\n",
    "    rl = sum([l.regularization_loss() for l in update_layers if hasattr(l, 'regularization_loss')])\n",
    "    regularized_loss = loss + rl\n",
    "\n",
    "    #Calculating accuracy \n",
    "    acc = accuracy(y_pred, y) \n",
    "\n",
    "    # Printing results\n",
    "    if not epoch % 100:\n",
    "        print(f\"Epoch:{epoch}, Loss:{loss:.3f}, Reg loss: {regularized_loss:.3f}, ({rl:.3f}), accuracy:{acc:.3f}\")\n",
    "\n",
    "    # Backward pass\n",
    "    softmax.backward(y_pred, y)\n",
    "    linear2.backward(softmax.grad)\n",
    "    relu.backward(linear2.grad)\n",
    "    linear1.backward(relu.grad)\n",
    "\n",
    "    # Optimization Step\n",
    "    optimizer.update(update_layers)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d282f129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vallidation: Accuracy: 0.817, loss: 0.450\n"
     ]
    }
   ],
   "source": [
    "# Validating the model \n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "y_test = one_hot_encode_index(y_test, 3)\n",
    "\n",
    "linear1.forward(X_test) \n",
    "relu.forward(linear1.output)\n",
    "linear2.forward(relu.output)\n",
    "y_pred = softmax.forward(linear2.output)\n",
    "\n",
    "# Calculating loss and accuracy \n",
    "loss = cce_loss.forward(y_pred, y_test)\n",
    "acc = accuracy(y_pred, y_test) \n",
    "\n",
    "print(f'Vallidation: Accuracy: {acc:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7e554e",
   "metadata": {},
   "source": [
    "### Test with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13409958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Loss:3.212, Reg loss: 3.276, (0.064), accuracy:0.373\n",
      "Epoch:100, Loss:0.847, Reg loss: 0.883, (0.036), accuracy:0.622\n",
      "Epoch:200, Loss:0.712, Reg loss: 0.757, (0.045), accuracy:0.683\n",
      "Epoch:300, Loss:0.655, Reg loss: 0.705, (0.050), accuracy:0.696\n",
      "Epoch:400, Loss:0.633, Reg loss: 0.684, (0.051), accuracy:0.707\n",
      "Epoch:500, Loss:0.602, Reg loss: 0.653, (0.051), accuracy:0.728\n",
      "Epoch:600, Loss:0.610, Reg loss: 0.660, (0.050), accuracy:0.723\n",
      "Epoch:700, Loss:0.614, Reg loss: 0.664, (0.049), accuracy:0.722\n",
      "Epoch:800, Loss:0.591, Reg loss: 0.640, (0.049), accuracy:0.733\n",
      "Epoch:900, Loss:0.591, Reg loss: 0.639, (0.048), accuracy:0.730\n",
      "Epoch:1000, Loss:0.584, Reg loss: 0.632, (0.048), accuracy:0.744\n",
      "Epoch:1100, Loss:0.567, Reg loss: 0.615, (0.047), accuracy:0.741\n",
      "Epoch:1200, Loss:0.559, Reg loss: 0.605, (0.046), accuracy:0.737\n",
      "Epoch:1300, Loss:0.558, Reg loss: 0.603, (0.045), accuracy:0.767\n",
      "Epoch:1400, Loss:0.556, Reg loss: 0.600, (0.044), accuracy:0.755\n",
      "Epoch:1500, Loss:0.566, Reg loss: 0.611, (0.044), accuracy:0.751\n",
      "Epoch:1600, Loss:0.576, Reg loss: 0.620, (0.043), accuracy:0.752\n",
      "Epoch:1700, Loss:0.557, Reg loss: 0.600, (0.043), accuracy:0.754\n",
      "Epoch:1800, Loss:0.540, Reg loss: 0.582, (0.042), accuracy:0.756\n",
      "Epoch:1900, Loss:0.565, Reg loss: 0.606, (0.042), accuracy:0.756\n",
      "Epoch:2000, Loss:0.556, Reg loss: 0.597, (0.041), accuracy:0.753\n",
      "Epoch:2100, Loss:0.547, Reg loss: 0.588, (0.041), accuracy:0.760\n",
      "Epoch:2200, Loss:0.567, Reg loss: 0.607, (0.040), accuracy:0.752\n",
      "Epoch:2300, Loss:0.560, Reg loss: 0.600, (0.041), accuracy:0.763\n",
      "Epoch:2400, Loss:0.557, Reg loss: 0.597, (0.041), accuracy:0.765\n",
      "Epoch:2500, Loss:0.542, Reg loss: 0.582, (0.040), accuracy:0.757\n"
     ]
    }
   ],
   "source": [
    "X, y = spiral_data(samples=1000, classes=3)\n",
    "y = one_hot_encode_index(y, 3)\n",
    "\n",
    "# Initializing Network Components \n",
    "relu = ReLU()\n",
    "softmax = Softmax()\n",
    "cce_loss = CategoricalCrossEntropyLoss\n",
    "optimizer = Adam(learning_rate=0.05, decay=5e-5)\n",
    "dropout = Dropout(0.9)\n",
    "linear1 = LinearLayer(2, 64, lambda_l2_weight=5e-4, lambda_l2_bias=5e-4)\n",
    "linear2 = LinearLayer(64, 3)\n",
    "\n",
    "update_layers = [linear1, linear2]\n",
    "\n",
    "n_epochs = 10000\n",
    "\n",
    "for epoch in range(n_epochs + 1):\n",
    "    # Forward Pass\n",
    "    linear1.forward(X) \n",
    "    relu.forward(linear1.output)\n",
    "    dropout.forward(relu.output)\n",
    "    linear2.forward(dropout.output)\n",
    "    y_pred = softmax.forward(linear2.output)\n",
    "\n",
    "    # Calculating loss and regularized loss  \n",
    "    loss = cce_loss.forward(y_pred, y)\n",
    "    # Regularised loss\n",
    "    rl = sum([l.regularization_loss() for l in update_layers if hasattr(l, 'regularization_loss')])\n",
    "    regularized_loss = loss + rl\n",
    "\n",
    "    #Calculating accuracy \n",
    "    acc = accuracy(y_pred, y) \n",
    "\n",
    "    # Printing results\n",
    "    if not epoch % 100:\n",
    "        print(f\"Epoch:{epoch}, Loss:{loss:.3f}, Reg loss: {regularized_loss:.3f}, ({rl:.3f}), accuracy:{acc:.3f}\")\n",
    "\n",
    "    # Backward pass\n",
    "    softmax.backward(y_pred, y)\n",
    "    linear2.backward(softmax.grad)\n",
    "    dropout.backward(linear2.grad)\n",
    "    relu.backward(dropout.grad)\n",
    "    linear1.backward(relu.grad)\n",
    "\n",
    "    # Optimization Step\n",
    "    optimizer.update(update_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68645370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vallidation: Accuracy: 0.740, loss: 0.585\n"
     ]
    }
   ],
   "source": [
    "# Validating the model \n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "y_test = one_hot_encode_index(y_test, 3)\n",
    "\n",
    "linear1.forward(X_test) \n",
    "relu.forward(linear1.output)\n",
    "linear2.forward(relu.output)\n",
    "y_pred = softmax.forward(linear2.output)\n",
    "\n",
    "# Calculating loss and accuracy \n",
    "loss = cce_loss.forward(y_pred, y_test)\n",
    "acc = accuracy(y_pred, y_test) \n",
    "\n",
    "print(f'Vallidation: Accuracy: {acc:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb1deb2",
   "metadata": {},
   "source": [
    "## Defining Model Object  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c4fe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"Model class designed as a container to simplify the building and training of networks.\n",
    "    \n",
    "    Args:\n",
    "    optimizer:  The optimizer that should be used\n",
    "    Loss:       The loss class that should be used\n",
    "\n",
    "    Attributes:\n",
    "        layers              (list):  List of all layers in the network in their activation sequence \n",
    "        trainable_layers    (list):  List of trainable layers in the network\n",
    "        loss                ():      The loss class that should be used\n",
    "        optim               ():      The optimizer that should be used\n",
    "        current_loss        (float): Latest loss recorded \n",
    "        current_accuracy    (float): Latest accuracy recorded\n",
    "        training_mode       (bool):  Boolean flag if the network is in training mode\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, loss) -> None:\n",
    "        self.layers = []\n",
    "        self.trainable_layers = []\n",
    "        self.loss = loss\n",
    "        self.optim = optimizer\n",
    "        self.current_loss = 0\n",
    "        self.current_accuracy = 0\n",
    "        self.training_mode = True\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Custom dunder representer method to print out all the layers of the network.\"\"\"\n",
    "        layer_str = \"\".join([f\"\\t ({i}): {type(l).__name__} (Trainable: {l in self.trainable_layers})\\n\" \n",
    "                            for i, l in enumerate(self.layers)])\n",
    "\n",
    "        return \"Model Architecture: \\n\" + layer_str\n",
    "    \n",
    "    def add(self, layer):\n",
    "        \"\"\"Appends a single layer to the end of the network\"\"\"\n",
    "        self.layers.append(layer)\n",
    "        if hasattr(layer, 'weights'):\n",
    "            self.trainable_layers.append(layer)\n",
    "\n",
    "    def set_sequence(self, layers):\n",
    "        \"\"\"Defines an entire sequence of layers. NOTE: will overwrite current network\"\"\"\n",
    "        self.layers = layers\n",
    "        self.trainable_layers = [l for l in layers if hasattr(l, 'weights')]\n",
    "\n",
    "    def get_loss(self, y_pred, y_true):\n",
    "        \"\"\"Calculates the current loss of the network\"\"\"\n",
    "        # Calculating loss and regularized loss  \n",
    "        loss = self.loss.forward(y_pred, y_true)\n",
    "        # Regularised loss\n",
    "        rl = sum([l.regularization_loss() for l in self.trainable_layers if hasattr(l, 'regularization_loss')])\n",
    "        return loss + rl\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Handles forward pass through all layers\"\"\"\n",
    "        # First layer\n",
    "        self.layers[0].forward(X)\n",
    "\n",
    "        # Rest of layers \n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.layers[i].forward(self.layers[i-1].output)\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"Handles backward pass through all layers\"\"\"\n",
    "        # Last layer\n",
    "        self.layers[-1].backward(y_pred, y_true)\n",
    "\n",
    "        # Rest of the layers\n",
    "        for i, l in reversed(list(enumerate(self.layers[:-1]))):\n",
    "            l.backward(self.layers[i+1].grad)\n",
    "        \n",
    "    def logger(self, epoch):\n",
    "        \"\"\"Prints current state of model\"\"\"\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, accuracy{self.current_accuracy:.3f}, loss{self.current_loss:.3f}, learning rate {self.optim.clr:.3f} \"\n",
    "        )\n",
    "    \n",
    "    def validate(self, X_val, y_val):\n",
    "        \"\"\"Handles the validation pass of the network\"\"\"\n",
    "        self.forward(X_val)\n",
    "\n",
    "        loss = self.loss.forward(self.layers[-1].output, y_val)\n",
    "        acc  = accuracy(self.layers[-1].output, y_val)\n",
    "\n",
    "        print(\n",
    "            f\"Validation : Loss: {loss:.3f}, Accuracy: {acc:.3f}\"\n",
    "        )\n",
    "\n",
    "    def mode_train(self):\n",
    "        \"\"\"Sets the model and all dropout layers to training mode.\"\"\"\n",
    "        self.training_mode = True\n",
    "        for l in self.layers:\n",
    "            if type(l) == Dropout:\n",
    "                l.training_mode = True\n",
    "    \n",
    "    def mode_eval(self):\n",
    "        \"\"\"Sets the model and all dropout layers to evaluation mode.\"\"\"\n",
    "        self.training_mode = False\n",
    "        for l in self.layers:\n",
    "            if type(l) == Dropout:\n",
    "                l.training_mode = False\n",
    "    \n",
    "    def train(self, X, y, epochs=1, log=True, log_freq=100):\n",
    "        \"\"\"Handles the trining loop.\"\"\"\n",
    "        for epoch in range(epochs + 1):\n",
    "\n",
    "            # Forward Pass\n",
    "            self.forward(X)\n",
    "\n",
    "            # Loss \n",
    "            self.current_loss = self.get_loss(self.layers[-1].output, y)\n",
    "\n",
    "            # accuracy \n",
    "            self.current_accuracy = accuracy(self.layers[-1].output, y) \n",
    "            \n",
    "            # Backward Pass\n",
    "            self.backward(self.layers[-1].output, y)\n",
    "\n",
    "            # Optimization \n",
    "            self.optim.update(self.trainable_layers)\n",
    "\n",
    "            # Logging \n",
    "            if log and not (epoch % log_freq):\n",
    "                self.logger(epoch)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5556a0db",
   "metadata": {},
   "source": [
    "#### Testing Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b217ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model Architecture: \n",
       "\t (0): LinearLayer (Trainable: True)\n",
       "\t (1): ReLU (Trainable: False)\n",
       "\t (2): Dropout (Trainable: False)\n",
       "\t (3): LinearLayer (Trainable: True)\n",
       "\t (4): Softmax (Trainable: False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cce_loss = CategoricalCrossEntropyLoss\n",
    "optimizer = Adam(learning_rate=0.05, decay=5e-5)\n",
    "\n",
    "model = Model(optimizer, cce_loss)\n",
    "model.set_sequence([\n",
    "    LinearLayer(2, 512, lambda_l2_weight=5e-4, lambda_l2_bias=5e-4),\n",
    "    ReLU(),\n",
    "    Dropout(0.9),\n",
    "    LinearLayer(512, 3)\n",
    "])\n",
    "model.add(Softmax())\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d117c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, accuracy0.380, loss1.645, learning rate 0.050 \n",
      "Epoch: 100, accuracy0.487, loss0.986, learning rate 0.050 \n",
      "Epoch: 200, accuracy0.688, loss0.839, learning rate 0.050 \n",
      "Epoch: 300, accuracy0.737, loss0.739, learning rate 0.049 \n",
      "Epoch: 400, accuracy0.767, loss0.690, learning rate 0.049 \n",
      "Epoch: 500, accuracy0.776, loss0.686, learning rate 0.049 \n",
      "Epoch: 600, accuracy0.787, loss0.641, learning rate 0.049 \n",
      "Epoch: 700, accuracy0.792, loss0.637, learning rate 0.048 \n",
      "Epoch: 800, accuracy0.821, loss0.584, learning rate 0.048 \n",
      "Epoch: 900, accuracy0.787, loss0.631, learning rate 0.048 \n",
      "Epoch: 1000, accuracy0.825, loss0.576, learning rate 0.048 \n",
      "Epoch: 1100, accuracy0.769, loss0.689, learning rate 0.047 \n",
      "Epoch: 1200, accuracy0.822, loss0.579, learning rate 0.047 \n",
      "Epoch: 1300, accuracy0.820, loss0.547, learning rate 0.047 \n",
      "Epoch: 1400, accuracy0.805, loss0.605, learning rate 0.047 \n",
      "Epoch: 1500, accuracy0.844, loss0.513, learning rate 0.047 \n",
      "Epoch: 1600, accuracy0.831, loss0.529, learning rate 0.046 \n",
      "Epoch: 1700, accuracy0.823, loss0.593, learning rate 0.046 \n",
      "Epoch: 1800, accuracy0.831, loss0.526, learning rate 0.046 \n",
      "Epoch: 1900, accuracy0.822, loss0.528, learning rate 0.046 \n",
      "Epoch: 2000, accuracy0.844, loss0.504, learning rate 0.045 \n",
      "Epoch: 2100, accuracy0.840, loss0.519, learning rate 0.045 \n",
      "Epoch: 2200, accuracy0.840, loss0.503, learning rate 0.045 \n",
      "Epoch: 2300, accuracy0.841, loss0.498, learning rate 0.045 \n",
      "Epoch: 2400, accuracy0.848, loss0.497, learning rate 0.045 \n",
      "Epoch: 2500, accuracy0.846, loss0.501, learning rate 0.044 \n",
      "Epoch: 2600, accuracy0.850, loss0.484, learning rate 0.044 \n",
      "Epoch: 2700, accuracy0.827, loss0.521, learning rate 0.044 \n",
      "Epoch: 2800, accuracy0.852, loss0.491, learning rate 0.044 \n",
      "Epoch: 2900, accuracy0.842, loss0.494, learning rate 0.044 \n",
      "Epoch: 3000, accuracy0.840, loss0.483, learning rate 0.043 \n",
      "Epoch: 3100, accuracy0.851, loss0.469, learning rate 0.043 \n",
      "Epoch: 3200, accuracy0.840, loss0.523, learning rate 0.043 \n",
      "Epoch: 3300, accuracy0.847, loss0.461, learning rate 0.043 \n",
      "Epoch: 3400, accuracy0.854, loss0.460, learning rate 0.043 \n",
      "Epoch: 3500, accuracy0.849, loss0.468, learning rate 0.043 \n",
      "Epoch: 3600, accuracy0.849, loss0.471, learning rate 0.042 \n",
      "Epoch: 3700, accuracy0.851, loss0.482, learning rate 0.042 \n",
      "Epoch: 3800, accuracy0.841, loss0.489, learning rate 0.042 \n",
      "Epoch: 3900, accuracy0.849, loss0.467, learning rate 0.042 \n",
      "Epoch: 4000, accuracy0.864, loss0.453, learning rate 0.042 \n",
      "Epoch: 4100, accuracy0.863, loss0.450, learning rate 0.041 \n",
      "Epoch: 4200, accuracy0.859, loss0.469, learning rate 0.041 \n",
      "Epoch: 4300, accuracy0.846, loss0.462, learning rate 0.041 \n",
      "Epoch: 4400, accuracy0.864, loss0.470, learning rate 0.041 \n",
      "Epoch: 4500, accuracy0.860, loss0.444, learning rate 0.041 \n",
      "Epoch: 4600, accuracy0.858, loss0.455, learning rate 0.041 \n",
      "Epoch: 4700, accuracy0.850, loss0.476, learning rate 0.040 \n",
      "Epoch: 4800, accuracy0.856, loss0.462, learning rate 0.040 \n",
      "Epoch: 4900, accuracy0.854, loss0.477, learning rate 0.040 \n",
      "Epoch: 5000, accuracy0.855, loss0.460, learning rate 0.040 \n",
      "Epoch: 5100, accuracy0.854, loss0.467, learning rate 0.040 \n",
      "Epoch: 5200, accuracy0.863, loss0.456, learning rate 0.040 \n",
      "Epoch: 5300, accuracy0.859, loss0.444, learning rate 0.040 \n",
      "Epoch: 5400, accuracy0.859, loss0.447, learning rate 0.039 \n",
      "Epoch: 5500, accuracy0.843, loss0.459, learning rate 0.039 \n",
      "Epoch: 5600, accuracy0.853, loss0.430, learning rate 0.039 \n",
      "Epoch: 5700, accuracy0.861, loss0.446, learning rate 0.039 \n",
      "Epoch: 5800, accuracy0.863, loss0.445, learning rate 0.039 \n",
      "Epoch: 5900, accuracy0.861, loss0.442, learning rate 0.039 \n",
      "Epoch: 6000, accuracy0.857, loss0.446, learning rate 0.038 \n",
      "Epoch: 6100, accuracy0.855, loss0.460, learning rate 0.038 \n",
      "Epoch: 6200, accuracy0.861, loss0.429, learning rate 0.038 \n",
      "Epoch: 6300, accuracy0.863, loss0.434, learning rate 0.038 \n",
      "Epoch: 6400, accuracy0.859, loss0.428, learning rate 0.038 \n",
      "Epoch: 6500, accuracy0.862, loss0.436, learning rate 0.038 \n",
      "Epoch: 6600, accuracy0.856, loss0.452, learning rate 0.038 \n",
      "Epoch: 6700, accuracy0.852, loss0.476, learning rate 0.037 \n",
      "Epoch: 6800, accuracy0.852, loss0.450, learning rate 0.037 \n",
      "Epoch: 6900, accuracy0.858, loss0.456, learning rate 0.037 \n",
      "Epoch: 7000, accuracy0.859, loss0.438, learning rate 0.037 \n",
      "Epoch: 7100, accuracy0.856, loss0.460, learning rate 0.037 \n",
      "Epoch: 7200, accuracy0.866, loss0.409, learning rate 0.037 \n",
      "Epoch: 7300, accuracy0.849, loss0.447, learning rate 0.037 \n",
      "Epoch: 7400, accuracy0.858, loss0.447, learning rate 0.036 \n",
      "Epoch: 7500, accuracy0.874, loss0.428, learning rate 0.036 \n",
      "Epoch: 7600, accuracy0.868, loss0.435, learning rate 0.036 \n",
      "Epoch: 7700, accuracy0.861, loss0.443, learning rate 0.036 \n",
      "Epoch: 7800, accuracy0.846, loss0.477, learning rate 0.036 \n",
      "Epoch: 7900, accuracy0.863, loss0.456, learning rate 0.036 \n",
      "Epoch: 8000, accuracy0.865, loss0.415, learning rate 0.036 \n",
      "Epoch: 8100, accuracy0.854, loss0.455, learning rate 0.036 \n",
      "Epoch: 8200, accuracy0.862, loss0.456, learning rate 0.035 \n",
      "Epoch: 8300, accuracy0.872, loss0.438, learning rate 0.035 \n",
      "Epoch: 8400, accuracy0.849, loss0.469, learning rate 0.035 \n",
      "Epoch: 8500, accuracy0.856, loss0.449, learning rate 0.035 \n",
      "Epoch: 8600, accuracy0.870, loss0.436, learning rate 0.035 \n",
      "Epoch: 8700, accuracy0.856, loss0.465, learning rate 0.035 \n",
      "Epoch: 8800, accuracy0.863, loss0.423, learning rate 0.035 \n",
      "Epoch: 8900, accuracy0.858, loss0.430, learning rate 0.035 \n",
      "Epoch: 9000, accuracy0.866, loss0.441, learning rate 0.034 \n",
      "Epoch: 9100, accuracy0.867, loss0.429, learning rate 0.034 \n",
      "Epoch: 9200, accuracy0.861, loss0.441, learning rate 0.034 \n",
      "Epoch: 9300, accuracy0.866, loss0.431, learning rate 0.034 \n",
      "Epoch: 9400, accuracy0.868, loss0.419, learning rate 0.034 \n",
      "Epoch: 9500, accuracy0.868, loss0.418, learning rate 0.034 \n",
      "Epoch: 9600, accuracy0.860, loss0.420, learning rate 0.034 \n",
      "Epoch: 9700, accuracy0.862, loss0.432, learning rate 0.034 \n",
      "Epoch: 9800, accuracy0.861, loss0.448, learning rate 0.034 \n",
      "Epoch: 9900, accuracy0.866, loss0.428, learning rate 0.033 \n",
      "Epoch: 10000, accuracy0.854, loss0.447, learning rate 0.033 \n"
     ]
    }
   ],
   "source": [
    "X, y = spiral_data(samples=1000, classes=3)\n",
    "y = one_hot_encode_index(y, 3)\n",
    "model.mode_train()\n",
    "model.train(X, y, epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120132a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation : Loss: 0.379, Accuracy: 0.900\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "y_test = one_hot_encode_index(y_test, 3)\n",
    "model.mode_eval()\n",
    "model.validate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca6e8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
